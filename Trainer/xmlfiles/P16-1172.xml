<sec_map><section><chunk>Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 18251836, Berlin, Germany, August 7-12, 2016. c 2016 Association for Computational Linguistics Optimizing an Approximation of ROUGE a Problem-Reduction Approach to Extractive Multi-Document Summarization Maxime Peyrard and Judith Eckle-Kohler Research Training Group AIPHES and UKP Lab Computer Science Department, Technische Universit  at Darmstadt www.aiphes.tu-darmstadt.de, www.ukp.tu-darmstadt.de Abstract This paper presents a problem-reduction approach to extractive multi-document summarization: we propose a reduction to the problem of scoring individual sen- tences with their ROUGE scores based on supervised learning. For the summariza- tion, we solve an optimization problem where the ROUGE score of the selected summary sentences is maximized. To this end, we derive an approximation of the ROUGE-N score of a set of sentences, and define a principled discrete optimization problem for sentence selection. Mathe- matical and empirical evidence suggests that the sentence selection step is solved almost exactly, thus reducing the problem to the sentence scoring task. We perform a detailed experimental evaluation on two DUC datasets to demonstrate the validity of our approach. </chunk></section><section><heading>1 Introduction </heading><chunk>Multi-document summarization (MDS) is the task of constructing a summary from a topically re- lated document collection. This paper focuses on the variant of extractive and generic MDS, which has been studied in detail for the news domain us- ing available benchmark datasets from the Docu- ment Understanding Conference (DUC) (Over et al., 2007). Extractive MDS can be cast as a budgeted sub- set selection problem (McDonald, 2007; Lin and Bilmes, 2011) where the document collection is considered as a set of sentences and the task is to select a subset of the sentences under a length constraint. State-of-the-art and recent works in extractive MDS solve this discrete optimization problem using integer linear programming (ILP) or submodular function maximization (Gillick and Favre, 2009; Mogren et al., 2015; Li et al., 2013b; Kulesza and Taskar, 2012; Hong and Nenkova, 2014). The objective function that is maximized in the optimization step varies considerably in pre- vious work. For instance, Yih et al. (2007) maxi- mize the number of informative words, Gillick and Favre (2009) the coverage of particular concepts, and others maximize a notion of summary wor- thiness, while minimizing summary redundancy (Lin and Bilmes, 2011; K  ageb  ack et al., 2014). There are also multiple approaches which max- imize the evaluation metric for system sum- maries itself based on supervised Machine Learn- ing (ML). System summaries are commonly eval- uated using ROUGE (Lin, 2004), a recall oriented metric that measures the n-gram overlap between a system summary and a set of human-written ref- erence summaries. The benchmark datasets for MDS can be em- ployed in two different ways for supervised learn- ing of ROUGE scores: either by training a model that assigns ROUGE scores to individual tex- tual units (e.g., sentences), or by performing structured output learning and directly maximiz- ing the ROUGE scores of the created summaries (Nishikawa et al., 2014; Takamura and Okumura, 2010; Sipos et al., 2012). The latter approach suf- fers both from the limited amount of training data and from the higher complexity of the machine learning models. In contrast, supervised learning of ROUGE scores for individual sentences can be performed with simple regression models using hundreds of sentences as training instances, taken from a sin- gle pair of documents and reference summaries. Extractive MDS can leverage the ROUGE scores of individual sentences in various ways, in partic- ular, as part of an optimization step. In our work, we follow the previously successful approaches to 1825 extractive MDS using discrete optimization, and make the following contributions: We provide a theoretical justification and em- pirical validation for using ROUGE scores of individual sentences as an optimization objec- tive. Assuming that ROUGE scores of individ- ual sentences have been estimated by a super- vised learner, we derive an approximation of the ROUGE-N score for a set of sentences from the ROUGE-N scores of the individual sentences in the general case of N &gt;= 1. We use our approximation to define a math- ematically principled discrete optimization prob- lem for sentence selection. We empirically evalu- ate our framework on two DUC datasets, demon- strating the validity of our approximation, as well as its ability to achieve competitive ROUGE scores in comparison to several strong baselines. Most importantly, the resulting framework re- duces the MDS task to the problem of scoring in- dividual sentences with their ROUGE scores. The overall summarization task is converted to two se- quential tasks: (i) scoring single sentences, and (ii) selecting summary sentences by solving an opti- mization problem where the ROUGE score of the selected sentences is maximized. The optimization objective we propose almost exactly solves (ii), which we justify by provid- ing both mathematical and empirical evidence. Hence, solving the whole problem of MDS is re- duced to solving (i). The rest of this paper is structured as follows: in Section 2, we discuss related work. Section 3 presents our subset selection framework consist- ing of an approximation of the ROUGE score of a set of sentences, and a mathematically principled discrete optimization problem for sentence selec- tion. We evaluate our framework in Section 4 and discuss the results in Section 5. Section 6 con- cludes. </chunk></section><section><heading>2 Related Work </heading><chunk>Related to our approach is previous work in ex- tractive MDS that (i) casts the summarization problem as budgeted subset selection, and (ii) employs supervised learning on MDS datasets to learn a scoring function for textual units. Budgeted Subset Selection Extractive MDS can be formulated as the problem of selecting a subset of textual units from a document collection such that the overall score of the created summary is maximal and a given length constraint is ob- served. The selection of textual units for the sum- mary relies on their individual scores, assigned by a scoring function which represents aspects of their relevance for a summary. Often, sentences are considered as textual units. Simultaneously maximizing the relevance scores of the selected units and minimizing their pairwise redundancy given a length constraint is a global inference problem which can be solved using ILP (McDonald, 2007). Several state-of-the-art results in MDS have been obtained by using ILP to maximize the number of relevant concepts in the created summary while minimiz- ing the pairwise similarity between the selected sentences (Gillick and Favre, 2009; Boudin et al., 2015; Woodsend and Lapata, 2012). Another way to formulate the problem of find- ing the best subset of textual units is to maxi- mize a submodular function. Maximizing sub- modular functions is a general technique that uses a greedy optimization algorithm with a mathe- matical guarantee on optimality (Nemhauser and Wolsey, 1978). Performing summarization in the framework of submodularity is natural because summaries try to maximize the coverage of rele- vant units while minimizing redundancy (Lin and Bilmes, 2011). However, several different cover- age and redundancy functions have been proposed (Lin and Bilmes, 2011; K  ageb  ack et al., 2014; Yin and Pei, 2015) recently, and there is not yet a clear consensus on which coverage function to maxi- mize. Supervised Learning Supervised learning us- ing datasets with reference summaries has already been employed in early work on summarization to classify sentences as summary-worthy or not (Ku- piec et al., 1995; Aone et al., 1995). Learning a scoring function for various kinds of textual units has become especially popular in the context of global optimization: scores of tex- tual units, learned from data, are fed into an ILP problem solver to find the subset of sentences with maximal overall score. For example, Yih et al. (2007) score each word in the document cluster based on frequency and position, Li et al. (2013b) learn bigram frequency in the reference summaries, and Hong and Nenkova (2014) learn word importance from a rich set of features. Closely related to our work are summarization approaches that include a supervised component 1826 which assigns ROUGE scores to individual sen- tences. For example, Ng et al. (2012), Li et al. (2013a) and Li et al. (2015) all use a regres- sion model to learn ROUGE-2 scores for indi- vidual sentences, but use it in different ways for the summarization. While Ng et al. (2012) use the ROUGE scores of sentences in combination with the Maximal Marginal Relevance algorithm as a baseline approach, Li et al. (2013a) use the scores to select the top-ranked sentences for sen- tence compression and subsequent summarization. Li et al. (2015), in contrast, use the ROUGE scores to re-rank a set of sentences that are output by an optimization step. While learning ROUGE scores of textual units is widely used in summarization systems, the the- oretical background on why this is useful has not been well studied yet. In our work, we present the mathematical and empirical justification for this common practice. In the next section, we start with the mathematical justification. </chunk></section><section><heading>3 Content Selection Framework </heading><chunk>3.1 Approximation of ROUGE-N Notation: Let S = {s i |i m} be a set of m sentences which constitute a system summary. We use N (S) or simply (S) to denote the ROUGE- N score of S. ROUGE-N evaluates the n-gram overlap between S and a set of reference sum- maries (Lin, 2004). Let S denote the reference summary and R N the number of n-gram tokens in S . R N is a function of the summary length in words, in particular, R 1 is the target size of the summary in words. Finally, let F S (g) denote the number of times the n-gram type g occurs in S. For a single reference summary, ROUGE-N is computed as follows: (S) = 1 R N gS min(F S (g), F S (g)) (1) For compactness, we use the following notation for any set of sentences X: C X,S (g) = min(F X (g), F S (g)) (2) C X,S (g) can be understood as the contribution of the n-gram g. ROUGE-N for a Pair of Sentences: Using this notation, the ROUGE-N score of a set of two sen- tences a and b can be written as: (a b) = 1 R N gS min(C ab,S (g), F S (g)) (3) We observe that (a b) can be expressed as a function of the individual scores (a) and (b): (a b) = (a) + (b) (a b) (4) where (a b) is an error correction term that dis- cards overcounted n-grams from the sum of (a) and (b): (a b) = 1 R N gS max(C a,S (g)+C b,S (g)F S (g), 0) (5) A proof that this error correction is correct is given in appendix A.1. General Formulation of ROUGE-N: We can extend the previous formulation of to sets of ar- bitrary cardinality using recursion. If (S) is given for a set of sentences S, and a is a sentence then: (S a) = (S) + (a) (S a) (6) We prove in appendix A.1 that this formula is the ROUGE-N score of S a. Another way to obtain for an arbitrary set S is to adapt the principle of inclusion-exclusion: (S) = m i=1 (s i )+ m k=2 (1) k+1 ( 1i 1 i k m (k) (s i 1 s i k )) (7) This formula can be understood as adding up scores of individual sentences, but n-grams ap- pearing in the intersection of two sentences might be overcounted. (2) is used to account for these n-grams. But now, n-grams in the intersection of three sentences might be undercounted and (3) is used to correct this. Each (k) contributes to im- proving the accuracy by refining the errors made by (k1) for the n-grams appearing in the inter- section of k sentences. When k = |S|, (S) is exactly the ROUGE-N of S. A rigorous proof and details about (k) are provided in appendix A.2. 1827 Approximation of ROUGE-N for a Pair of Sen- tences: To find a valid approximation of as de- fined in (7), we first consider the (a b) from equation (3) and then extend it to the general case. When maximizing , scores for sentences are as- sumed to be given (e.g., estimated by a ML com- ponent). We still need to estimate (a b), which means, according to (5), to estimate: gS max(C a,S (g) + C b,S (g) F S (g), 0) (8) At inference time, neither S (the reference sum- mary) nor F S (number of occurrences of n-grams in the reference summary) is known. At this point, we can observe that, similar as for sentence scoring, can be estimated via a su- pervised ML component. Such an ML model can easily be trained on the intersections of all sen- tence pairs in a given training dataset. Hence, we can assume that both the scores for individual sen- tences and the are learned empirically from data using ML. As a result, we have pushed all estima- tion steps into supervised ML components, which leaves the subset selection step fully principled. However, we found in our experiments that even a simple heuristic yields a decent approximation of . The heuristic uses the frequency f req(g) of an n-gram g observed in the source documents: gS max(C a,S (g) + C b,S (g) F S (g), 0) gab 1[freq(g) ] (9) The threshold tells us which n-grams are likely to appear in the reference summary, and it is de- termined by grid-search on the training set. This is penalizing n-grams which appear twice and are likely to occur in the summary. It can be under- stood as a way of limiting redundancy. In prac- tice, we used = 0.3. However, we experimented with various values of the hyper-parameter and found that its value has no significant impact as long as it is fairly small (&lt; 0.5). Higher values will ignore too many redundant n-grams and the summary will have a high redundancy. R N is known since it is simply the number of n-gram tokens in the summaries. We end up with the following approximation for the pairwise case: (a b) = (a) + (b) (a b), where (a b) = 1 R N gab 1[freq(g) ] (10) General Approximation of ROUGE-N: Now, we can approximate (S) for the general case de- fined by equation (7). We recall that (S) con- tains the sum of (s i ), the pairwise error terms (2) (s i s j ), the error terms of three sentences (3) and so on. We can restrict ourselves to the individual sen- tences and the pairwise error corrections. Indeed, the intersection between more than two sentences is often empty, and accounting for it does not im- prove the accuracy significantly, but greatly in- creases the computational cost. A formulation of in the case of two sentences has already been defined in (10). Thus, we have an approximation of the ROUGE-N function for any set of sentences that can be computed at inference time: (S) = n i=1 (s i ) a,bS,a =b (a b) (11) We empirically checked the validity of this ap- proximation. For this, we sampled 1000 sets of sentences from source documents of DUC-2003 (sets of 2 to 5 sentences) and compared their score to the real ROUGE-N. We observe a pear- sons r correlation 0.97, which validates . </chunk></section><section><heading>3.2 Discrete Optimization </heading><chunk>from equation (11) defines a set function that scores a set of sentences. The task of summariza- tion is now to select the set S with maximal (S ) under a length constraint. Submodularity: A submodular function is a set function obeying the diminishing returns property: S T and a sentence a: F (S a) F (S) F (T a) F (T ). Submodular functions are con- venient because maximization under constraints can be done greedily with a guarantee of the op- timality of the solution (Nemhauser et al., 1978). It has been shown that ROUGE-N is submodu- lar (Lin and Bilmes, 2011) and it is easy to verify that is submodular as well (the proof is given in the supplemental material). We can therefore apply the greedy maximiza- tion algorithm to find a good set of sentences. This has the advantage of being straightforward and fast, however it does not necessarily find the op- timal solution. ILP: A common way to solve a discrete opti- mization problem is to formulate it as an ILP. It 1828 maximizes (or minimizes) a linear objective func- tion with some linear constraints where the vari- ables are integers. ILP has been well studied and existing tools can efficiently retrieve the exact so- lution of an ILP problem. We observe that it is possible to formulate the maximization of (S) as an ILP. Let x be the binary vector whose i-th entry indicates whether sentence i is in the summary or not, (s i ) the scores of sentences, and K the length constraint. We pre-compute the symmetric matrix P where P i,j = (s i s j ) and solve the following ILP: max( n i=1 x i (s i ) d 1 R ij i,j P i, j) n i=1 x i len(s i ) K (i, j), i,j x i 0 (i, j), i,j x j 0 (i, j), x i + x j i,j 1 d is a damping factor that allows to account for approximation errors. When d = 0, the problem becomes the maximization of summary worthi- ness under a length constraint, with summary worthiness being defined by (s i ). In practice, we used a value d = 0.9 because we observed that the learner tends to slightly over- estimate the ROUGE-N scores of sentences. The mathematical derivation implies d = 1, however we can easily adjust for shifts in average scores of sentences from the estimation step by adjust- ing d. Another option would be to post-process the scores after the estimation step to fix the av- erage and let d = 1 in the optimization step. In- deed, if d moves away from 1, we move away from the mathematical framework of ROUGE-N maxi- mization. If d = 0, it seems intuitive to interpret the sec- ond term as minimizing the summary redundancy, which is in accordance to previous works. However, in our framework, this term has a pre- cise interpretation: it maximizes ROUGE-N scores up to the second order of precision, and the ROUGE-N formula it- self already induces a notion of summary worthi- ness and redundancy, which we can empirically infer from data via supervised ML for sentence scoring, and a simple heuristic for sentence inter- sections. </chunk></section><section><heading>4 Evaluation </heading><chunk>We perform three kinds of experiments in order to empirically evaluate our framework: first, we show that our proposed approximation is valid, then we analyze a basic supervised sentence scor- ing component, and finally we perform an extrin- sic evaluation on end-to-end extractive MDS. In our experiments, we use the DUC datasets from 2002 and 2003 (DUC-02 and DUC-03). We use the variants of ROUGE identified by Owczarzak et al. (2012) as strongly correlating with human evaluation methods: ROUGE-2 re- call with stemming and stopwords not removed (giving the best agreement with human evalua- tion), and ROUGE-1 recall (as the measure with the highest ability to identify the better summary in a pair of system summaries). For DUC-03, summaries are truncated to 100 words, and to 200 words for DUC-02. 1 The truncation is done auto- matically by ROUGE. 2 </chunk></section><section><heading>4.1 Framework Validity </heading><chunk>Given that sentences receive scores close to their individual ROUGE-N, we presented a function that approximates the ROUGE-N of sets of these sentences and proposed an optimization to find the best scoring set under a length constraint. To validate our framework empirically, we con- sider its upper-bound, which is obtained when our ILP/submodular optimizations use the real ROUGE-N scores of the individual sentences, cal- culated based on the reference summaries. We compare this upper bound to a greedy approach, which simply adds the best scoring sentences one by one to the subset until the length limit is reached, and to the real upper bound for extractive summarization which is determined by solving a maximum coverage problem for n-grams from the reference summary (as it was done by Takamura and Okumura (2010)). Table 1 shows the results. We observe that ILP- R produces scores close to the reference, thus re- ducing the problem of extractive summarization to the task of sentence scoring, because the perfect scores induced near perfect extracted summaries in this framework. SBL-R seems less promising than ILP-R because it greedily maximizes a func- tion which ILP-R exactly maximizes. Therefore, we continue our experiments in the following sec- 1 In the official DUC-03 competitions, summaries of length 665 bytes were expected. Systems could produce dif- ferent numbers of words. The variation in length has a no- ticeable impact on ROUGE recall scores. 2 ROUGE-1.5.5 with the parameters: -n 2 -m -a -l 100 -x -c 95 -r 1000 -f A -p 0.5 -t 0. The length parameter becomes -l 200 for DUC-02. 1829 tions with ILP-R only. However, SBL-R offers a nice trade-off between performance and computa- tion cost. The greedy optimization of SBL-R is noticeably faster than ILP-R. DUC-02 DUC-03 R1 R2 R1 R2 Greedy 0.597 0.414 0.391 0.148 SBL-R 0.630 0.484 0.424 0.160 ILP-R 0.644 0.495 0.447 0.178 Upper Bound 0.648 0.497 0.452 0.181 Table 1: Upper bound of our framework compared to extractive upper bound. In practice, the learner will not produce per- fect scores. We experimentally validated that with learned scores converging to true scores, the ex- tracted summary converges to the best extrac- tive summary (w.r.t to ROUGE-N). To this end, we simulated approximate learners by artificially randomizing the true scores to end up with lists having various correlations with the true scores. We fed these scores to ILP-R and computed the ROUGE-1 of the generated summaries for an ex- ample topic from DUC-2003. Figure 1 displays the expected ROUGE-1 versus the performance of the artificial learner (correlation with true scores of sentences). We observe that, as the learner im- proves, the generated summaries approach the best ROUGE scoring summary. Figure 1: ROUGE-1 of summary against sentence scores correlation with true ROUGE-1 scores of sentences (d30003t from DUC-2003). </chunk></section><section><heading>4.2 Sentence Scoring </heading><chunk>Now we look at the supervised learning compo- nent which learns ROUGE-N scores for individ- ual sentences. We know that we can achieve an overall summary ROUGE-N score close to the up- per bound, if a learner would be able to learn the scores perfectly. For better understanding the dif- ficulty of the task of sentence scoring, we look at the correlation of the scores produced by a basic learner and the true scores given in a reference dataset. Model and Features From an existing summa- rization dataset (e.g. a DUC dataset), a training set can straightforwardly be extracted by annotat- ing each sentence in the source documents with its ROUGE-N score. For each topic in the dataset, this yields a list of sentences and their target score. To support the claim that learning ROUGE scores for individual sentences is easier than solv- ing the whole summarization task, it is suffi- cient to choose a basic learner with simple fea- tures and little in-domain training data (models are trained on one DUC dataset and evaluated on another). Specifically, we employ a support vector regression (SVR). 3 We use only classical surface-level features to represent sentences (po- sition, length, overlap with title) and combine them with frequency features. The latter include TF*IDF weighting of the terms (similar to Luhn (1958)), the sum of the frequency of the bi-grams in the sentence, as well as the sum of the document frequency (number of source documents in which the n-grams appear) of the terms and bi-grams in a sentence. We trained two models, R1 and R2 on DUC- 02 and DUC-03. For R1, the target score is the ROUGE-1 recall, while R2 learns ROUGE-2 re- call. Correlation Analysis We evaluated our sen- tence scoring models R1 and R2 by calculating the correlation of the scores produced by R1 and R2 and the true scores given in the DUC-03 data. We compare both models to the true ROUGE-1 and ROUGE-2 scores. In addition, we calculated the correlation of the TF*IDF and LexRank scores, in order to understand how well they would fit into our framework (TF*IDF and LexRank are de- scribed in section 4.3). The results are displayed in Table 2. Even with a basic learner it is possible to learn scores that correlate well with the true ROUGE-N scores, which supports the claim that it is easier to learn scores for individual sentences than to solve the whole problem of summarization. This finding strongly supports our proposed reduction of the extractive MDS problem to the task of learning </chunk></section><section><chunk>3 We use the implementation in scikit-learn (Pedregosa et al., 2011).  1830 with ROUGE-1 with ROUGE-2 Pearsons r Kendalls tau nDCG@15 Pearsons r Kendalls tau nDCG@15 TF*IDF 0.923 0.788 0.916 0.607 0.512 0.580 LexRank 0.210 0.120 0.534 0.286 0.178 0.379 model R1 0.940 0.813 0.951 0.653 0.545 0.693 model R2 0.729 0.496 0.891 0.743 0.576 0.752 Table 2: Correlation of different kinds of sentence scores and their true ROUGE-1 and ROUGE-2 scores. scores for individual sentences, which correlate well with their true ROUGE-N scores. We observe that TF*IDF correlates surprisingly well with the ROUGE-1 score, which indicates that we can expect a significant performance gain when feeding TF*IDF scores to our optimization framework. LexRank, on the other hand, orders sentences according to their centrality and does not look at individual sentences. Accordingly, we observe a low correlation with the true ROUGE- N scores, and thus LexRank may not benefit from the optimization (which we confirmed in our ex- periments). Finally, we observe that there is significant room for improvement regarding ROUGE-2, as well as for Kendalls tau in ROUGE-1 where a more sophisticated learner could produce scores that correlate better with the true scores. The higher the correlation of the sentence scores as- signed by a learner and the true scores, the better the summary produced by the subsequent subset selection. </chunk></section><section><heading>4.3 End-to-End Evaluation </heading><chunk>In our end-to-end evaluation on extractive MDS, we use the following baselines for comparison: TF*IDF weighting: This simple heuristic was introduced by Luhn (1958). Each sen- tence receives a score from the TF*IDF of its terms. We trained IDFs (Inverse Document Frequencies) on a background corpus 4 to im- prove the original algorithm. LexRank: Among other graph-based ap- proaches to summarization (Mani and Bloe- dorn, 1997; Radev et al., 2000; Mihalcea, 2004), LexRank (Erkan and Radev, 2004) has become the most popular one. A similar- ity graph G(V, E) is constructed where V is the set of sentences and an edge e ij is drawn between sentences v i and v j if and only if </chunk></section><section><heading>4 We used DBpedia long abstract: http://wiki.dbpedia.org/Downloads2015-04. </heading><chunk>the cosine similarity between them is above a given threshold. Sentences are scored ac- cording to their PageRank score in G. For our experiments, we use the implementation available in the sumy package. 5 ICSI: ICSI is a recent system that has been identified as one of the state-of-the-art sys- tems by Hong et al. (2014). It is a global linear optimization framework that extracts a summary by solving a maximum coverage problem considering the most important con- cepts in the source documents. Concepts are identified as bi-grams and their importance is estimated via their frequency in the source documents. Boudin et al. (2015) released a Python implementation (ICSI sume) that we use in our experiments. SFOUR: SFOUR is a structured prediction approach that trains an end-to-end system with a large-margin method to optimize a convex relaxation of ROUGE (Sipos et al., 2012). We use the publicly available imple- mentation. 6 As described in the previous section, two models are trained: R1 and R2. We evaluate both of them in the end-to-end setup with and without our op- timization. In the greedy version, sentences are added as long as the summary length is valid. We apply the optimization for sentence scor- ing models trained on ROUGE-1 and ROUGE-2 as well. The scoring models are trained on one dataset and evaluated on the other. For the ILP op- timization, the damping factor can vary and leads to different performance. We report the best re- sults among few variations. In order to speed-up the ILP step, we propose to limit the search space by only looking at the top K sentences 7 (hence 5 https://github.com/miso-belica/sumy 6 http://www.cs.cornell.edu/ rs/sfour/ 7 We used K=50 and observed that a range from K=25 to K=70 yields a good trade-off between computation cost and performance. 1831 the importance of learning a correct ordering as well, like Kendalls tau). This results in a mas- sive speed-up and can even lead to better results as it prunes parts of the noise. Finally, we perform significance testing with the t-test to compare dif- ferences between two means. 8 DUC-02 DUC-03 R1 R2 R1 R2 TFIDF 0.403 0.120 0.322 0.066 LexRank 0.446 0.158 0.354 0.077 ICSI 0.445 0.155 0.375 0.094 SFOUR 0.442 0.181 0.365 0.087 Greedy-R1 0.480 0.115 0.353 0.084 Greedy-R2 0.499 0.132 0.369 0.093 TFIDF+ILP 0.415 0.135 0.335 0.075 R1+ILP 0.509 0.187 0.378 0.101 R2+ILP 0.516 * 0.192 * 0.379 0.102 Table 3: Impact of the optimization step on sen- tence subset selection. Results Table 3 shows the results. The proposed optimization significantly and systematically im- proves TF*IDF performance as we expected from our analysis in the previous section. This re- sult suggests that using only a frequency signal in source documents is enough to get high scor- ing summaries, which supports the common belief that frequency is one of the most useful features for generic news summarization. It also aligns well with the strong performance of ICSI, which combines an ILP step with frequency information as well. The optimization also significantly and system- atically improves upon the greedy approach com- bined with our scoring models. Combining a SVR learner (SVR-1 and SVR-2) and our ILP-R pro- duces results on par with ICSI and sometimes sig- nificantly better. SFOUR maximizes ROUGE in an end-to-end fashion, but is outperformed by our framework when using the same training data. The framework is able to reach a competitive perfor- mance even with a basic learner. These results again suggest that investigating better learners for sentence scoring might be promising in order to improve the quality of the summaries. We observe that the model trained on ROUGE- 2 is performing better than the model trained on ROUGE-1, although learning the ROUGE-2 scores seems to be harder than learning ROUGE-1 8 The symbol * indicates that the difference compared to the previous best baseline is significant with p 0.05. scores (as shown in table 2). However, errors and approximations propagate less easily in ROUGE- 2, because the number of bi-grams in the intersec- tion of two given sentences is far less. Hence we conclude that learning ROUGE-2 scores should be put into the focus of future work on improving sentence scoring. </chunk></section><section><heading>5 Discussion </heading><chunk>This section discusses our contributions in a broader context. ROUGE Our subset selection framework per- forms the task of content selection, selecting an unordered set of textual units (sentences for now) for a system summary. The re-ordering of the sentences is left to a subsequent processing step, which accounts for aspects of discourse coherence and readability. While we justified our choice of ROUGE-1 re- call and ROUGE-2 recall as optimization objec- tives by their strong correlation with human evalu- ation methods, ROUGE-N has also various draw- backs. In particular, it does not take into account the overall discourse coherence of a system sum- mary (see the supplemental material for examples of summaries generated by our framework). From a broader perspective, systems that have high ROUGE scores can only be as good as ROUGE is, as a proxy for summary quality. However, as long as systems are evaluated with ROUGE, a natural approach is to develop systems that maximize it. Should novel automatic evaluation metrics be developed, our approach can still be applied, pro- vided that the new metrics can be expressed as a function of the scores of individual sentences. Structured Learning Compared to MDS ap- proaches using structured learning, our problem- reduction has the important advantage that it con- siderably scales-up the available training data by working on sentences instead of docu- ments/summaries pairs. Moreover, the task of sen- tence scoring is not dependent on arbitrary param- eters such as the summary length which are inher- ently abstracted from the summary worthiness of individual textual units. Error Propagation The first step of the frame- work is left to a ML component which can only produce approximate scores. Empirical results (in Figure 1 and Table 2) suggest that even with an 1832 imperfect first step, the subsequent optimization is able to produce high scoring summaries. How- ever, it might be insightful to study rigorously and in greater detail the propagation of errors induced by the first step. Other Metrics This work focused on maxi- mizing ROUGE-N recall because it is a widely acknowledged automatic evaluation metric. ROUGE-N relies on reference summaries which forces us to perform an estimation step. In our framework, we use ML to estimate the individual scores of sentences without using reference summaries. However, Louis and Nenkova (2013) proposed several alternative evaluation metrics for system summaries which do not need reference sum- maries. They are based on the properties of the system summary and the source documents alone, and correlate well with human evaluation. Some of them can even reach a correlation with human evaluation similar to the ROUGE-2 recall. An example of such a metric is the Jensen- Shannon Divergence (JSD) which is a symmet- ric smoothed version of the Kullback-Leibler di- vergence. Maximizing JSD can not be solved ex- actly with an ILP because it can not be factorized into individual sentences. However, applying an efficient greedy algorithm or maximizing a fac- torizable relaxation might produce strong results as well (for example, a simple greedy maximiza- tion of Kullback-Leibler divergence already yields good results (Haghighi and Vanderwende, 2009)). Future Work In this work, we developed a prin- cipled subset selection framework and empirically justified it. We focused on solving the second step of the framework while keeping the machine learning component as simple as possible. Essen- tially, our framework performs a modularization of the task of MDS, where all characteristics of the data and feature representations are pushed into a separate machine learning module they should not affect the subsequent optimization step which remains fixed. The promising results we obtained for sum- marization with a basic learner (see Section 4.3) encourage future work on plugging in more so- phisticated supervised learners in our framework. For example, we plan to incorporate lexical- semantic information in the feature representa- tion and leverage large-scale unsupervised pre- training. This direction is particularly promising because we have shown that we can expect sig- nificant performance gains for end-to-end MDS as the sentence scoring component improves. 6 Conclusion We proposed a problem-reduction approach to ex- tractive MDS, which performs a reduction to the problem of scoring individual sentences with their ROUGE scores based on supervised learning. We defined a principled discrete optimization prob- lem for sentence selection which relies on an ap- proximation of ROUGE. We empirically checked the validity of the approach on standard datasets and observed that even with a basic learner the framework produces promising results. The code for our optimizers is available at github.com/ UKPLab/acl2016-optimizing-rouge. Acknowledgments This work has been supported by the German Re- search Foundation as part of the Research Training Group Adaptive Preparation of Information from Heterogeneous Sources (AIPHES) under grant No. GRK 1994/1. References Chinatsu Aone, Mary Ellen Okurowski, James Gor- linsky, and Bjornar Larsen. 1995. A Trainable Summarizer with Knowledge Acquired from Robust NLP Techniques. In Inderjeet Mani and Mark T. Maybury, editors, Advances in Automatic Text Sum- marization, pages 6873. MIT Press, Cambridge, MA, USA. Florian Boudin, Hugo Mougard, and Benot Favre. 2015. Concept-based Summarization using Inte- ger Linear Programming: From Concept Pruning to Multiple Optimal Solutions. In Llus Mrquez, Chris Callison-Burch, Jian Su, Daniele Pighin, and Yuval Marton, editors, Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 19141918, Lisbon, Portugal. G  unes Erkan and Dragomir R. Radev. 2004. LexRank: Graph-based Lexical Centrality As Salience in Text Summarization. Journal of Artificial Intelligence Research, pages 457479. Dan Gillick and Benoit Favre. 2009. A Scalable Global Model for Summarization. In Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing, ILP 09, pages 10 18, Boulder, Colorado. 1833 Aria Haghighi and Lucy Vanderwende. 2009. Explor- ing Content Models for Multi-document Summa- rization. In Proceedings of Human Language Tech- nologies: The 2009 Annual Conference of the North American Chapter of the Association for Computa- tional Linguistics, pages 362370. Kai Hong and Ani Nenkova. 2014. Improving the Estimation of Word Importance for News Multi- Document Summarization. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL), pages 712721, Gothenburg, Sweden. Kai Hong, John Conroy, benoit Favre, Alex Kulesza, Hui Lin, and Ani Nenkova. 2014. A Reposi- tory of State of the Art and Competitive Baseline Summaries for Generic News Summarization. In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC14), pages 16081616, Reykjavik, Iceland. Mikael K  ageb  ack, Olof Mogren, Nina Tahmasebi, and Devdatt Dubhashi. 2014. Extractive Summariza- tion using Continuous Vector Space Models. In Pro- ceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC), pages 3139, Gothenburg, Sweden. Alex Kulesza and Ben Taskar. 2012. Determinantal Point Processes for Machine Learning. Foundations and Trends in Machine Learning, 5:123286. Julian Kupiec, Jan Pedersen, and Francine Chen. 1995. A Trainable Document Summarizer. In Proceed- ings of the 18th Annual International ACM SIGIR Conference on Research and Development in Infor- mation Retrieval, pages 6873, Seattle, Washington, USA. Association for Computing Machinery. Chen Li, Fei Liu, Fuliang Weng, and Yang Liu. 2013a. Document Summarization via Guided Sen- tence Compression. In Proceedings of the 2013 Conference on Empirical Methods in Natural Lan- guage Processing (EMNLP), pages 490500, Seat- tle, Washington, USA. Chen Li, Xian Qian, and Yang Liu. 2013b. Using Su- pervised Bigram-based ILP for Extractive Summa- rization. In Proceedings of the 51st Annual Meet- ing of the Association for Computational Linguistics (ACL), pages 10041013, Sofia, Bulgaria. Chen Li, Yang Liu, and Lin Zhao. 2015. Improving Update Summarization via Supervised ILP and Sen- tence Reranking. In Proceedings of the 2015 Con- ference of the North American Chapter of the Asso- ciation for Computational Linguistics: Human Lan- guage Technologies, pages 13171322, Denver, Col- orado. Hui Lin and Jeff A. Bilmes. 2011. A Class of Submod- ular Functions for Document Summarization. In Proceedings of the 49th Annual Meeting of the Asso- ciation for Computational Linguistics (ACL), pages 510520, Portland, Oregon. Chin-Yew Lin. 2004. ROUGE: A Package for Auto- matic Evaluation of summaries. In Proceedings of the Workshop on Text Summarization Branches Out at ACL, pages 7481, Barcelona, Spain. Annie Louis and Ani Nenkova. 2013. Automati- cally Assessing Machine Summary Content With- out a Gold Standard. Computational Linguistic, 39(2):267300, June. Hans Peter Luhn. 1958. The Automatic Creation of Literature Abstracts. IBM Journal of Research De- velopment, 2:159165. Inderjeet Mani and Eric Bloedorn. 1997. Multi- document Summarization by Graph Search and Matching. In Proceedings of the Fourteenth Na- tional Conference on Artificial Intelligence and Ninth Conference on Innovative Applications of Ar- tificial Intelligence, pages 622628, Providence, Rhode Island. AAAI Press. Ryan McDonald. 2007. A Study of Global Inference Algorithms in Multi-document Summarization. In Proceedings of the 29th European Conference on IR Research, pages 557564, Rome, Italy. Springer- Verlag. Rada Mihalcea. 2004. Graph-based Ranking Al- gorithms for Sentence Extraction, Applied to Text Summarization. In Proceedings of the ACL 2004 on Interactive Poster and Demonstration Sessions, ACLdemo 04, page 20, Barcelona, Spain. Olof Mogren, Mikael K  ageb  ack, and Devdatt Dub- hashi. 2015. Extractive Summarization by Aggre- gating Multiple Similarities. In Recent Advances in Natural Language Processing, pages 451457, Hissar, Bulgaria. George L. Nemhauser and Laurence A. Wolsey. 1978. Best Algorithms for Approximating the Maximum of a Submodular Set Function. Mathematics of Op- erations Research, 3(3):177188. George L. Nemhauser, Laurence A. Wolsey, and Marschall L. Fisher. 1978. An Analysis of Ap- proximations for Maximizing Submodular Set Func- tionsI. Mathematical Programming, 14:265294. Jun-Ping Ng, Praveen Bysani, Ziheng Lin, Min-Yen Kan, and Chew-Lim Tan. 2012. Exploiting Category-Specific Information for Multi-Document Summarization. In Proceedings of the 24th Inter- national Conference on Computational Linguistics (COLING 2012), pages 20932108, Mumbai, India. Hitoshi Nishikawa, Kazuho Arita, Katsumi Tanaka, Tsutomu Hirao, Toshiro Makino, and Yoshihiro Matsuo. 2014. Learning to Generate Coherent Summary with Discriminative Hidden Semi-Markov Model. In Proceedings of COLING 2014, the 25th International Conference on Computational Lin- guistics: Technical Papers, pages 16481659. 1834 Paul Over, Hoa Dang, and Donna Harman. 2007. DUC in Context. Information Processing and Manage- ment, 43(6):15061520. Karolina Owczarzak, John M. Conroy, Hoa Trang Dang, and Ani Nenkova. 2012. An Assessment of the Accuracy of Automatic Evaluation in Summa- rization. In Proceedings of Workshop on Evaluation Metrics and System Comparison for Automatic Sum- marization, pages 19, Montreal, Canada. Fabian Pedregosa, Gael Varoquaux, Alexandre Gram- fort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexan- dre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and Edouard Duchesnay. 2011. Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research, 12:28252830. Dragomir R. Radev, Hongyan Jing, and Malgorzata Budzikowska. 2000. Centroid-based Summariza- tion of Multiple Documents: Sentence Extraction, Utility-based Evaluation, and User Studies. In Pro- ceedings of the NAACL-ANLP Workshop on Auto- matic Summarization, volume 4, pages 2130, Seat- tle, Washington. Ruben Sipos, Pannaga Shivaswamy, and Thorsten Joachims. 2012. Large-margin Learning of Sub- modular Summarization Models. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 224233, Avignon, France. Hiroya Takamura and Manabu Okumura. 2010. Learn- ing to Generate Summary as Structured Output. In Proceedings of the 19th ACM international Confer- ence on Information and Knowledge Management, pages 14371440. Association for Computing Ma- chinery. Kristian Woodsend and Mirella Lapata. 2012. Multi- ple Aspect Summarization Using Integer Linear Pro- gramming. In Proceedings of the 2012 Joint Con- ference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, (EMNLP-CoNLL), pages 233243, Jeju Island, Korea. Wen-tau Yih, Joshua Goodman, Lucy Vanderwende, and Hisami Suzuki. 2007. Multi-document Summa- rization by Maximizing Informative Content-words. In Proceedings of the 20th International Joint Con- ference on Artifical Intelligence, pages 17761782, Hyderabad, India. Morgan Kaufmann Publishers Inc. Wenpeng Yin and Yulong Pei. 2015. Optimizing Sen- tence Modeling and Selection for Document Sum- marization. In Proceedings of the 24th International Conference on Artificial Intelligence, pages 1383 1389, Buenos Aires, Argentina. AAAI Press. </chunk></section><section><heading>A Supplemental Material </heading><chunk>A.1 Recursive Expression of ROUGE-N Let S = {s i |i m} and T = {t i |i l} be two sets of sentences, S the reference summary, and (X) denote the ROUGE-N score of the set of sentences X. Assuming that (S) and (T ) are given, we prove the following recursive formula: (S T ) = (S) + (T ) (S T ) (12) For compactness, we use the following notation as well: C X,S (g) = min(F X (g), F S (g)) (13) Proof: We have the following definitions: (S) = 1 R N gS F S,S (g) (14) (T ) = 1 R N gS F T,S (g) (15) (S T ) = 1 R N gS max(C S,S (g)+C T,S (g)F S (g), 0) (16) And by definition of ROUGE, the formula of S T : (S T ) = 1 R N gS min(F ST (g), F S (g)) (17) In order to prove equation (12), we have to show that the following equation holds: gS C S,S (g) + gS C T,S (g) gS max(C S,S (g) + C T,S (g) F S (g), 0) = gS min(F ST (g), F S (g)) (18) It is sufficient to show: g S , C S,S (g) + C T,S (g) max(C S,S (g) + C T,S (g) F S (g), 0) = min(F ST (g), F S (g)) (19) Let g S be a n-gram. There are two possi- bilities: 1835 F S (g) + F T (g) F S (g): g appears less times in S T than in the reference sum- mary. It implies: min(F ST (g), F S (g)) = F ST (g) = F S (g) + F T (g). Moreover, all F X (g) are positive numbers by defini- tion, and F S (g) F S (g) is equivalent to: C S,S (g) = min(F S (g), F S (g)) = F S (g). Similarly, we have: C T,S (g) = min(F T (g), F S (g)) = F T (g). Since max(C S,S (g) + C T,S (g) F S (g), 0) = 0, the equation (19) holds in this case. F S (g) + F T (g) F S (g): g appears more frequently in S T than in the reference sum- mary. It implies: min(F ST (g), F S (g)) = F S (g). Here we have: max(C S,S (g) + C T,S (g) F S (g), 0) = C S,S (g) + C T,S (g) F S (g), and it directly follows that equation (19) holds in this case as well. Equation (19) has been proved, which proves (12) as well. A.2 Expanded Expression of ROUGE-N Let S = {s i |i m} be a set of sentences and (S) its ROUGE-N score. We prove the following formula: (S) = m i=1 (s i )+ m k=2 (1) k+1 ( 1i 1 i k m (k) (s i 1 s i k )) (20) Proof: Let g S be a n-gram in the ref- erence summary, and k [1, m] the number of sentences in which it appears. Specifically, {s i 1 , , s i k }, s i j {s i 1 , . . . , s i k }, g s i j . In order to prove the formula (20), we have to find an expression for the (k) that gives to g the correct contribution to the formula: 1 R N min(F S (g), F S (g)) (21) First, we observe that g does not appear in the terms that contain the intersection of more than k sentences. Specifically, (t) is not affected by g if t k. However, g is affected by all the (t) for which t k. Given that g appears in the sentences {s i 1 , . . . , s i j }, we can determine the score attributed to g by the previous (t) (t k): S (k1) (g) = s{s i 1 ,...,s i k } (s)+ k l=2 (1) (l+1) 1i 1 i l k (l) (s i 1 s i l )) (22) Now, g receives the correct contribution to the overall scores if (k) is defined as follows: (k) (s i 1 s i j ) = 1 R gs i 1 s i j min(C {s i 1 ,...,s i k } (g), F S (g)) S (k1) (g) (23) Indeed, with this expression for (k) , the score of g is: S (k1) (g) + 1 R N min(C {s i 1 ,...,s i k } (g), F S (g)) S (k1) (g) (24) Which can be simplified to: 1 R N min(C {s i 1 ,...,s i k } (g), F S (g)) (25) Since g appears only in the sentences {s i 1 , . . . , s i k }, F {s i 1 ,...,s i k } (g) = F S (g) and it follows that: 1 R N min(C {s i 1 ,...,s i k } (g), F S (g)) = 1 R N min(F S (g), F S (g)) (26) This proves equation (20) because we observe that g will not be affected by any other terms. Ev- ery (t) for t k including g is counted by S (k1) , and no other terms from (k) will affect g because all the other terms (k) should contain at least one sentence that is not in {s i 1 , . . . , s i k } and g would not belong to this intersection by definition. Finally, it has been proved in the appendix A.1 that for k = 2, (2) has a reduced form: (2) (s a s b ) = 1 R N gS max(C sa,S (g)+C s b ,S (g)F S (g), 0) (27) In the paper, we ignore the terms for k 2, there- fore we do not search for a reduced form for these terms. 1836 </chunk></section></sec_map>