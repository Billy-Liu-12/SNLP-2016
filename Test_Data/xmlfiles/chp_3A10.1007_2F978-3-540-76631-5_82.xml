<sec_map><section><chunk>Textual Energy of Associative Memories: Performant Applications of Enertex Algorithm in Text Summarization and Topic Segmentation Silvia Fernandez 1,2 , Eric SanJuan 1 , and Juan Manuel Torres-Moreno 1,3,, 1 Laboratoire Informatique dAvignon, BP 1228 F-84911 Avignon Cedex 9 France 2 Laboratoire de Physique des Materiaux, CNRS UMR 7556, Nancy, France 3 Ecole Polytechnique de Montreal - Departement de genie informatique CP 6079 Succ. Centre Ville H3C 3A7, Montreal (Quebec), Canada {silvia.fernandez,eric.sanjuan,juan-manuel.torres}@univ-avignon.fr http://www.lia.univ-avignon.fr Abstract. In this paper we present a Neural Network approach, in- spired by statistical physics of magnetic systems, to study fundamental problems of Natural Language Processing (NLP). The algorithm mod- els documents as neural network whose Textual Energy is studied. We obtained good results on the application of this method to automatic summarization and Topic Segmentation. Keywords: Automatic Summarization, Topic Segmentation, Statistical Methods, Statistical Physics. 1 Introduction Hopfield [1,2] took as a starting point physical systems like the magnetic Ising model (formalism resulting from statistical physics describing a system com- posed of units with two possible states named spins) to build a Neural Network (NN) with abilities of learning and retrieving of patterns. The capacities and limitations of this Network, called associative memory, were well established in a theoretical frame in several studies [1,2]: the patterns must be not correlated to obtain free error retrieving, the system saturates quickly and only a little fraction of the patterns can be stored correctly. As soon as their number ex- ceeds 0, 14N , any pattern is recognized. This situation strongly restricts the practical applications of Hopfield Network. However, in NLP, we think that it is possible to exploit this behavior. Vector Space Model (VSM) [3] represents the sentences of a document into vectors. These vectors can be studied as Hopfield NN. With a vocabulary of N terms, it is possible to represent a sentence as a chain of N active neurons (words are present) or inactive neurons (words are absent). A document with P sentences is formed of P chains in the vector space of dimension N . These vectors are correlated according to the shared words. If thematics are close, it is raisonable to suppose that the degree of correlation will Corresponding author. A. Gelbukh and A.F. Kuri Morales (Eds.): MICAI 2007, LNAI 4827, pp. 861871, 2007. c Springer-Verlag Berlin Heidelberg 2007 862 S. Fernandez, E. SanJuan, and J.M. Torres-Moreno be very high. That is a problem if we want to store and retrieve these represen- tations from a Hopfield NN. However, our interest does not relate in retrieving, but on studying the interactions between the terms and the sentences. From these interactions we have defined the Textual Energy of a document. It can be useful, for example, to score or to detect changes between sentences. We have developed a model which makes possible to use the concept of Textual Energy in automatic summarization or topic segmentation tasks. We present in Section 2 a short introduction to the model of Hopfield. In Section 3, we show an ex- tension of this approach in Natural Language Processing. We use elementary notions of graph theory to give an interpretation of Textual Energy like a new measure of similarity. In Section 4 we apply our algorithms to the generation of automatic summaries and the detection of topic boundaries, before concluding and presenting some prospects. </chunk></section><section><heading>2 The Model of Hopfield </heading><chunk>Certainly the most important contribution of Hopfield to the theory of NN was the introduction of the notion of energy that comes from the analogy with the magnetic systems. A magnetic system is constituted of a set of N small magnets called spins. These spins can turn according to several directions. The simplest case is represented by the Ising model which considers only two possible direc- tions: up (, +1 or 1) or down (, -1 or 0). The Ising model is used in several systems which can be described by binary variables [4]. A system of N binary units has = 1, ..., 2 N possible configurations (patterns). In the Hopfield model the spins correspond to the neurons, interacting with the Hebb learning rule 1 : J i,j = P =1 s i s j (1) s i et s j are the states of neurons i and j. Autocorrelations are not calculated (i = j). The summation concerns the P patterns to store. This rule of interaction is local, because J i,j depends only on the states of the connected units. This model is also known as associative memory. It has the capacity to store and to retrieve certain number of configurations of the system, because the Hebb rule transforms these configurations into attractors (minimal local) of the energy function [1]: E = 1 2 N i=1 N j=1 s i J i,j s j (2) Clearly the energy is a function of the system configuration, that is, of the state (of activation or non-activation) of all these units. If we present a pattern , every spin will undergo a local field h i = N j=1 J i,j s j induced by the others N spins (figure 1). Spins will align themselves according to h i in order to restore 1 The connections are proportionals to the correlation between neurons states [2]. Textual Energy of Associative Memories 863 Fig. 1. Field hi created by the units of the pattern affects the pattern the stored pattern that is the nearest one to the presented pattern . We will not detail the pattern retrieving method 2 , because our interest will concern the dis- tribution and the properties of the energy of the system (2). This monotonic and decreasing function had only been used to show that the retrieving is convergent. VSM [3] transforms documents in an adequate space where a matrix S contains the information of the text in the form of bags of words. We can consider S as the configuration set of a system which we can calculate its energy. </chunk></section><section><heading>3 Applications in NLP </heading><chunk>Documents are pre-treated with classical algorithms of functional words filter- ing 3 , normalization and lemmatisation [6,7] to reduce the dimensionality. A bag of words representation produces a matrix S [P N ] of frequencies consisting in = 1, , P sentences (lines); = {s 1 , , s i , , s N } and a vocabulary of i = 1, , N terms (columns). S = s 1 1 s 2 1 s N 1 s 1 2 s 2 2 s N 2 . . . . . . . . . . . . s 1 P s 2 P s N P ; s i = T F i if word i exists 0 elsewhere (3) Because the presence of the word i represents a spin s i with a magnitude given by its frequency T F i (its absence by respectively), a sentence is therefore a chain of N spins. We differ from [1] on two points: S is a whole matrix (its elements take absolute frequential values) and we use the elements J i,i because this autocorrelation makes possible to establish the interaction of the word i among the P sentences, which is important in NLP. We apply Hebbs rule (in matricial form) to calculate the interactions between N terms of the vocabulary: J = S T S (4) Each element J i,j J [N N ] is equivalent to the calculation of (1). The Textual Energy of interaction between patterns (figure 1) (2) can be expressed: E = 1 2 S J S T ; E , E [P P ] (5) 2 However the interested reader can consult, for example, [1,5,2]. 3 Filtering of numbers and stop-words. 864 S. Fernandez, E. SanJuan, and J.M. Torres-Moreno E , represents the energy of interaction between patterns and . </chunk></section><section><heading>3.1 Textual Energy: A New Similarity Measure </heading><chunk>At this level we are going to explain theoretically the nature of the links between sentences that Textual Energy infers. To do that, we use some elementary notions of the graph theory. The interpretation that we are going to do, is based on the fact that the matrix (5) can be rewritten: E = 1 2 S (S T S) S T = 1 2 (S S T ) 2 (6) Let us consider the sentences as sets of words. These sets constitute the vertices of the graph. We draw an edge between two of these vertices , every time they share at least a word in common = . We obtain the intersection graph I(S) of the sentences (see an example of four sentences in figure 2). We evaluate these pairs { 1 , 2 }, which we call edges, by the exact number | 1 2 | of words that share the two connected vertices. Finally, we add to each vertex an edge of reflexivity {} valued by the cardinal || of . This valued intersection graph is isomorphic to the adjacency graph G(S S T ) of the square matrix S S T . In fact, G(S S T ) contains P vertices. There is an edge between two vertices , if and only if [S S T ] , &gt; 0. If it is the case, this edge is valued by [S S T ] , and this value corresponds to the number of words in common between the sentences and . Each vertex is balanced by [S S T ] , , which corresponds to the addition of an edge of reflexivity. It results that the matrix of Textual Energy E is the adjacency matrix of the graph G(S S T ) 2 in which: the vertices are the same ones that those of the intersection graph I(S); there is an edge between two vertices each time that there is a way of length 2 in the intersection graph; the value of an edge ( , ): a) where = (loop) is the sum of the squares of the values of adjacent edges, and b) the sum of the products of the values of the edges on any path of length 2 between and otherwise. These paths can include loops. From this representation we deduce that the matrix of Textual Energy con- nects at the same time sentences having common words because it includes the Fig. 2. Adjacency graphs from the matrix of energy Textual Energy of Associative Memories 865 intersection graph, as well as sentences which share the same neighbourhood without necessarily sharing the same vocabulary. So, two sentences 1 , 3 not sharing any word in common but for which there is at least one third sentence 2 such that 1 2 = and 3 2 = , will be connected all the same. 4 Experiments and Results Textual Energy can be used as a similarity measure in NLP applications. In an intuitive way, this similarity can be used in order to score the sentences of a document and thus separate those which are relevant from those which are not. This leads immediately to a strategy for automatic summarization by extraction of sentences. Another approach, less evident, consists in using the information of this energy (seen as a spectrum or numerical signal of the sentence) and to compare with the spectrum of all the others. A statistical test can then indicate if this signal is similar to the signal of other sentences grouped together in segments or not. This can be seen as a detection of thematic boundaries in a document. </chunk></section><section><heading>4.1 Mono-Document Generic Summarization </heading><chunk>Under the hypothesis that the energy of a sentence reflects its weight in the document, we applied (6) to summarization by extraction of sentences [8,9]. The summarization algorithm includes three modules. The first one makes the vec- torial transformation of the text using filtering, lemmatisation/stemming and standardization processes. The second module applies the spin model and com- pute the matrix of textual energy (6). We obtain the weighting of a sentence by using its absolute energy values, by sorting according to |E , |. So, the relevant sentences will be selected as having the biggest absolute energy. Finally, the third module generates summaries by displaying and concatenating the relevant sentences. The two first modules are based on the Cortex system 4 . French texts 5 choosed are: 3-melanges made up of three topics, Puces of two topics and Jaccuse (Emile Zolas letter). Three texts of the Wikipedia in En- glish were analysed, Lewinksky, Quebec and Nazca Lines 6 . We evaluated the summaries produced by our system with Rouge 1.5.5 [11], which measures the similarity, according to several strategies, between a candidate summary (pro- duced automatically) and summaries of reference (created by humans). In table 1 we compare the performances of the energy method against Mead system 7 that produces only English summaries (symbols in table), Copernic Summarizer 8 , Cortex and a Baseline where the sentences were randomly selected. The com- pression rate was variable (following the size of the texts) and computed as a 4 The Cortex system [10] is an unsupervised summarizer of relevant sentences using several metrics controlled by an algorithm of decision. 5 http://www.lia.univ-avignon.fr/chercheurs/torres/recherche/cortex 6 http://en.wikipedia.org/wiki/Quebec_sovereignty_movement; Monica_Lewinsky; Nazca_lines 7 http://tangra.si.umich.edu/clair/md/demo.cgi 8 http://www.copernic.com 866 S. Fernandez, E. SanJuan, and J.M. Torres-Moreno Table 1. Rouge-2 (R2) and SU4 score recall. 25%: 3-melanges (8 ref), Puces (8 ref), Quebec (8 ref) and Nazca (6 ref) ; 12%: Jaccuse (6 ref); 20%: Lewinsky (7 ref). Corpus Mead Copernic Enertex Cortex Baseline R2 SU4 R2 SU4 R2 SU4 R2 SU4 R2 SU4 3-melanges 0.4231 0.4348 0.4958 0.5064 0.4968 0.5064 0.3074 0.3294 Puces 0.5775 0.5896 0.5204 0.5336 0.5360 0.5588 0.3053 0.3272 Jaccuse 0.2235 0.2707 0.6146 0.6419 0.6316 0.6599 0.2177 0.2615 Lewinsky 0.4756 0.4744 0.5580 0.5610 0.5611 0.5786 0.6183 0.6271 0.2767 0.2925 Quebec 0.4820 0.3891 0.4492 0.4859 0.5095 0.5377 0.5636 0.5872 0.2999 0.3524 Nazca 0.4446 0.4671 0.4270 0.4495 0.6158 0.6257 0.5894 0.5966 0.3041 0.3288 rate on the number of sentences in text. The best performances are in bold and those in 2d position are in italic (all scores). Enertex is a powerful summarizing system (it obtains 3 firsts places and 7 second), close to Cortex system. </chunk></section><section><heading>4.2 Query Oriented Multi-document Summarization </heading><chunk>The main task of the NIST-Document Understanding Conference DUC07 9 is given 45 topics and their 25 document clusters, to generate 250-word fluent summaries that answer the question(s) in the topics statements. In order to calculate the similarity between every topic and the sentences contained in the corresponding cluster we have used Textual Energy (2). Consequently the sum- mary is formed with the sentences that present the maximum interaction en- ergy with the topic. We describe now the process of summary construction using the matricial forms of J and E (4 and 5). First, the 25 documents of a cluster are concatenated into a single document following chronological or- der. Then the Textual Energy between the topic, view as a supplementary sentence, and each of the other sentences in the document is computed us- ing (5). We construct the summary by sorting the most relevant values in the row of matrix E which correspond to interaction energy of the topic vs. the document. Redundancy removal. In general, in multi-document summarization there is a significant probability of including duplicated information. To avoid this problem, a redundancy elimination strategy has to be implemented. Our system does not include any linguistic processing, then our non-redundancy strategy consists in comparing the energy values of sentences in the generated summary. We suppose that (in a long corpora) the probability that two sentences have the same values of energy is very small. Then we detected the duplicated sentences (with exactly the same energy value) and we replace them by the following ones in the score table. Another strategy, enabling to diversify the content, is to omit long sentences. The threshold with which we obtained the best results was two times the average of the number of words per sentence. Figure 3 shows the 9 http://www-nlpir.nist.gov/projects/duc Textual Energy of Associative Memories 867 15 29 4 24 13 20 23 7 3 30 ENERTEX 8 9 22 14 17 28 32 NIST 2 18 31 26 21 5 11 12 19 25 10 6 27 NIST 1 16 0,03 0,04 0,05 0,06 0,07 0,08 0,09 0,10 0,11 0,12 0,13 DUC 2007 ROUGE-2 Score System Id 15 24 29 4 13 8 23 ENERTEX 3 7 9 20 30 17 22 14 28 32 18 31 NIST 2 26 21 19 12 5 10 25 11 6 27 NIST 1 16 0,07 0,08 0,09 0,10 0,11 0,12 0,13 0,14 0,15 0,16 0,17 0,18 DUC 2007 SU4 Score System Id Fig. 3. Recall Rouge-2 and SU4 of the 30 participants in DUC07 and two baselines position of our system in the Rouge automatic evaluation comparing to the 30 participants and two baselines IDs 1 (random) and 2 (generic summarization system). </chunk></section><section><heading>4.3 Topic Segmentation </heading><chunk>Several strategies have been developed to segment a text thematically. Most of them are based on Markov models [12], classification of the terms [13,14], lex- ical chains [15] or on PLSA model [16], that estimates the probabilities of the terms to belong to latent semantic classes. In an original way, we have used the matrix of energy E (6). This choice makes possible to adapt to new topics and to remain independent from document language. We show in figure 4 the energy of interaction between some sentences of a text made up of two top- ics. Given that (6) is capable of detecting and of balancing the neighbourhood of a sentence, we can notice a similarity between the curves of the one (bold line) and the other topics (dotted line). In order to compare energies between themselves we have used Kendalls coefficient of correlation. Given two sen- tences and , we estimate the probability P [ = ] of being in distinct topics by the probability of [ (x, y) &gt; (E ,. , E ,. )]. This is done using the normal approximation of Kendalls law valid if vectors E ,. , E ,. have more than 30 terms. coefficient does not depend on exact energy values, only on their rank in the vectors E ,. , E ,. . Basically, it evaluates the degree of concordance be- tween two rankings and makes possible robust non parametric statistical test of agreement between two judges classifying a set of P objects using the fact that P [ (x, y) &gt; (E ,. , E ,. )] = 1 if the ranking vectors associated with E ,. and E ,. are two statistically independent variables. Here the judges are two sentences that classify all other sentences based on the interaction energy. We shall say that it is almost sure that two sentences and are in the same topic if P [ = ] &gt; 0.05 We have used this test to find the thematic borders between segments. As illustrated in figure 5, a sentence is considered to be at the border of one segment if it is almost sure that: 1/ it is in the same topic as at least two over the three previous sentences; and 2/ it is not in the same topic as at least 868 S. Fernandez, E. SanJuan, and J.M. Torres-Moreno 24 23 22 19 18 16 15 14 13 12 10 9 7 5 4 3 2 1 Fig. 4. Textual Energy of 2-melanges. In continuous line, the energy of the sentences of 1 th topic, in dotted line that of 2 th . The change of shape of the curves between sentences 14-15 corresponds to a topic boundary. The horizontal axis indicates the number of sentence in the order of the document. The vertical axis, the Textual Energy of the showed sentence vs. others. 13 14 15 16 17 i-3 i-2 i-1 i i+1 i+2 i+3 p i-3 =0,1 p i-2 =0,4 p i-1 =0,002 p i+1 =0,001 p i+2 =0,004 p i+3 =0,003 12 18 Fig. 5. Kendalls in window. p ik = probability of concordance between i k and i. two over the three following sentences. We have implemented this approach of topic segmentation as a slippery window of seven sentences. As the window is moving on, the sencence on its center is compared to all other sentences in the window based on Kendalls coefficient. If a border is found then the window jumps over the next three sentences. Our programs have been optimized for stan- dard PERL 5 libraries. Figures 6 and 7 show the detection of the boundaries for the texts with 2 and 3 topics. The true boundaries are indicated in dotted line. For the text 3-melanges, the test found two borders between the segments 8-9 and 16-18. In both cases, that corresponds indeed to the thematic boundaries. The third (false) boundary was indicated between sentences 14-15 of the text 2-melanges. It deserves to be commented on. If we look at figure 4 we can notice that energy of the sentence 23 is very different from that of the sentences 22 or 24. Sentence 23 presents a curve overlapping the two topics. It is the reason why the test cannot identify it like pertaining to the same class. This reasoning Textual Energy of Associative Memories 869 01-02 02-03 03-04 04-05 05-06 06-07 07-09 09-10 10-11 11-12 12-13 13-14 14-15 15-16 16-18 18-19 19-22 22-23 23-24 0,00 0,05 0,10 0,15 0,20 0,25 0,30 0,35 0,40 0,45 0,50 p (Test de Kendall) Phrase (i, j) 00-1 01-2 02-3 03-4 04-5 05-6 06-7 07-8 08-9 09-10 10-11 11-13 13-14 14-15 15-16 16-18 18-19 19-20 20-23 23-25 25-26 0,00 0,05 0,10 0,15 0,20 0,25 0,30 0,35 0,40 0,45 0,50 p (Test de Kendall) Phrase (i, j) Fig. 6. Topic segmentation for the text 2-melanges (2 topics, on the left) and 3- melanges (3 topics, on the right) can be extended to all other false borders. We show in figure 7 the boundary detection for texts with 3 and 4 thematics. For the text physique-climat-chanel we have detected three boundaries between the sentences 5-6 and 12-15, which corresponds to the real boundaries. For the text in English with two topics the test found one boundary between the segments 44-45 which also corresponds to the real one. In another experiment, we have compared our system to two oth- 00-02 02-05 05-06 06-07 07-08 08-10 10-11 11-12 12-15 15-16 16-17 17-18 18-19 19-22 0,00 0,05 0,10 0,15 0,20 0,25 0,30 0,35 0,40 0,45 0,50 p (Test de Kendall) Phrase (i, j) 01-2 03-4 05-6 08-9 11-12 13-14 15-16 17-18 20-21 23-24 25-26 27-29 30-31 32-33 34-35 36-37 38-39 40-41 42-44 45-46 47-48 51-52 53-54 55-56 57-58 59-60 61-62 63-64 65-66 67-68 69-70 71-72 73-74 0,00 0,05 0,10 0,15 0,20 0,25 0,30 0,35 0,40 0,45 0,50 p (Test de Kendall) Phrase (i, j) Fig. 7. Topic segmentation for the text in French with 3 topics physique-climat-chanel on the left and in English Quebec-Lewinsky on the right ers: LCseg [17] and LIA_seg [15] that are based on lexical chains. The corpus of reference was built by [15] from articles of the newspaper Le Monde. It is composed of sets of 100 documents where each one corresponds to the average size of the predefined segments. A document is composed of 10 segments (9 bor- ders) extracted from articles of different topics selected at random. The scores are calculated with [18], used in the topic segmentation. This function measures the difference between the real boundaries and those found automatically in a slippery window: the smaller the value is, the more the system is performant. 870 S. Fernandez, E. SanJuan, and J.M. Torres-Moreno LIA_seg depends on a parameter which gives place to various performances (that is why the evaluation of this system gives rise to a range of values). Our method, that uses much less parameters as we do not make any assumption on the number of topics to detect, obtains close performances to the systems in the state of the art. In table 2 we show these results as well as the average number of borders found by Enertex. Table 2. Windiff for LCseg, LIA_seg and Enertex (variable size segments) Segment size (sentences) LCseg LIA_seg Enertex (Found boundaries) 9-11 0.3272 (0.3187-0.4635) 0.4134 7.10/9 3-11 0.3837 (0.3685-0.5105) 0.4264 7.15/9 3-5 0.4344 (0.4204-0.5856) 0.4140 5.08/9 5 Conclusion and Perspectives We have introduced the concept of Textual Energy based on approaches of NN that have enabled us to develop a new algorithm of automatic summarization. Several experiments have shown that our algorithm is adapted to extract rel- evant sentences. The majority of the topics are approached in the final digest. The summaries are obtained independently of the text size, topics and languages (except for the preprocessing part), and a few quantity of noise is tolerated. Query-guided summaries has been obtained by introducing the topic as supple- mentary sentence. Some concluding tests on the DUC07 corpora were realized. We also have studied the problem of topic segmentation of the documents. The method, based on the energy matrix of the system of spins, is coupled with a robust statistical non-parametric test based on Kendalls . The results are very encouraging. A criticism of this algorithm could be that it requires the han- dling (produced, transposed) of a matrix of size [P P ]. However the graph representation performs these calculations in time P log(P ) and in space P 2 . </chunk></section><section><heading>References </heading><chunk>1. Hopfield, J.: Neural networks and physical systems with emergent collective com- putational abilities. National Academy of Sciences 9, 25542558 (1982) 2. Hertz, J., Krogh, A., Palmer, G.: Introduction to the theorie of Neural Computa- tion. Addison Wesley, Redwood City, CA (1991) 3. Salton, G., McGill, M.: Introduction to modern information retrieval. Computer Science Series. McGraw-Hill, New York (1983) 4. Ma, S.: Statistical Mechanics. World Scientific, Philadelphia, CA (1985) 5. Kosko, B.: Bidirectional associative memories. IEEE Transactions Systems Man, Cybernetics 18, 4960 (1988) 6. Porter, M.: An algorithm for suffix stripping. Program 14, 130137 (1980) 7. Manning, C.D., Schutze, H.: Foundations of Statistical Natural Language Process- ing. The MIT Press, Cambridge (2000) Textual Energy of Associative Memories 871 8. Mani, I., Maybury, M.T.: Automatic Text Summarization. MIT Press, Cambridge (1999) 9. Radev, D., Winkel, A., Topper, M.: Multi Document Centroid-based Text Sum- marization. In: ACL 2002 (2002) 10. Torres-Moreno, J.M., Velazquez-Morales, P., Meunier, J.: Condenses de textes par des methodes numeriques. In: JADT. vol. 2, pp. 723734 (2002) 11. Lin, C.Y.: Rouge: A package for automatic evaluation of summaries. In: WAS 2004 (2004) 12. Amini, M.R., Zaragoza, H., Gallinari, P.: Learning for sequence extraction tasks. In: RIAO 2000 pp. 476489 (2000) 13. Caillet, M., Pessiot, J.F., Amini, M., Gallinari, P.: Unsupervised learning with term clustering for thematic segmentation of texts. In: RIAO 2004, pp. 648657 (2004) 14. Chuang, S.L., Chien, L.F.: A practical web-based approach to generating Topic hierarchy for Text segments. In: ACM IKM, Washington, pp. 127136 (2004) 15. Sitbon, L., Bellot, P.: Segmentation thematique par chaines lexicales ponderees. In: TALN 2005, vol. 1 pp. 505510 (2005) 16. Brants, T., Chen, F., Tsochantaridis, I.: Topic-based document segmentation with probabilistic latent semantic anaysis. In: CIKM 2002, Virginia, USA, pp. 211218 (2002) 17. Galley, M., McKeown, K.R., Fosler-Lussier, E., Jing, H.: Discourse segmentation of multi-party conversation. In: ACL-2003, Sapporo, Japan, pp. 562569 (2003) 18. Pevzner, L., Hearst, M.: A critique and improvement of an evaluation metric for text segmentation. In: Computational Linguistic. vol. 1, pp. 1936 (2002) </chunk></section></sec_map>