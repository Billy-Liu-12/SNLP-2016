<sec_map><section><chunk>Automated Lossless Hyper-Minimization for Morphological Analyzers Senka Drobac and Miikka Silfverberg and Krister Lind  en Department of Modern Languages PO Box 24 00014 University of Helsinki {senka.drobac, miikka.silfverberg, krister.linden}@helsinki.fi Abstract This paper presents a fully automated loss- less hyper-minimization method for finite- state morphological analyzers in Xerox lexc formalism. The method utilizes flag diacritics to preserve the structure of the original lexc description in the finite-state analyzer, which results in re- duced size of the analyzer. We compare our method against an earlier solution by Drobac et al. (2014) which requires man- ual selection of flag diacritics and results in slow lookup. We show that our method gives similar size reductions while main- taining fast lookup without requiring any manual input. </chunk></section><section><heading>1 Introduction </heading><chunk>Morphological analyzers are commonly imple- mented as finite state machines (FSM) because finite-state technology enables both fast process- ing of large amounts of input and manipulation of the analyzer using finite-state algebra. Sometimes finite-state analyzers may, however, become quite large. This can be a problem e.g. when analyz- ers are used on mobile devices where a moderate memory footprint is required. The usual way to reduce the size of FSMs is to use a minimization algorithm (Hopcroft, 1971). Minimization can have a substantial effect on the size of the FSM but, as it is only able to combine suffix-equivalent states, there may still be residual redundancy in the state space of the machine. Further size reduction can be accomplished by introducing a limited form of context-free struc- ture into the finite-state graph using special sym- bols called flag diacritics (Beesley, 1998). Using flag diacritics, it is possible to combine sub-graphs which are equivalent, i.e. accept the same strings, but which are not necessarily suffix-equivalent. Flag diacritics are used to couple entrance points of the sub-graphs with appropriate exit points. During lookup, paths whose flag diacritics do not match are filtered out. Thus, the original language of the machine is preserved. Traditionally, the lexicon writer manually in- serts flag diacritics into the lexicon of the mor- phological analyzer. There are two major prob- lems with this approach: (1) In practice, manu- ally inserted flag diacritics often do not result in great size reduction because many lexicon writ- ers have poor understanding of the structure of the finite-state networks built from lexicographical- morphological descriptions; (2) The addition of flag diacritics to these descriptions makes them unreadable and unmanageable since the amount of non-linguistic data in the linguistic description in- creases. This paper introduces an automated method for inducing flag diacritics into finite-state morpho- logical analyzers based on the Xerox lexc for- malism. We refine an earlier approach by Drobac et al. (2014), which requires manual selection of flag diacritics, to obtain substantial size reduction. We show that our approach achieves similar reduc- tions in size, but with a fully automated process. Moreover, the approach presented in this paper is conceptually simpler and faster because, unlike Drobac et al. (2014), we do not need additional processing after applying phonological rules. Ad- ditionally, our approach results in substantially im- proved lookup speed compared to Drobac et al. (2014) due to an operation which we call path con- densation. We apply our approach to morphological an- alyzers for three morphologically complex lan- guages: Greenlandic, North Saami, and Finnish. Compared to Drobac et al. (2014), our approach results in near equal size reduction for these lan- guages without requiring any manual intervention. Furthermore, due to path condensation introduced in Section 3.3, lookup time is reduced for all three languages and because of the new lexc approach, compilation time is reduced for all languages. 2 Background Finite state morphology (Beesley and Karttunen, 2003) is the state-of-the-art in writing morpho- logical analysers for natural languages of the whole range of typologically varying morpholog- ical features. The finite-state approach is built around two practical concepts: constructing lex- icographical descriptions of a language by using a grammar formalism called lexc and express- ing morphophonological variations as regular ex- pression rules. In this paper, we study lossless hyper-minimization of finite-state machines de- rived from lexicographic descriptions in the lexc formalism. In this paper, we use the term hyper- minimization of minimal deterministic finite-state machines to refer to procedures which produce an even smaller finite-state machine, that preserves some of the qualities of the original determinis- tic minimal machine. This definition of hyper- minimization is broad enough to encompass a number of different approaches. The generally used definition of hyper-minimization, introduced by Badr et al. (2009) and further developed by for example Maletti and Quarnheim (2011), is more restricted. The hyper-minimization algorithms investi- gated by Badr et al. (2009) and by Maletti and Quarnheim (2011) introduce a limited amount of changes to the language accepted by the origi- nal machine. This makes the machine suscepti- ble to further size reduction using conventional minimization algorithms. We call this kind of hyper-minimization lossy hyper-minimization be- cause the resulting finite-state machine does not accept the same language as the original machine. Lossy hyper-minimization results in a determinis- tic machine which allows fast lookup. In contrast to lossy hyper-minimization, the ap- proach presented in this paper is lossless. We in- troduce a limited amount of non-determinism into the finite-state machine using labeled epsilon sym- bols (flag diacritics). This allows us to achieve a size reduction, while at the same time preserving the original language accepted by the finite-state machine. The non-determinism introduced by the algorithm results in some reduction in lookup LEXICON Root 0 N ; 0 Adj ; LEXICON N cat+N:cat Num ; LEXICON Adj small+A:small Deg ; LEXICON Num +Sg:0 # ; +Pl:s # ; LEXICON Deg +Pos:0 # ; +Comp:er # ; +Sup:est # ; Figure 1: A lexc lexicon speed, which is not prohibitive in practice. Finding the smallest non-deterministic finite- state machine equivalent to a given machine is PSPACE complete (Jiang and Ravikumar, 1993) for general finite-state machines and therefore in- tractable. Nevertheless, using a linguistic descrip- tion rather than a compiled finite-state machine as the starting point, it is possible to achieve substan- tial reduction in the size of the machine without penalties in compilation time. </chunk></section><section><heading>3 Methods </heading><chunk>In this section, we describe our lossless hyper- minimization algorithm of morphological lexi- cons. The algorithm is applicable on finite-state lexicons formulated as regular grammars. An ex- ample of this kind of formalism is the Xerox lexc formalism (Beesley and Karttunen, 2003). Sec- tions 3.1 and 3.2 closely follow Drobac et al. (2014) but path condensation presented in Section 3.3 represents new work. In the lexc formalism, lexicons are formulated as right branching regular grammars of morpheme continuation classes as demonstrated in Figure 1. Each path through the grammar defines one word form and its analysis. lexc lexicons are compiled into finite-state transducers, which accept exactly the set of cor- respondences between word forms (e.g. cats) and analyses (cat+N+Pl) defined by the regular grammar using well known methods (Hopcroft et al., 2006). Our method is not based on transforming the grammar into a finite-state machine directly. In- stead we utilize so called lexicon joiners intro- duced by (Lind  en et al., 2009). Joiners are spe- cialized labels (e.g. $N$ and $Deg$) that are ap- pended to each entry in the lexicon. They iden- tify the sub-lexicon of the entry and its continua- tion class. Together with a set of finite-state con- straints, this completely encodes the structure of the original grammar. All regular expressions and regular rewrite rules in this paper use Xerox xfst formalism. </chunk></section><section><heading>3.1 Compiling lexicons using joiners </heading><chunk>Compiling a lexicon using joiners starts with ap- pending appropriate joiners to each lexicon en- try. E.g. the entry cat+N:cat with con- tinuation class Num in sub-lexicon N becomes $N$cat+N$Num$:$N$cat$Num$. We then compile all the modified lexicon entries into one trie T and form its Kleene closure T * . The closure T * accepts arbitrary combinations of morphs from the original lexicon. In order to restrict it to valid combinations produced by the original lexc lexicon, we append root and end joiners to T * and get a language $Root$ T * $#$, and apply one finite-state constraint CJ for each joiner $J$ type, which requires that each $J$ has to occur next to another identical joiner $J$. E.g. CJ = NOJ * [$J$ $J$ NOJ * ] * where NOJ = [? * - $J$]. These constraints encode the structure of the original lexicon. After composing the language $Root$ T * $#$ and the joiner constraints, the resulting lexicon transducer L accepts exactly the word forms and analyses defined by the original regular grammar, though interspersed with joiner symbols. In a final processing state, the joiners are removed and the lexicon is determinized and minimized. </chunk></section><section><heading>3.2 Hyperminimization using Flag Diacritics </heading><chunk>When compiling lexicons using joiners, it is often the case that the lexicon is small before the join- ers are removed and the lexicon is determinized and minimized but grows in size after this final processing stage. This may seem surprising, as the transducers essentially encode the same strings notwithstanding joiner symbols. However, joiner symbols seem to add useful structure into the lex- icon which helps to maintain a smaller size. By transforming each joiner $J$ into a flag diacritic @P.J.ON@ 1 , we can maintain the use- ful structure while at the same time allowing for lookup of word forms and further processing such as application of rules and minimization. Note, that there is no actual flag diacritic functionality involved apart from the fact that the symbols are treated as labeled epsilon symbols. We use flag diacritics instead of custom made labeled epsilon symbols because flag diacritics are supported by a number of different finite-state toolkits. </chunk></section><section><heading>3.3 Path Condensation </heading><chunk>Although using joiners results in a smaller lexicon, it also slows down transducer lookup. Sometimes the slowdown can be rather drastic, even around 70% (Drobac et al., 2014). Our initial experiments showed that this happens mainly because of empty lexicon entries, which can result in long sequences of joiner symbols when chained together. During regular compilation, the chains of joiners vanish when all joiners are removed. In our case, how- ever, the chains are unfortunately preserved. In order to speed up lookup, we apply a final op- timization stage before converting joiners into flag diacritics. In this stage we apply a parallel rewrite rule which transforms all sequences of joiner sym- bols into a single joiner symbol, i.e. condenses consecutive joiners into one joiner. Let JOINER be the set of of joiner symbols [$J1$| ... | $JN$], then the rule (in Xerox xfst notation) is JOINER -&gt; 0 || _ JOINER ; The rule and its inverse are composed with the out- put and input side of lexicon, respectively. Finally, the lexicon is determinized and minimized. It is easy to see that path condensation preserves the original language encoded by the lexc lexi- con disregarding joiner symbols. 4 Experiments We performed experiments using three full scale open-source morphological analyzers distributed by the Giellatekno project 2 . We used the analyzers for Finnish (fin), Greenlandic (kal) and Northern Sami (sme) available from the Giellatakno repos- </chunk></section><section><chunk>1 The flag diacritic @P.J.ON@ sets the value ON for fea- ture J. 2 http://giellatekno.uit.no/  itory 3 . For compilation we use the Helsinki Fi- nite State Transducer (HFST) library and tools 4 (Lind  en et al., 2011). We compile the morphological analyzers in three different ways Basic compilation without joiner symbols. Compilation using hyper-minimization. Compilation using hyper-minimization and path condensation. For each compilation method, we report results on Compilation time. Size of the final morphological analyzer. Lookup speed of the final morphological an- alyzer. Lookup speed is tested using continuous text spanning tens of thousands of words for each lan- guage. All experiments were performed on an In- tel Core i5-4300U laptop with a dual core 1.90 GHz processor and 16 GB of RAM. </chunk></section><section><heading>5 Results </heading><chunk>The results of experiments are shown in Table 1. Compilation time using hyper-minimization and path condensation does not seem to be pro- hibitive for any of the analyzers. In fact compila- tion time for both Greenlandic and Northern Sami is reduced by over 60% compared to compila- tion without hyper-minimization. The compilation time for Finnish, however, increases compared with compilation without hyper-minimization. Hyper-minimization together with path conden- sation decreases the size of the Greenlandic lexi- con by over 90% from 140 MB to 13 MB. The size of the Northern Sami transducer also decreases by approximately 7% but hyper-minimization does not seem to have an appreciable effect on the Finnish analyzer. These results are almost as good as the results obtained by Drobac et al. (2014) who report size reduction of 90% for Greenlandic, 17% for Northern Sami and 6% for Finnish. Lookup speeds for hyper-minimized transduc- ers with path condensation are greatly improved 3 svn co -r 109628 https://victorio.uit.no/langtech/trunk main 4 hfst.sf.net to at least 77 % of the original lookup speed from around 30 % of the original speed using only hyper-minimization without path condensa- tion. These are much better than the lookup speeds reported by Drobac et al. (2014), where the lookup speeds for hyper-minimized Finnish and Nortern Sami dropped to below 40 % of the origi- nal lookup speeds. FINNISH None H-M H-M + PC Compile (min) 1 6 5 Size (MB) 18 17 18 Lookup (kw/s) 103 31 79 Speed of orig. 100% 30% 77% GREENLANDIC None H-M H-M + PC Compile (min) 6 2 2 Size (MB) 140 12 13 Lookup (kw/s) 2 2 2 Speed of orig. 100% 100% 100% NORTHERN SAMI None H-M H-M + PC Compile (min) 7 1 1 Size (MB) 14 13 13 Lookup (kw/s) 39 13 31 Speed of orig. 100% 33% 79% Table 1: Results of experiments. The columns de- note (1) compilation without hyper-minimization (None), (2) with hyper-minimization (H-M) and (3) hyper-minimization together with path con- densation (H-M + PC). The rows denote (1) com- pilation time, (2) fst binary size, (3) fst lookup speed (as thousands of words per second) and (4) lookup speed compared with the original compiled transducer. </chunk></section><section><heading>6 Discussion and Conclusions </heading><chunk>Although comparison between our method and (Drobac et al., 2014) is not entirely fair, because our method is fully automatic, and their method allows manual selection of joiner flags, it is clear that our results are similar with regard to size of the analyzers. Lookup speed, however, is greatly improved because of path condensation. Removal of some manually selected flag diacrit- ics following Drobac et al. (2014) would probably give us an even smaller binary size while main- taining high lookup speed. 7 Acknowledgements We want to thank the anonymous reviewers for their useful comments and suggestions. </chunk></section><section><heading>References </heading><chunk>Andrew Badr, Viliam Geffert, and Ian Shipman. 2009. Hyper-minimizing minimized deterministic finite state automata. RAIRO-Theoretical Informatics and Applications, 43(01):6994. Kenneth R Beesley and Lauri Karttunen. 2003. Fi- nite state morphology, volume 18. CSLI publica- tions Stanford. Kenneth R Beesley. 1998. Constraining separated morphotactic dependencies in finite-state grammars. In Proceedings of the International Workshop on Fi- nite State Methods in Natural Language Processing, pages 118127. Association for Computational Lin- guistics. Senka Drobac, Krister Lind  en, Tommi A Pirinen, and Miikka Silfverberg. 2014. Heuristic hyper- minimization of finite state lexicons. In Proceed- ings of the Ninth International Conference on Lan- guage Resources and Evaluation (LREC14), Reyk- javik, Iceland, May. John E. Hopcroft, Rajeev Motwani, and Jeffrey D. Ull- man. 2006. Introduction to Automata Theory, Lan- guages, and Computation (3rd Edition). Addison- Wesley Longman Publishing Co., Inc., Boston, MA, USA. John Hopcroft. 1971. An n log n algorithm for mini- mizing states in a finite automaton. Technical report, DTIC Document. Tao Jiang and B. Ravikumar. 1993. Minimal nfa prob- lems are hard. SIAM J. Comput., 22(6):11171141, December. Krister Lind  en, Miikka Silfverberg, and Tommi Piri- nen. 2009. Hfst tools for morphology an efficient open-source package for construction of morpho- logical analyzers. In Cerstin Mahlow and Michael Piotrowski, editors, State of the Art in Computa- tional Morphology, volume 41 of Communications in Computer and Information Science, pages 2847. Springer Berlin Heidelberg. Krister Lind  en, Erik Axelson, Sam Hardwick, Tommi A Pirinen, and Miikka Silfverberg. 2011. HFSTFramework for Compiling and Applying Mor- phologies. In Cerstin Mahlow and Michael Pi- otrowski, editors, Systems and Frameworks for Computational Morphology, volume 100 of Com- munications in Computer and Information Science, pages 6785. Springer Berlin Heidelberg. Andreas Maletti and Daniel Quernheim. 2011. Op- timal hyper-minimization. Int. J. Found. Comput. Sci., 22(8):18771891. </chunk></section></sec_map>