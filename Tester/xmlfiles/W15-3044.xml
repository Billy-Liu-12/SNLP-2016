<sec_map><section><chunk>Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 361365, Lisboa, Portugal, 17-18 September 2015. c 2015 Association for Computational Linguistics. Multi-level Evaluation for Machine Translation Boxing Chen, Hongyu Guo and Roland Kuhn National Research Council Canada first.last@nrc-cnrc.gc.ca </chunk></section><section><heading>Abstract </heading><chunk>Translations generated by current statisti- cal systems often have a large variance, in terms of their quality against human ref- erences. To cope with such variation, we propose to evaluate translations using a multi-level framework. The method varies the evaluation criteria based on the clus- ters to which a translation belongs. Our experiments on the WMT metric task data show that the multi-level framework con- sistently improves the performance of two benchmarking metrics, resulting in better correlation with human judgment. </chunk></section><section><heading>1 Introduction </heading><chunk>The aims of automatic Machine Translation (MT) evaluation metrics, which measure the quality of translations against human references, are twofold. Firstly, they enable rapid comparisons between different statistical machine translation (SMT) systems. Secondly, they are necessary to the tuning of parameter values during system trainings. To attain these goals, many machine transla- tion metrics have been introduced in recent years. For example, metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and TER (Snover et al., 2006) rely on word n-gram surface matching. Also, metrics that make use of linguis- tic resources such as synonym dictionaries, part- of-speech tagging, or paraphrasing tables, have been proposed, including Meteor (Banerjee and Lavie, 2005) and its extensions, TER-Plus (Snover et al., 2009), and TESLA (Liu et al., 2011). In ad- dition, attempts to deploy syntactic features or se- mantic information for evaluation have also been made, giving rise to the STM and DSTM (Liu and Gildea, 2005), DEPREF (Wu et al., 2013) and MEANT family (Lo and Wu, 2011) metrics. All these evaluation metrics deploy a single evaluation criterion or use the same source of in- formation to evaluate translations. Nevertheless, translations generated by current statistical sys- tems often have widely varying scores, in terms fren Meteor score Density encs Meteor score Density fren BLEU score Density encs BLEU score Density Figure 1: Distributions of translation quality. X- axis is in the range of [0,1]. wordbased Fscore Syntaxbased Fscore Figure 2: Clusters of translations based on quality. Both X-axis and Y-axis are in the range of [0,1]. of their quality against human references. As a re- sult, current metrics often perform better for a por- tion of translations but worse against the others. Consider, for example, two widely used metrics, namely the sentence-level Meteor and BLUE. Fig- ure 1 depicts the distributions of the two metrics evaluation scores, computed on system outputs for two WMT test sets, i.e., the newstest2013.fr-en and newstest2012.en-cs. As shown in Figures 1, the variances of the created evaluation scores are large across evaluation metrics as well as test sets. Such widely varying evaluation quality, how- ever, may be clustered into multiple sub-regions, as illustrated in Figure 2. Here, we sample 300 sentences from the system output of the newstest2013.fr-en test set; we depict the F- measure based on dependency triplet (dependency type, governor word, and dependent word) on the Y-axis against the word-based F-measure on the X-axis. We observe a straight line at the bot- tom left corner (blue box) of the graph represent- 361 ing sentences which all have dependency triplet F- score of zero; if we want to distinguish between them in terms of their quality score, we must rely on word matching rather than on syntax. The sit- uation in the upper right corner (green box) of the graph is quite different. Here, the word-based F- measure and dependency-based F-measure have a roughly linear correlation, suggesting that a com- bination of word-based and syntactic information might be a better measure of quality than either alone. These observations imply that a metric may benefit from applying different sources of infor- mation at different quality levels. In this paper, we propose a multi-level auto- matic evaluation framework for MT. Our strategy first roughly classifies the translations into differ- ent quality levels. Next, it rates the translations by exploiting several different information sources, with the weight on each source depending on its quality level. We apply our method to two met- rics: the Meteor and a new metric, DREEM, which is based on distributed representations. Our exper- iments on the WMT metric task data show that the multi-level framework consistently improves the performance of these two metrics. </chunk></section><section><heading>2 Multi-level Evaluation </heading><chunk>The multi-level evaluation framework works on the sentence level. Specifically, we first assign each test sentence to one of the three categories: low-, medium-, or high-quality translations. Next, we evaluate the translations within each category with a tailored set of weights of the metric on the information sources. To this end, we deploy a simple strategy for the category clustering. Note that more sophisticate strategies could be deployed; we leave this to our future work. Here, we first use a scoring func- tion to compute a score between the translation and its reference. Next, the category assignment of the translation is then determined by a pre-defined score threshold. In detail, suppose we have a translation (t) and its reference (r). The multi-level metric scores the translation pair as follows. Score(t,r) = M (t, r, w l ) if (F (t, r) 1 ) M (t, r, w m ) if ( 1 &lt; F (t, r) 2 ) M (t, r, w h ) otherwise where M (t, r, w) is a metric, w is the weight, F (t, r) is the simple classification scoring func- tion. Also, is a threshold, and its value is auto- matically tuned on development data set. For the classification function, we employ a formula which combines word-based F-measure (denoted as F W (t, r)) and a F-measure (denoted as F D (t, r)) based on dependency triplet (depen- dency type, governor word, dependent word), as follows: F (t, r) = F W (t, r) + (1 ) F D (t, r) (1) where the free parameter is tuned on develop- ment data. It is worth noting that, for languages which de- pendency parser is not available, we only use the word-based F-measure as the classification func- tion. Specifically, we use Equation 1 for Into- English task, and the word-based F-measure for Out-of-English task in this paper. In a scenario where there are multiple refer- ences, we compute the score with each reference, then choose the highest one. In addition, we treat the document-level score as the weighted average of sentence-level scores, with the weights being the reference lengths, as follows. Score d = D i=1 len(r i )Score i D i=1 len(r i ) (2) where Score i is the score of sentence i, and D is the number of sentences in the document. </chunk></section><section><heading>3 Evaluation metrics </heading><chunk>We apply our multi-level approach to two met- rics. The first one is Meteor (Banerjee and Lavie, 2005), which has been widely used for ma- chine translation evaluations. The second one is DREEM, a new metric based on distributed repre- sentations generated by deep neural networks. 3.1 Metric Meteor We use the latest version of Meteor, i.e. Me- teor Universal (Denkowski and Lavie, 2014) in this paper. Meteor computes a one-to-one align- ment between matching words in a translation and a reference. The space of possible align- ments is constructed by exhaustively identifying all possible matches of the following types: ex- act word matches, word stem matches, synonym word matches, and matches between phrases listed as paraphrases. Alignment is then conducted as a beam search. From the final alignment, the translations Me- teor score is calculated as follows. First, content 362 and function words are identified in the hypoth- esis and reference according to a function word list. Next, the weighted precision and recall us- ing match weights (w i ...w n ) and content-function word weight () are computed, as follows: P = i wi ( mi(tc) + (1 ) mi(t f )) |tc| + (1 ) |t f | (3) R = i wi ( mi(rc) + (1 ) mi(r f )) |rc| + (1 ) |r f | (4) These two are then combined into a weighted harmonic mean, where a large means recall is weighted more heavily. Fmean = P R P + (1 ) R (5) To penalize reorderings, this value is then scaled by a fragmentation penalty based on the number of chunks and number of matched words. Meteor(t, r) = (1 ( #chunk #match ) ) Fmean (6) In our studies, we fine-tune all the parameters for both multi-level and non-multi-level scoring frameworks. 3.2 Representation based metric Distributed representations for words and sen- tences have been shown to significantly boost the performance of a NLP system (Turian et al., 2010). A representation-based translation evalu- ation metric, DREEM, is introduced in (Anony- mous, 2015). The metric has shown to be able to achieve state-of-the-art performance, compared to popular metrics such as BLEU and Meteor. There- fore, in this paper, we also adapt this metric for our experiments. In a nutshell, the DREEM metric evaluates translations by employing three different types of word and sentence representations: one-hot representations, distributed word representations learned from a neural network model, and dis- tributed sentence representations computed with a recursive autoencoder (RAE). Two different RAE- based representations are used in this metric: one is based on a greedy unsupervised RAE, while the other is based on a syntactic parse tree. To com- bine the advantages of these four different repre- sentations, the authors concatenate them to form one vector representation for each sentence. In detail, suppose that we have the sentence representations for the translations (t) and refer- ences (r). The translation quality is measured by DREEM with a similarity score computed with the Cosine function and a length penalty. Let the size of the vector be N . The quality score is calculated as follows. Score(t, r) = Cos (t, r) P len (7) Cos(t, r) = i=N i=1 vi(t) vi (r) i=N i=1 v 2 i (t) i=N i=1 v 2 i (r) (8) P len = exp(1 l r /l t ) if (l t &lt; l r ) exp(1 l t /l r ) if (l t l r ) (9) where is a free parameter, v i (.) is the value of the vector element, P len is the length penalty, and l r , l t are lengths of the translation and reference, respectively. To use this metric in the multi-level framework, we keep the parameter consistent for all levels, but use different weights to combine the represen- tations. That is, we construct the representation vector as follows: V =&lt; w1 V oh , w2 V wd , w3 VgRAE, w4 VtRAE &gt; (10) where V oh is the one-hot representation, V wd de- notes the word representations, and V gRAE and V tRAE are representations learned with greedy RAE and tree-based RAE, respectively. The weights w 1 ... w 4 are tuned on development data. </chunk></section><section><heading>4 Experiments </heading><chunk>4.1 Settings We conducted experiments on the WMT met- ric task data. Development sets include WMT 2012 all-to-English, and English-to-all submis- sions. Test sets contain WMT 2013, and WMT 2014 all-to-English, plus 2013, 2014 English- to-all submissions. The languages all include French, Spanish, German, Czech and Russian. For training the word embedding and recursive auto-encoder model, we used WMT 2014 train- ing data 1 . We used the English, French, German and Czech sentences in Europarl v7 and News Commentary for our experiments. To train the representations for Russian, we used the Yandex 1M corpus. 4.2 Results Following WMT 2014s metric task (Machacek and Bojar, 2014), to measure the correlation with 1 http://www.statmt.org/wmt14/translation-task.html 363 Into-English metric seg sys Original BLEU 0.821 Sentence BLEU 0.259 0.841 Original Meteor 0.279 0.849 Sentence Meteor 0.279 0.863 M ulti level w Meteor 0.285 0.871 M ulti level wd Meteor 0.294 0.885 DREEM 0.287 0.875 M ulti level w DREEM 0.293 0.880 M ulti level wd DREEM 0.303 0.892 Table 1: Correlations with human judgment on the WMT data for the Into-English task. Results are averaged on all into-English test sets. M ulti levelw stands for only using word-based F-measure as the classification function, while M ulti level wd denotes the use of a combination of word- based F-measure and dependency triplet based F-measure. indicates the improvement over the non-multi-level metric is statistically significant, with a significance level of 0.05. human judgment, we employed Kendalls rank correlation coefficient for the segment level, and used Pearsons correlation coefficient ( in the be- low tables) for the system-level. We tested the significance through bootstrap resampling (confi- dence level of 95%). We tuned the weights for the Into-English and Out-of-English tasks separately. According to the tuned thresholds, about 25% of the translations are classified to low-quality translations, around 20% belong to high-quality translations, and the rest fall in the medium-quality category. Experimental results conducted on the Into- English and Out-of-English tasks are reported in Tables 1 and 2. We also compared to the standard de facto metric BLEU (Papineni et al., 2002). Results, as shown in Tables 1 and 2, indicate that the representation-based metric DREEM ob- tained better performance than BLEU and Meteor on both tasks at both segment and system lev- els. The multi-level versions of these metrics con- sistently improved the performance over the non- multi-level ones on both segment and system lev- els. 4.3 Further Analysis In addition to showing the superior performance of the multi-level framework, our experiments also indicate the following observations. Firstly, for BLEU and Meteor, document-level score computed by weighted averaging sentence- level scores can get better system-level correla- tion with human judgment, compared to that of the original document-level score which is computed from aggregate statistics accumulated over the en- Out-of-English metric seg sys Original BLEU 0.843 Sentence BLEU 0.221 0.846 Original Meteor 0.228 0.845 Sentence Meteor 0.228 0.853 M ulti level w Meteor 0.234 0.861 DREEM 0.236 0.904 # M ulti level w DREEM 0.241 0.922 # Table 2: Correlations with human judgment on the WMT data for Out-of-English task. Results are averaged over all out-of-English test sets. # indicates DREEM is significantly better than its corresponding version of Meteor, with a sig- nificance level of 0.05. indicates the improvement over the non-multi-level metric is statistically significant. tire document. task low medium high Into-En 0.93 0.81 0.75 Out-of-En 0.99 0.90 0.81 Table 3: The value of parameter in multi-level Meteor. Secondly, for Meteor, recall received a larger weight for low-quality translations than for high- quality translations. For instance, as depicted in Table 3, the parameter in Meteor is higher for low-quality translations. Finally, the syntax feature received higher weight for high-quality translations than for low- quality translations. In contrast, as shown in Table 4, the surface n-gram feature was assigned larger weight for low-quality translations . task low medium high one-hot 0.23 0.11 0.05 word vec 0.42 0.42 0.40 greedy RAE 0.18 0.20 0.20 tree RAE 0.17 0.27 0.35 Table 4: The weights of each representation in the multi- level DREEM tuned for Into-English task. The syntax-based tree RAE representation received higher weight for high- quality translations, while one-hot representation received higher weight for low-quality translations. </chunk></section><section><heading>5 Conclusions </heading><chunk>Translations generated by statistical systems typi- cally have a large variance in terms of their scores against human references. Motivated by such ob- servation, we propose a multi-level framework. It enables a metric to deploy different criteria for various quality levels of translations. Our exper- iments on the WMT metric task data show that the multi-level strategy consistently improves the performance of two benchmarking metrics on both segment and system levels. 364 </chunk></section><section><heading>References </heading><chunk>Anonymous. 2015. Representation based translation evaluation metrics. In Proceedings of the Associa- tion for Computational Linguistics (ACL), Beijing, China, July. Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with im- proved correlation with human judgments. In Pro- ceedings of the ACL Workshop on Intrinsic and Ex- trinsic Evaluation Measures for Machine Transla- tion and/or Summarization, pages 6572, Ann Ar- bor, Michigan, June. Association for Computational Linguistics. Michael Denkowski and Alon Lavie. 2014. Meteor universal: Language specific translation evaluation for any target language. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 376380, Baltimore, Maryland, USA, June. Associ- ation for Computational Linguistics. George Doddington. 2002. Authomatic evaluation of machine translation quality using n-gram co- occurrence statistics. In Proceedings of the Human Language Technology Conference, page 128132, San Diego, CA. Ding Liu and Daniel Gildea. 2005. Syntactic fea- tures for evaluation of machine translation. In ACL 2005 Workshop on Intrinsic and Extrinsic Evalua- tion Measures for Machine Translation and/or Sum- marization, pages 2532, Ann Arbor, MI. Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng. 2011. Better evaluation metrics lead to better ma- chine translation. In Proceedings of the 2011 Con- ference on Empirical Methods in Natural Language Processing, pages 375384, Edinburgh, Scotland, UK., July. Association for Computational Linguis- tics. Chi-kiu Lo and Dekai Wu. 2011. Meant: An inexpen- sive, high-accuracy, semi-automatic metric for eval- uating translation utility based on semantic roles. In Proceedings of the 49th Annual Meeting of the Asso- ciation for Computational Linguistics: Human Lan- guage Technologies, pages 220229, Portland, Ore- gon, USA, June. Association for Computational Lin- guistics. Matous Machacek and Ondrej Bojar. 2014. Results of the wmt14 metrics shared task. In Proceedings of the Ninth Workshop on Statistical Machine Trans- lation, pages 293301, Baltimore, Maryland, USA, June. Association for Computational Linguistics. Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. BLEU: A method for automatic evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 311318, Philadelphia, July. ACL. Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin- nea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of Association for Machine Transla- tion in the Americas. Matthew G. Snover, Nitin Madnani, Bonnie Dorr, and Richard Schwartz. 2009. Ter-plus: Paraphrase, se- mantic, and alignment enhancements to translation edit rate. In Machine Translation, volume 23, pages 117127. Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In ACL, pages 384 394. Xiaofeng Wu, Hui Yu, and Qun Liu. 2013. DCU par- ticipation in WMT2013 metrics task. In Proceed- ings of the Eighth Workshop on Statistical Machine Translation, pages 435439, Sofia, Bulgaria, Au- gust. Association for Computational Linguistics. 365 </chunk></section></sec_map>