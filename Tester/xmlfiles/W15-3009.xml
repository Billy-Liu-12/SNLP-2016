<sec_map><section><chunk>Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 98104, Lisboa, Portugal, 17-18 September 2015. c 2015 Association for Computational Linguistics. New Language Pairs in TectoMT Ond rej Dusek, Luis Gomes, Michal Novak, Martin Popel, and Rudolf Rosa Charles University in Prague, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics {odusek,mnovak,popel,rosa}@ufal.mff.cuni.cz University of Lisbon, Faculty of Sciences, Department of Informatics luis.gomes@di.fc.ul.pt </chunk></section><section><heading>Abstract </heading><chunk>The TectoMT tree-to-tree machine transla- tion system has been updated this year to support easier retraining for more transla- tion directions. We use multilingual stan- dards for morphology and syntax annota- tion and language-independent base rules. We include a simple, non-parametric way of combining TectoMTs transfer model outputs. We submitted translations by the English- to-Czech and Czech-to-English TectoMT pipelines to the WMT shared task. While the former offers a stable performance, the latter is completely new and will require more tuning and debugging. 1 Introduction The TectoMT tree-to-tree machine translation (MT) system (Zabokrtsky et al., 2008) has been competing in WMT translation tasks since 2008 and has seen a number of improvements. Un- til now, the only supported translation direction was English to Czech. This year, as a part of the QTLeap project, 1 we have enhanced TectoMT and its underlying natural language processing (NLP) framework, Treex (Popel and Zabokrtsky, 2010), to support more language pairs. We simpli- fied the training pipeline to be able to retrain the translation models faster, and we use abstracted language-independent rules with the help of Inter- set (Zeman, 2008) where possible. Together with our partners on the QTLeap project, we have implemented translation systems for other language pairs (English to and from Dutch, Spanish, Basque, and Portuguese) which are not part of WMT shared Translation Task this year. However, we were also able to submit the results of a newly built Czech-English translation 1 http://qtleap.eu system in the shared task. The performance of the current version leaves a lot of room for improve- ment, but proves the potential of TectoMT for dif- ferent language pairs. The original TectoMT system for English- Czech translation has seen just small changes, e.g., adding specialized translation models for se- lected pronouns (Novak et al., 2013a; Novak et al., 2013b) and fine-tuning of a handful of rules. Therefore, its performance is virtually identical to that of the last years version. This paper is structured as follows: in Section 2, we introduce the TectoMT basic architecture. In Section 3, we describe the improvements to Tec- toMT that were added for an easier support of new language pairs. Section 4 then details the Czech-to-English TectoMT system submitted to WMT15. We discuss TectoMTs performance in the task and examine the most severe error sources in Section 5. Section 6 then concludes the paper. </chunk></section><section><heading>2 The TectoMT Translation System </heading><chunk>TectoMT (Zabokrtsky et al., 2008) is a tree-to- tree MT system system consisting of an analysis- transfer-synthesis pipeline, with transfer on the level of deep syntax. It is based on the Prague Tec- togrammatics theory (Sgall et al., 1986) and dis- tinguishes two levels of syntactic description (see Figure 1): Surface dependency syntax (a-layer) sur- face dependency trees containing all the to- kens in the sentence. Deep syntax (t-layer) dependency trees that contain only content words (nouns, main verbs, adjectives, adverbs) as nodes. Each node has a deep lemma (t-lemma), a semantic function label (functor), a morpho-syntactic form label (formeme), and various grammat- ical attributes (grammatemes), such as num- ber, gender, tense, or modality. 98 a+tree zone4cs_src Tak Adv Db nad AuxP RR6 tim Adv PDZS6 jsem AuxV VB+SiPA neuvazoval Pred VpYSXRA # AuxK Z0 t+tree zone4cs_src tak MANN adv ten CAUS n0nadC6 pPersPron ACTdrop uvazovat PRED v0fin t+tree zone4en_tst so MANN adv pPersPron ACTn0subj consider PRED v0fin pPersPron CAUS n0obj a+tree zone4en_tst So I Sb did AuxV not Neg consider Obj it # AuxK declarative active past negation indicative singular 1st person singular consider think wonder consideration +i#wI6 +i#2If +g#2h6 +O#I2O v0fin v0rc n0subj n0obj adv +f#8hi +g#2I8 +O#fOO +O#ii6 +O#IgI Adv Obj Figure 1: Example TectoMT translation. From the left to the right: (1) source Czech sentence analyzed to surface dependencies (a-layer), (2) Czech sentence analyzed to deep syntax (t-layer), with t-lemmas (black), functors (capitals), formemes (purple), and grammatemes (teal), (3) translated English t-layer tree (with MaxEnt model logarithmic probabilities for t-lemmas and formemes shown in red for a selected node), (4) generated English surface dependency tree. Formemes are not part of the t-layer accord- ing to the original theory; they have been added in TectoMT to work around the difficult task of functor assignment (semantic role labeling). Formemes are much simpler to obtain they are assigned by rules based on the surface dependency trees (Dusek et al., 2012). Apart from a few spe- cific cases, functors are not used in TectoMT, and formemes are used instead. T-layer representations of the same sentence in different languages are closer to each other than the surface texts; in many cases, there is a 1:1 node correspondence among the t-layer trees. Tec- toMTs transfer exploits this by translating the tree isomorphically, i.e., node-by-node and assuming that the shape will not change in most cases (apart from a few exceptions handled by specific rules). The translation is further factorized t-lemmas, formemes, and grammatemes are translated us- ing separate models. The t-lemma and formeme translation models are an interpolation of maxi- mum entropy discriminative models (MaxEnt) of Mare cek et al. (2010) and simple conditional prob- ability models. The MaxEnt models are in fact an ensemble of models, one for each individual source t-lemma/formeme. The combined transla- tion models provide several translation options for each node along with their estimated probability (see Section 1). The best options are then selected using a Hidden Markov Tree Model (HMTM) with a target-language tree model (Zabokrtsky and Popel, 2009), which roughly corresponds to the target-language n-gram model in phrase-based MT. Grammateme transfer is rule-based; in most cases, grammatemes remain the same as in the source language. </chunk></section><section><heading>3 Adding New Language Pairs </heading><chunk>Using different languages in an MT system with deep transfer is mainly hindered by differences in the analysis and synthesis of the individual lan- guages. To overcome these problems, we decided to use existing multilingual annotation standards (see Section 3.1) and to simplify and automate translation model training (see Section 3.2). In addition, we introduce an easier way of combin- ing the results of the individual translation models than HMTM (in Section 3.3). </chunk></section><section><heading>3.1 Annotation Standards for Language Independence </heading><chunk>We decided to use Interset (Zeman, 2008) as the standard morphological representation since its features capture all important morphological phenomena in many different languages, includ- ing all languages required in the QTLeap project. The Interset Perl library includes conversions from many commonly used language-specific tagsets. To represent surface dependency syntax, we use the HamleDT 1.5 annotation style (Zeman et al., 2012; Zeman et al., 2014), which also supports many different languages and comes with tools for the conversion of various pre-existing treebanks. This allows us to use existing taggers and parsers without retraining them analyzed sentences are simply converted to Interset+HamleDT annotation style. Most TectoMT/Treex rules for the conversion from surface dependencies to deep syntax (t-layer) have been adapted to expect Interset morpholog- ical features and HamleDT-style dependencies, which improves their usability for different lan- 99 guages. Their implementation involves a common language-independent base class and language- specific derived classes. 2 For t-layer representation, we stick to the Tec- toMT annotation style as used for Czech and English, which is originally based on PDT and Prague Czech-English Dependency Treebank an- notation (Haji c et al., 2006; Haji c et al., 2012). However, we are aware that this annotation style has problems in other languages (e.g., gram- matemes cannot express all required grammatical meaning), and that changing or extending it will probably be required. </chunk></section><section><heading>3.2 Support for Training New Language Pairs </heading><chunk>Other improvements to support adding new lan- guage pairs quickly are rather technical. We au- tomated the translation model training in a set of makefiles. To train a new translation pair, one only needs to implement analysis and synthesis pipelines for both languages and edit a configu- ration file. Debugging and testing of the new anal- ysis and synthesis pipelines is supported by mono- lingual roundtrip experiments: a development data set is first analyzed up to t-layer, then synthe- sized back to word forms. BLEU score measure- ments (Papineni et al., 2002) and a direct compar- ison of the results are then used to improve per- formance before the translation models are trained and other transfer blocks are implemented. 3 </chunk></section><section><heading>3.3 Combining Transfer Models More Simply </heading><chunk>The t-lemma and formeme translation models are independent of each other to simplify their deci- sions and reduce data sparsity. This often results in the best translation alternatives suggested by both models being incompatible with each other, which leads to disfluent outputs. In English-to-Czech translation, an HMTM is used to select compatible t-lemmaformeme pairs (see Section 2). However, the HMTM needs to be trained on a large monolingual data set annotated on the t-layer. To simplify and speed up devel- 2 Some Czech and English TectoMT blocks have not been converted to Interset yet; they use the Czech positional tagset from the Prague Dependency Treebank (PDT) of Haji c et al. (2006) and the Penn Treebank tagset (Santorini, 1990). 3 The roundtrip experiments are not necessarily needed for the translation. We just consider them a best practice which helps to quickly reveal bugs that could deteriorate the translation, but remain unnoticed for a long time. opment of TectoMT translation for new language pairs, we have introduced a simpler method of se- lecting a compatible t-lemmaformeme pair which does not require any training. In this approach, t-lemma and formeme probabilities of congruous pairs 4 are combined by a non-parametric function into a single score that is then used to select the best translation option. Incongruous combinations are discarded. 5 We evaluated five non-parametric functions combining the two translation models outputs: AM-P arithmetic mean of probabilities, GM-P geometric mean of probabilities, 6 HM-P harmonic mean of probabilities, GM-Log-P geometric mean of logarithmic probabilities, 7 HM-Log-P harmonic mean of logarithmic probabilities. 8 We compared the functions against a baseline of just using the first option given by each of the models (regardless of compatibility). We used corpora of 1,000 sentences from the IT do- main collected in the QTLeap project to evalu- ate all variants in English-to-Czech, English-to- Spanish, and English-to-Portuguese translation. For the English-to-Czech direction, we could also compare our combination functions to using an HMTM. The results are given in Tables 1, 2, and 3 for English to Czech, Spanish, and Portuguese, re- spectively. We can see that the performance of the individ- ual variants is very similar and that they bring an improvement over the baseline in almost all cases. 4 The congruency of t-lemma and formeme is based on the syntactic part-of-speech encoded in the formeme and the Interset part-of-speech of the t-lemma. There are five sim- ple rules, e.g., verbal t-lemmas are compatible only with formemes beginning with v:. 5 The non-parametric functions are weaker than the HMTM with the target-language tree model, which considers the context of the parent t-lemma and models the compatibil- ity with real-valued probabilities. 6 Maximizing GM-P gives the same result as maximizing the product of probabilities P (t-lemma)P (formeme), which is the theoretically sound approach. 7 Logarithmic probabilities are negative and geometric mean of two negative numbers is positive, so we actually use negative GM-Log-P, so the best option has the highest score. 8 AM-Log-P, the arithmetic mean of logarithmic probabil- ities, seems to be missing from the list above, but since maxi- mizing over AM-Log-P gives the same results as maximizing over GM-P, we omit AM-Log-P from our experiments. 100 Function NIST BLEU Baseline 6.7500 0.2785 HMTM 6.8212 0.2876 AM-P 6.7602 0.2811 GM-P 6.7690 0.2818 HM-P 6.7713 0.2820 GM-Log-P 6.7707 0.2817 HM-Log-P 6.7580 0.2810 Table 1: NIST and BLEU scores for non- parametric combining functions in English-to- Czech translation. Function NIST BLEU Baseline 5.2757 0.1670 AM-P 5.4342 0.1808 GM-P 5.4315 0.1806 HM-P 5.4306 0.1806 GM-Log-P 5.4314 0.1809 HM-Log-P 5.4336 0.1808 Table 2: NIST and BLEU scores for non- parametric combining functions in English-to- Spanish translation. HMTM in the English-to-Czech translation per- forms better as expected. 4 Czech to English Translation This section is a detailed description of the Tec- toMT Czech-to-English translation pipeline as used in the WMT translation task. The analysis part (Section 4.1) is not new and thus is described only briefly, we focus more on the simple trans- fer (Section 4.2) and the English synthesis (Sec- tion 4.3). Function NIST BLEU Baseline 5.1584 0.1677 AM-P 5.2612 0.1719 GM-P 5.2219 0.1711 HM-P 5.0613 0.1620 GM-Log-P 5.2452 0.1719 HM-Log-P 5.2583 0.1719 Table 3: NIST and BLEU scores for non- parametric combining functions in English-to- Portuguese translation. </chunk></section><section><heading>4.1 Czech Analysis </heading><chunk>The Czech analysis is a slightly improved version of the pipeline used to train previous versions of the English-to-Czech translation in TectoMT as well as to analyze the Czech part of the CzEng 1.0 parallel corpus (Bojar et al., 2012). The first part, the surface syntactic analysis, consists of a rule-based sentence segmenter and tokenizer, followed by a part-of-speech tagger we use MorphoDiTa (Strakova et al., 2014) in the current version and a dependency parser (McDonald et al., 2005; Novak and Zabokrtsky, 2007). The surface dependency trees are then con- verted into deep syntactic (t-layer) trees using a series of mostly rule-based modules that collapse auxiliary words and decide upon the t-lemma, formeme, and grammatemes. They also recon- struct pro-drop pronoun subjects based on verbal morphology. </chunk></section><section><heading>4.2 Transfer </heading><chunk>The Czech-to-English transfer is relatively basic and does not contain many components besides the translation models for t-lemmas and formemes (see Section 2). Due to limited time to train the system for the new translation direction, we used the non-parametric t-lemmaformeme combina- tion functions as described in Section 3.3 instead of a Hidden Markov Tree Model (cf. Section 2). We chose the HM-P setting based on performance on the development set. 9 The additional components are rule-based and are listed below: Overrides and additions to the translation models, tuned on the development set, Removing Czech gender from common nouns not referring to persons, Fixing translation of names based on a lexi- con compiled from Wikipedia (in particular, reverting the Czech female surname ending -ova in non-Czech names), Removing subjects of verbs where the trans- lation model chose an infinitival form, Removing double negatives (which are the rule in Czech but not in English), 9 We used the WMT news-test2012 data to tune our sys- tem. 101 Fixing grammatemes, in particular number and negation, for some translations, such as t estoviny (pl.) pasta (sg.), or nedbaly (neg- ative) sloppy (positive). </chunk></section><section><heading>4.3 English Synthesis </heading><chunk>The English synthesis (surface realization) pipeline has been newly developed for TectoMT translation into English; it is mostly rule-based and is inspired by the Czech synthesis pipeline. Besides the Czech-to-English translation, it is used in other TectoMT systems translating into English within the QTLeap project and in the TGen natural language generator (Dusek and Jur ci cek, 2015). In the synthesis pipeline, a new surface depen- dency (a-layer) tree is created as a copy of the source t-layer tree, with lemmas copied from t- lemmas and dependency labels, word forms, and morphology left undecided. All further changes are performed on the surface dependency tree, consulting information from the t-layer tree. The pipeline consists of the following steps: </chunk></section><section><heading>1. Morphological attributes are filled in based on grammatemes. </heading></section><section><heading>2. Subjects are marked (to support subject- predicate agreement). </heading><chunk>3. Basic English word order for declarative sen- tences is enforced. This only contains very general rules, e.g., SVO-order or adjective- noun order, but preliminary tests with source- language ordering from several different lan- guages indicated that it is sufficient in most cases. 4. Subject-predicate agreement in number and person is enforced predicates have their number and person filled based on their sub- ject(s). 5. Auxiliary words are added. These are based on the contents of formemes (prepo- sitions, subordinating conjunction, infinitive particles, possessive markers) and t-lemmas (phrasal verb particles). 6. English articles are added based on a hand- ful of rules from an older surface realizer by Pta cek (2008). 7. Auxiliary verbs are added, expressing the voice, tense, and modality. Auxiliaries are also added for questions and sentences with existential there. </chunk></section><section /><section><chunk>9. Negation particles are added for verbs as well as selected adjectives and adverbs.  10. Punctuation is added to the end of the sen- tence, into coordinations and appositions, af- ter clause-initial phrases preceding the sub- ject, and in selected phrases (based on formemes). 11. Words are inflected based on their lemma and morphological attributes. We use rules for personal pronouns, MorphoDiTa (Strakova et al., 2014) English dictionary for unambigu- ous words, and Flect (Dusek and Jur ci cek, 2013) for all remaining words requiring in- flection. 10 </chunk></section><section /><section><heading>13. Repeated coordinated prepositions and con- junctions are deleted. </heading></section><section><heading>14. The first word in the sentence is capitalized. </heading><chunk>The output sentence is then obtained by just com- bining all the nodes in the resulting surface depen- dency tree. 5 WMT 2015 Translation Task Results TectoMT reached a BLEU score of 13.9 for the English-to-Czech direction in the WMT 2015 Translation Task. This ranks it among the last sys- tems, which is consistent with results from previ- ous years. However, English-to-Czech TectoMT has also been used in the Chimera system com- bination, which ranks first in both automatic and human evaluation results. TectoMT plays a very important role in Chimera (Tamchyna and Bojar, 2015). TectoMTs Czech-to-English translation reached a BLEU score of 12.8, and finished last 10 Alternatively, an n-gram language model could be used to select the word forms. Flect uses just a short context of neighboring lemmas, but it generalizes also to unseen words (thanks to morphological features). Currently, no n-gram lan- guage model is used in the whole TectoMT system. 102 in the automatic evaluation; human evaluation scores indicate a second-to-last position. We believe that the major cause for the lower scores does not lie in TectoMTs basic architec- ture, but that improvements to translation mod- els are required, as well as better tuning and de- bugging of the whole pipeline for the Czech-to- English direction. We examined closely a sample of the translation output (in both directions) and identified the following error sources: Translation models will require more tuning and possibly more powerful features. The English-to-Czech model leaves many rela- tively common words untranslated, which suggests that pruning has been too strict. 11 The non-parametric t-lemmaformeme com- bination functions are not ideal; training an HMTM will be necessary to improve English-to-Czech performance. Word ordering rules need to be improved, and more different cases need to be covered. We consider using a statistical ranker for local node ordering. The rule-based article assignment in English synthesis is lacking; indefinite articles are as- signed much more often than they should be. This will probably not be possible without us- ing a statistical module. There are also other, rather technical issues re- lated to punctuation or tokenization that will re- quire more debugging. 6 Conclusions and Future Work We presented TectoMT, a tree-to-tree machine translation system with deep transfer, and its new features in this years edition of the WMT shared task, the main one being opening the system to new language pairs. TectoMT in the English-to- Czech direction is stable and provides useful trans- lations though its results are worse than that of other systems; it is also used in the Chimera sys- tem combination. The new Czech-to-English sys- tem requires more development but shows that it 11 Same as for the English-to-Czech direction, the MaxEnt model was trained only for (source) lemmas occurring at least 100 times in the training data and only with translations (tar- get lemmas) occurring at least 5 times. For the simple condi- tional (static) model, we used the same constants (by mis- take). is possible to adapt TectoMT to a new translation direction in a very short amount of time. In future, we plan to tune the current Czech- to-English setup, and to include further improve- ments. We intend to use Interset instead of gram- matemes on the t-layer to support categories of grammatical meaning not present in grammatemes (see Section 3.1). We also consider switching the TectoMT annotation style to Universal Dependen- cies. To improve translation models, we are plan- ning to use Vowpal Wabbit (Langford et al., 2007) and to include word embeddings from word2vec (Mikolov et al., 2013) as features. We are also investigating the possibilities of non-isomorphic transfer in TectoMT. Acknowledgments This work has been supported by the 7th Frame- work Programme of the EU grant QTLeap (No. 610516), and SVV project 260 104 and GAUK grants 2058214 and 338915 of the Charles University in Prague. It is using language re- sources hosted by the LINDAT/CLARIN Re- search Infrastructure, Project No. LM2010013 of the Ministry of Education, Youth and Sports. </chunk></section><section><heading>References </heading><chunk>O. Bojar, Z. Zabokrtsky, O. Dusek, P. Galus cakova, M. Majlis, D. Mare cek, J. Marsik, M. Novak, M. Popel, and A. Tamchyna. 2012. The joy of paral- lelism with CzEng 1.0. In LREC, page 39213928, Istanbul. O. Dusek and F. Jur ci cek. 2013. Robust Multilingual Statistical Morphological Generation Models. In 51st Annual Meeting of the Association for Compu- tational Linguistics Proceedings of the Student Re- search Workshop, pages 158164, Sofia. Associa- tion for Computational Linguistics. O. Dusek and F. Jur ci cek. 2015. Training a natural lan- guage generator from unaligned data. In Proceed- ings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Interna- tional Joint Conference on Natural Language Pro- cessing, pages 451461. Association for Computa- tional Linguistics. O. Dusek, Z. Zabokrtsky, M. Popel, M. Majlis, M. Novak, and D. Mare cek. 2012. Formemes in English-Czech deep syntactic MT. In Proceed- ings of the Seventh Workshop on Statistical Machine Translation, page 267274. J. Haji c, E. Haji cova, J. Panevova, P. Sgall, O. Bo- jar, S. Cinkova, E. Fu cikova, M. Mikulova, P. Pajas, 103 J. Popelka, J. Semecky, J. Sindlerova, J. St epanek, J. Toman, Z. Uresova, and Z. Zabokrtsky. 2012. Announcing Prague Czech-English Dependency Treebank 2.0. In Proceedings of LREC, pages 3153 3160, Istanbul. J. Haji c, J. Panevova, E. Haji cova, P. Sgall, P. Pajas, J. St epanek, J. Havelka, M. Mikulova, Z. Zabokrt- sky, M. Sev cikova Razimova, and Z. Uresova. 2006. Prague Dependency Treebank 2.0. Number LDC2006T01. LDC, Philadelphia, PA, USA. </chunk></section><section><chunk>J. Langford, L. Li, and A. Strehl. 2007. Vowpal Wabbit online learning project. http://hunch.net/~vw/.  D. Mare cek, M. Popel, and Z. Zabokrtsky. 2010. Maximum entropy translation model in dependency- based mt framework. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 201206. Association for Computational Linguistics. R. McDonald, F. Pereira, K. Ribarov, and J. Haji c. 2005. Non-projective dependency parsing using spanning tree algorithms. In Proceedings of the con- ference on Human Language Technology and Em- pirical Methods in Natural Language Processing, pages 523530. T. Mikolov, K. Chen, G. Corrado, and J. Dean. 2013. Efficient estimation of word representations in vec- tor space. arXiv preprint arXiv:1301.3781. M. Novak, A. Nedoluzhko, and Z. Zabokrtsky. 2013a. Translation of it in a deep syntax framework. In 51st Annual Meeting of the Association for Com- putational Linguistics Proceedings of the Workshop on Discourse in Machine Translation, pages 5159, Sofija, Bulgaria. B  algarska akademija na naukite, Omnipress, Inc. M. Novak, Z. Zabokrtsky, and A. Nedoluzhko. 2013b. Two case studies on translating pronouns in a deep syntax framework. In Proceedings of the 6th In- ternational Joint Conference on Natural Language Processing, pages 10371041, Nagoya, Japan. Asian Federation of Natural Language Processing. V. Novak and Z. Zabokrtsky. 2007. Feature en- gineering in maximum spanning tree dependency parser. In Text, Speech and Dialogue, pages 9298. Springer. K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. BLEU: a method for automatic evaluation of ma- chine translation. In Proceedings of the 40th annual meeting of the Association for Computational Lin- guistics, page 311318. M. Popel and Z. Zabokrtsky. 2010. TectoMT: modu- lar NLP framework. Advances in Natural Language Processing, pages 293304. J. Pta cek. 2008. Two Tectogrammatical Realizers Side by Side: Case of English and Czech. In Fourth In- ternational Workshop on Human-Computer Conver- sation, Bellagio, Italy. B. Santorini. 1990. Part-of-speech tagging guide- lines for the Penn Treebank project (3rd revision). Technical Report No. MS-CIS-90-47, University of Pennsylvania Department of Computer and Informa- tion Science, Philadelphia, PA, USA. P. Sgall, E. Haji cova, and J. Panevova. 1986. The meaning of the sentence in its semantic and prag- matic aspects. D. Reidel, Dordrecht. J. Strakova, M. Straka, and J. Haji c. 2014. Open- Source Tools for Morphology, Lemmatization, POS Tagging and Named Entity Recognition. In Pro- ceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstra- tions, pages 1318. Association for Computational Linguistics. A. Tamchyna and O. Bojar. 2015. What a transfer- based system brings to the combination with PBMT. In Proceedings of the Fourth Workshop on Hybrid Approaches to Translation (HyTra), pages 1120, Beijing, July. Association for Computational Lin- guistics. Z. Zabokrtsky and M. Popel. 2009. Hidden Markov Tree Model in Dependency-based Machine Transla- tion. In Proceedings of the ACL-IJCNLP 2009 Con- ference Short Papers, pages 145148, Singapore. Association for Computational Linguistics. Z. Zabokrtsky, J. Pta cek, and P. Pajas. 2008. Tec- toMT: highly modular MT system with tectogram- matics used as transfer layer. In Proceedings of the Third Workshop on Statistical Machine Translation, page 167170. Association for Computational Lin- guistics. D. Zeman, D. Mare cek, M. Popel, L. Ramasamy, J. St epanek, Z. Zabokrtsky, and J. Haji c. 2012. HamleDT: To parse or not to parse? In Proceed- ings of the Eight International Conference on Lan- guage Resources and Evaluation (LREC12), Istan- bul, Turkey. European Language Resources Associ- ation (ELRA). D. Zeman, O. Dusek, D. Mare cek, M. Popel, L. Ra- masamy, J. St epanek, Z. Zabokrtsky, and J. Haji c. 2014. HamleDT: Harmonized multi-language de- pendency treebank. Language Resources and Eval- uation, 48(4):601637. D. Zeman. 2008. Reusable Tagset Conversion Us- ing Tagset Drivers. In Proceedings of LREC, pages 213218. 104 </chunk></section></sec_map>