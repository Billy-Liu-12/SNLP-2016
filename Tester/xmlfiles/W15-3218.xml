<sec_map><section><chunk>Proceedings of the Second Workshop on Arabic Natural Language Processing, pages 150154, Beijing, China, July 26-31, 2015. c 2014 Association for Computational Linguistics QCRI@QALB-2015 Shared Task:Correction of Arabic Text for Native and Non-Native Speakers Errors Hamdy Mubarak, Kareem Darwish, Ahmed Abdelali Qatar Computing Research Institute Hamad Bin Khalifa University Doha, Qatar {hmubarak, kdarwish, aabdelali}@qf.org.qa Abstract This paper describes the error correc- tion model that we used for the QALB- 2015 Automatic Correction of Arabic Text shared task. We employed a case-specific correction approach that handles specific error types such as dialectal word substi- tution and word splits and merges with the aid of a language model. We also ap- plied corrections that are specific to sec- ond language learners that handle erro- neous preposition selection, definiteness, and gender-number agreement. </chunk></section><section><heading>1 Introduction </heading><chunk>In This paper, we provide a system description for our submissions to the Arabic error correction shared task (QALB-2015 Shared Task on Auto- matic Correction of Arabic) as part of the Arabic NLP workshop. The QALB-2015 shared task is an extension of the first QALB shared task (Mo- hit et al., 2014) which addressed errors in com- ments written to Aljazeera articles by native Ara- bic speakers (Zaghouani et al., 2014). The cur- rent competition includes two tracks, and, in ad- dition to errors produced by native speakers, also includes correction of texts written by learners of Arabic as a foreign language (L2) (Zaghouani et al., 2015). The native track includes Alj- train-2014, Alj-dev-2014, Alj-test-2014 texts from QALB-2014. The L2 track includes L2-train-2015 and L2-dev-2015. This data was released for the development of the systems. The systems were scored on blind test sets Alj-test-2015 and L2-test- 2015. We submitted runs to the automatic correction of text generated by native speaker (L1) and non- native speakers (L2). For both L1 and L2, we employed a case-specific approach that is aided by a language model (LM) to handle specific error types such as dialectal word substitutions and word splits. We also constructed a list of corrections that we observed in the QALB-2014 data set and in the QALB-2015 training set. We made use of these corrections to generate alter- native corrections for words. When dealing with L2 text, we noticed specific patterns of mistakes mainly related to gender-number agreement, pho- netic spellings, and definiteness. As for punctua- tion recovery, we opted only to place periods at the end of sentences and to correct reversed question marks. We opted not to invest in punctuation re- covery based on the mixed results we obtained for the QALB-2014 shared task (Mubarak and Dar- wish, 2014). </chunk></section><section><heading>2 QALB L2 Corpus Error Analysis </heading><chunk>The QALB corpus used for the task contains over two million words of manually corrected Arabic text. The corpus is composed of text that is pro- duced by native speakers as well as non-native speakers (Habash et al., 2013). While annotat- ing the corpus, Zaghouani et al. (2014) detailed various types of errors that were encountered and addressed - mainly L1. Additional proposed cor- rections for L2 errors were summarized with no details. Understanding the error types would shed light on their manifestations and help correct them properly. We inspected the training and develop- ment sets and noticed a number of potential issues that can be summarized as follows: 1. Syntax Errors due to first language influence: L2 learners may carry over rules from their na- tive languages resulting in syntactic and mor- phological errors, such as: (a) Definiteness: In Arabic syntax, a possessive case, idafa construct, which happens between two words, mostly requires that the first word be indefinite while the second be definite. Such as the case of (ktAb Al- 150 tlmy* 1 The book of the student). Note, the first Arabic word doesnt contain the defi- nite article Al while the second does. Er- roneous application, or not, of the definite article was common. For example, the stu- dent may say: (ktAb tlmy*) or (AlktAb Altlmy*). (b) Gender-number agreement: Gender-number agreement is another common error type. The inflectional morphology of Arabic may: embed gender-number markers in verbs as in (&gt;Ejbtny Almdynp I liked the city) and the learner may write (&gt;Ejbny Almdynp) with- out the feminine marker; and the use of feminine singular adjectives with masculine plural inanimate nouns as in (mdn EZymp great cities) and the learner may write (mdn EZymwn) or (mdn EZymAt). (c) Prepositions: Mixing the usage of prepo- sitions is another typical challenge for L2 learners, as it requires good understanding of spacio-temporal aspects of language. Thus, L2 learners tend to mix between these prepo- sitions as in (wSlt fy Alm- dynp I arrived in the city) instead of (wSlt lY Almdynp I arrived to the city). 2. Spelling errors: Grasping sounds is another challenging issue particularly given: (a) Letter that sound the same but written differ- ently, such as (t) and (p), may lead to erroneous spellings like (mbArAt game) instead of (mbArAp). Other example letter pairs are (S) and (s) and (T) and (t) (b) Letters that have similar shapes but a differ number of dots on or below them. We noticed that L2 learners often confuse letter such as: (j), (H), and (x); and (S) and (D). This may lead to errors such as (Sbb AlxAdv) instead of 1 Buckwalter transliteration (sbb AlHAdv the reason for the accident). 3 Word Error Correction In this section we describe our case-specific error correction system that handles specific error types with the aid of a language model (LM) generated from an Aljazeera corpus. We built a word bigram LM from a set of 234,638 Aljazeera articles 2 that span 10 years. Mubarak et al. (2010) reported that spelling mistakes in Aljazeera articles are infre- quent. We used this language model in all sub- sequent steps. We attempted to address specific types of er- rors including dialectal words, word normalization errors, and words that were erroneously split or merged. Before applying any correction, we al- ways consulted the LM. We handled the following cases in order (L2 specific corrections are noted): Switching from English punctuation marks to Arabic ones, namely changing: ? and , . Correcting errors in definite article ( Al) when its preceded by the preposition ( l) ex: lAlEml llEml. Handling common dialectal words and com- mon word-level mistakes. To do so, we ex- tracted all the errors and their corrections from the QALB-2014 (train, dev, and test) and the training split of the QALB-2015 data set. In all, we ex- tracted 221,460 errors from this corpus. If an er- ror had 1 seen correction and the correction was done at least 2 times, we used the correction as a deterministic correction. For example, the word ( AlAHdAv the events) was found 86 times in this corpus, and in all cases it was corrected to ( Al &gt; HdAv ). There were 10,255 such corrections. Further, we man- ually revised words for which a specific correc- tion was made in 60% or more of the cases (2,076 words) to extract a list of valid alternatives for each word. For example, the word ( AlAmwr) appeared 157 times and was corrected to ( Al &gt; mwr ) in 99% of the cases. We ig- nored the remaining seen corrections. An example dialectal word is ( Ally this or that) 2 http://www.aljazeera.net 151 which could be mapped to ( Al*y), ( Alty), or ( Al*yn). An example of a com- mon mistake is ( &gt; n$A Allh God willing) which is corrected to ( &gt;n $A Allh). When performing correction, given a word appearing in our list, we either replaced it deterministically if it had one correction, or we consulted our LM to pick between the different al- ternatives. When dealing with L2 data, we added 297 more deterministic errors (ex: wvm was always corrected to vm). Handling split conjunctions ( w) that should be concatenated with the next word (ex: w HnAk wHnAk). Handling errors pertaining to the different forms of alef, alef maqsoura and ya, and ta marbouta and ha as described in Table 1 and Table 2. We used an approach similar to the open suggested by Moussa et al. (Moussa et al., 2012), and we also added the following cases, namely attempting to replace: &amp; with &amp;w or }w; and } with y or vice versa (ex: mr&amp;s mr&amp;ws, qAry qAr}). To generate the alternatives for words, we normalized all the unique words in the Aljazeera corpus, and we constructed a reverse look-up table that has the normalized form as the key and a list of seen alternatives that could have generated the normalized form. The look-up table contained 905k normalized word entries with corresponding denormalized forms. When correcting, a word is normalized and looked-up in the table to retrieve possible alternatives. We used the LM to pick the best alternative in context. Table 2 shows examples from the look-up table for normalized words and their alternative corrections. Removing repeated letters. Often people repeat letters, particularly long vowels, for em- phasis as in ( &gt;xyyyyrAAA) (meaning at last). We corrected for elongation in a manner similar to that of Darwish et al. (Darwish et al., 2012). When a long vowel is repeated, we replaced it with a either the vowel (ex. &gt;xyrA finally) or the vowel with one repetition (ex. sEwdyyn Saudis) and scored it using the LM. This was expanded to consonants also (ex. bkvyrrrr bkvyr). If a repeated alef appeared in the beginning of the word, we attempted to replace it with alef lam (ex. AAHDArp AlHDArp civilization). A trailing alef-hamza-alef sequence was replaced by alef-hamza (ex. smAA smA (meaning sky)). Also, we replaced ( lll) at the beginning of word by ( ll) (ex. lllgp llgp). Handling grammar errors in verb suffixes to restore missing alef (ex. AfElw AfElwA do (plural); syfElwA syfElwn they will do; ltH- fZwn ltHfZwA that you may memorize/protect). Handling merges and splits. Often words are concatenated erroneously. Thus, we attempted to split all words that were at least 5 letters long after letters that dont change their shapes when they are connected to the letters following them, namely different alef forms, d, *, r, z, w, p, and Y (ex: yArbnA yA rbnA). If the bigram was observed in the LM and the LM score was higher (in con- text) than when they were concatenated, then the word was split. Conversely, some words were split in the middle. We attempted to merge every two words in sequence. If the LM score was higher (in context) after the merge, then the two words would be merged (ex: AntSAr At AntSArAt). Correcting out-of-vocabulary (OOV) words. For words that were not observed in the LM, we attempted replacing phonetically or visually simi- lar letters and deleting/replacing letters that appear in dialectal words as shown in Table 3. Generated suggestions are scored in context using the LM. Many of these errors are common in the L2 data set. For L2 data only, as we mentioned earlier we observed errors pertaining to definiteness and gender-number agreement. We generated possi- ble corrections as follows: words that start with definite article, we scored the word with and with- 152 out a definite article. We did the same with words ending with ta marbouta (p). We also added other alternatives for the word by adding the definite ar- ticle and/or that ta marbouta (for words without one or the other or neither). In all cases, we used the LM to select the most probable alternative in contexts. Letter Norm. Example &gt;, &lt;, | A AHmd &gt; Hmd AqnAE &lt; qnAE Amn |mn Y y qSwy qSwY p h qyAdh qyAdp &amp;, } ms&amp;wl mswl diacritics null mvqf muvaq f K kashida null kbyr kby r Table 1: Word Normalization. </chunk></section><section><heading>4 Official Shared Task Experiments and Results </heading><chunk>We submitted 1 run for L1 errors (QCRI-1-ALJ), and 2 runs for L2 errors (QCRI-1-L2, QCRI-2-L2) as follows: </chunk></section><section><heading>1. QCRI-1-ALJ: case-based correction for L1 test. </heading><chunk>2. QCRI-2-L1: case-based correction for L2 test file and also by adding alternatives for possible errors in the definite article Al and femi- nine mark p as described in section 3. </chunk></section><section><chunk>3. QCRI-1-L2: case-based correction for L2 test file with handling the definiteness or feminine marker.  Table 4 and Table 5 report the officially submit- ted results against the development set and test set in order, and Table 6 reports the results of the new system against the development set and test set of QALB 2014 shared task. Word Alternatives and Frequencies 20352, 632, 5 AElAm &lt; ElAm 20352, &gt; ElAm 632, AElAm 5 1271, 1 HDArh HDArp 1271, HDArh 1 Table 2: Word Alternatives. Case Example Z, D DAbT ZAbT d, * Al*hb Aldhb b+ ylEb bylEb d+ ylEb dylEb H+, h+ syktb Hyktb hAl+ h*h Albnt hAlbnt EAl+ ElY AlArD EAlArD t, T AllAtynyp AllATynyp j, H, x AltxSS AltHSS q, k dktwrAh dqtwrAh Al+ ... +y Altxrj Altxrjy Table 3: Phonetic, Dialectal, and L2 Errors 5 Conclusion In this paper, we presented an automatic approach for correcting Arabic text based on handling spe- cific error types. We handled common dialectal words, some dialectal morphological features, let- ter normalization errors (ex. alef, ta marbouta, etc.), and word splitting and merging. For the L2 corpus, we also corrected letters that L2 learn- ers often confuse because of similarity in shape or sound, and we attempted to correct errors pertain- ing to definiteness and gender-number agreement. For punctuation recovery, we opted to put periods at the end of sentences. Preliminary experiments using fuzzy match using a character-based mod- 153 Run P R F1 QCRI-1-ALJ 84.2 49.8 62.6 (Alj-dev-2015) QCRI-1-L2 46.3 19.2 27.1 (L2-dev-2015) QCRI-2-L2 57.6 16.3 25.4 (L2-dev-2015) Table 4: Official Results for Dev. Data Run P R F1 QCRI-1-ALJ 84.74 58.10 68.94 (Alj-test-2015) QCRI-1-L2 45.86 20.16 28.01 (L2-test-2015) QCRI-2-L2 54.87 17.63 26.69 (L2-test-2015) Table 5: Official Results for Test Data els showed promising results(Sajjad et al., 2012; Durrani et al., 2014; Darwish et al., 2014). We in- tend to incorporate this development among others in our on-going research. The fuzzy match algo- rithm will correct cases like: ( , Al&gt;bEA , ystxdnwnhA) to ( , Al&lt;rbEA , ystxdmwnhA). L2 learners present new spelling error types. Such types may not typical spelling errors as they may produce valid words that are erroneous in context. Hence employing a methodology to de- tect such cases will be of great help.Also, we plan to handle more grammar errors for cases like: numbers, case endings, gender-number agree- ment, irregular (broken) plurals, and Tanween er- rors ( ). </chunk></section><section><heading>References </heading><chunk>Kareem Darwish, Walid Magdy, and Ahmed Mourad. 2012. Language processing for arabic microblog retrieval. In Proceedings of the 21st ACM inter- national conference on Information and knowledge management, pages 24272430. ACM. Kareem Darwish, Ahmed M. Ali, and Ahmed Abde- lali. 2014. Query term expansion by automatic learning of morphological equivalence patterns from wikipedia. In SIGIR 2014 Workshop on Semantic Matching in Information Retrieval (SMIR), volume 1204, pages 2429. CEUR-WS. Nadir Durrani, Hassan Sajjad, Hieu Hoang, and Philipp Koehn. 2014. Integrating an unsupervised translit- Run P R F1 Alj-dev-2014 65.42 62.96 64.17 Alj-test-2014 65.79 61.94 63.81 Table 6: Results for QALB 2014 Data Sets eration model into statistical machine translation. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Lin- guistics, volume 2: Short Papers, pages 148153, Gothenburg, Sweden, April. Association for Com- putational Linguistics. Nizar Habash, Behrang Mohit, Ossama Obeid, Kemal Oflazer, Nadi Tomeh, and Wajdi Zaghouani. 2013. Qalb: Qatar arabic language bank. In Proceedings of Qatar Annual Research Conference (ARC-2013), Doha, Qatar. Behrang Mohit, Alla Rozovskaya, Nizar Habash, Wa- jdi Zaghouani, and Ossama Obeid. 2014. The First QALB Shared Task on Automatic Text Correction for Arabic. In Proceedings of EMNLP Workshop on Arabic Natural Language Processing, Doha, Qatar, October. Mohammed Moussa, Mohamed Waleed Fakhr, and Ka- reem Darwish. 2012. Statistical denormalization for arabic text. In In Empirical Methods in Natural Lan- guage Processing. Hamdy Mubarak and Kareem Darwish. 2014. Auto- matic correction of arabic text: a cascaded approach. Arabic NLP 2014 Workshop. Hamdy Mubarak, Mostafa Ramadan, and Ahmed Met- wali. 2010. Spelling mistakes in arabic newspa- pers. In Arabic Language and Scientific Researches conference, Faculty of Arts, Ain Shams University, Cairo, Egypt. Hassan Sajjad, Alexander Fraser, and Helmut Schmid. 2012. A statistical model for unsupervised and semi-supervised transliteration mining. In Proceed- ings of the Association for Computational Linguis- tics, ACL 12, pages 469477, Jeju, Korea. Wajdi Zaghouani, Behrang Mohit, Nizar Habash, Os- sama Obeid, Nadi Tomeh, Alla Rozovskaya, Noura Farra, Sarah Alkuhlani, and Kemal Oflazer. 2014. Large scale arabic error annotation: Guidelines and framework. In Proceedings of the Ninth Interna- tional Conference on Language Resources and Eval- uation (LREC14), Reykjavik, Iceland, May. Wajdi Zaghouani, Nizar Habash, Houda Bouamor, Alla Rozovskaya, Behrang Mohit, Abeer Heider, and Ke- mal Oflazer. 2015. Correction annotation for non- native arabic texts: Guidelines and corpus. In Pro- ceedings of The 9th Linguistic Annotation Work- shop, pages 129139, Denver, Colorado, USA, June. Association for Computational Linguistics. 154 </chunk></section></sec_map>