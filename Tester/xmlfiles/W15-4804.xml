<sec_map><section><chunk>Count-based State Merging for Probabilistic Regular Tree Grammars Toni Dietze Faculty of Computer Science Technische Universitat Dresden 01062 Dresden, Germany toni.dietze@tu-dresden.de Mark-Jan Nederhof School of Computer Science University of St Andrews KY16 9SX, UK Abstract We present an approach to obtain language models from a tree corpus using proba- bilistic regular tree grammars (prtg). Start- ing with a prtg only generating trees from the corpus, the prtg is generalized step by step by merging nonterminals. We focus on bottom-up deterministic prtg to sim- plify the calculations. </chunk></section><section><heading>1 Introduction </heading><chunk>Constituent parsing plays an important role in nat- ural language processing (nlp). One can easily read off a pcfg from a tree corpus and use it for parsing. This might work quite well (Charniak, 1996), but it can be even more fruitful to intro- duce a state behaviour that is not visible in the cor- pus (Klein and Manning, 2003). The Expectation- Maximization Algorithm (Dempster et al., 1977) can be used to train probabilities if the state be- haviour is fixed (Matsuzaki et al., 2005). This can be improved by adapting the state behaviour auto- matically by cleverly splitting and merging states (Petrov et al., 2006). More generally, finding a grammar by exam- ining terminal objects is one of the problems in- vestigated in the field of grammatical inference. There are many results for the string case, e.g., on how to learn deterministic stochastic finite (string) automata from text (Carrasco and Oncina, 1994; Carrasco and Oncina, 1999). For the tree case, there are, e.g., results for identifying function dis- tinguishable regular tree languages from text (Fer- nau, 2002). There is also a generalization of n-grams to trees including smoothing techniques (Rico-Juan et al., 2000; Rico-Juan et al., 2002). The mentioned results for deterministic stochas- tic finite (string) automata were generalized to an algorithm that learns stochastic deterministic tree automata from trees (Carrasco et al., 2001). Given a tree corpus, this approach yields a single gram- mar. The authors experimentally showed that if the corpus is too small, the grammar tends to be too general. In contrast, the split-merge approach of Petrov et al. (2006) produces a sequence of dif- ferent grammars. One can use cross validation to select a grammar from that sequence that suitably abstracts away from the training corpus. Because of the intricate combination of splitting and merg- ing however, the behavior is very difficult to anal- yse theoretically. Our approach is similar to the split-merge procedure in that a sequence of gram- mars is obtained. However, it differs by operat- ing without splitting and relying on merging alone, thereby obtaining a simpler framework. Our goal is to create a sequence of probabilistic regular tree grammars (prtg) from a corpus such that every prtg abstracts away from the corpus more than its predecessors in the sequence (cf. Al- gorithm 1). We start with a prtg that admits no more and no less than the trees in the corpus. The rules of the prtg are then changed step by step to make the grammar more general. We generalize a prtg by merging nonterminals, which means we replace several nonterminals by a single new one. The weights of the resulting prtg are assigned by maximum likelihood estimation on the corpus. To make the approach easier, we only consider bottom-up deterministic prtg. On the one hand, this simplifies our calculations, e.g., the maximum likelihood estimate; on the other hand, this simpli- fies the application of the prtg as language model, since the search for the most probable tree for a given yield does not have to consider several derivations for a single tree. </chunk></section><section><heading>2 Preliminaries </heading><chunk>Let X be a set. The identity relation on X is de- fined as {(x, x) | x X}. An equivalence rela- tion on X is a reflexive, symmetric, and transitive relation () X X. We write x y instead of (x, y) () for x, y X. Note that the iden- tity relation is an equivalence relation. Let x X. The equivalence class of x (induced by ()), de- noted by [x] (or just [x] if () is clear from the context), is defined as {y X | x y}. The quo- tient set of X by (), denoted by X/, is defined as {[x] | x X}. Note that X/ is a partition of X and that for every partition P of X there is an equivalence relation ( ) such that P = X/ . The set of equivalence relations over X forms a complete lattice ordered by set inclusion. We denote the set of the natural numbers includ- ing 0 by N. An alphabet (of symbols) is a finite, non-empty set. A ranked alphabet is an alphabet where we associate a rank rk() N with ev- ery . Let be a ranked alphabet. The set of trees over , denoted by T , is the smallest set T such that (t 1 , . . . , t rk() ) T for every and t 1 , . . . , t rk() T . Additionally, we define T = . Let t = (t 1 , . . . , t k ) T . The set of positions of t, denoted by pos(t), is defined as {} {iw | i {1, . . . , k}, w pos(t i )}. Let w pos(t). The symbol of t at w, denoted by t(w), is defined as if w = , and by t i (w ) if w = iw . The subtree of t at w, denoted by t| w , is defined as t if w = , and by t i | w if w = iw . Let X be a set and f : X R 0 a mapping. The support of f , denoted by supp(f ), is defined as {x X | f (x) = 0}. The size of f , denoted by |f |, is defined as xsupp(f ) f (x). We call f a corpus (over X) if supp(f ) is finite and non- empty. We call f a probability distribution (over X) if |f | = 1. We denote the set of all probabil- ity distributions over X by Pd(X). Note that f may be both a corpus and a probability distribu- tion. Sometimes we refer to the values of a corpus by the word counts. Definition 1. A regular tree grammar (rtg) is a tuple (N, , I, R) where N is an alphabet (of nonterminals), is a ranked alphabet (of terminals) such that N = , I N is a non-empty set (of initial nontermi- nals), and R is a finite set of rules of the form A 0 (A 1 , . . . , A rk() ) where and A 0 , . . . , A rk() N . Our definition above corresponds to a normal form of rtg with regard to a more general definition, which was given by, for example, Gecseg and Steinby (1984, Chapter II, Section 3). Let G = (N, , I, R) be an rtg. For a rule r R of the form A 0 (A 1 , . . . , A rk() ) we define lhs(r) = A 0 and rhs(r) = (A 1 , . . . , A rk() ). We associate with r the rank rk(r) = rk(), hence we may view R as a ranked alphabet if R is non-empty. We call G bottom-up deterministic if rhs(r 1 ) = rhs(r 2 ) implies r 1 = r 2 for every r 1 , r 2 R. Let t T . A derivation tree of G for t is a tree d T R such that pos(d) = pos(t), lhs(d()) I, and for every w pos(t) we have rhs(d(w)) = t(w)(lhs(d(w1)), . . . , lhs(d(w rk(t(w))))). The set of all derivation trees of G for t is denoted by D G (t). The language of trees generated by G, de- noted by G, is defined as {t T | D G (t) = }. Note that if G is bottom-up deterministic, D G (t) has at most one element. We denote this element, if it exists, by d t G . Definition 2. A probabilistic regular tree gram- mar (prtg) is a tuple (G, , ) where G = (N, , I, R) is an rtg, : I [0, 1] is a mapping (initial weights), and : R [0, 1] is a mapping (rule weights). Let P = (G, , ) be a prtg. Terminology for rtg is carried over to prtg, e.g., P is bottom-up de- terministic iff G is bottom-up deterministic. We call P proper if is a probability distribution and rR : lhs(r)=A (r) = 1 for every A N . Let t T and d D P (t). The weight of d (induced by P ), denoted by P (d), is defined as (lhs(d())) wpos(t) (d(w)). The weight of t (induced by P ), denoted by P (t), is defined as dD P (t) P (d). If P is a probability distribu- tion over T , then P is called consistent. Let c be a corpus over T . The canonical rtg of c and the canonical prtg of c are defined as G = (N, , I, R) and P = (G, , ), respectively, where N = {t| w | t supp(c), w pos(t)}, I = supp(c), R = {t| t()(t| 1 , . . . , t| rk(t()) ) | t N }, (t) = c(t) |c| for every t I, and (r) = 1 for every r R. Note that every canonical prtg is bottom-up deter- ministic, proper, and consistent, and that P (t) = (t) for every t supp(P ); hence, P repre- sents the relative frequencies of the trees in c. Let c : X R 0 be a corpus and p Pd(X). The likelihood of c given p is defined as L(c | p) = tsupp(c) p(t) c(t) . Let M Pd(X). The maximum likelihood es- timate from M for c, denoted by mle M (c), is de- fined as mle M (c) = argmax pM L(c | p). If M = Pd(T ), then mle M (c) maps a tree to its relative frequency in c (Prescher, 2004, Theo- rem 1). Hence, the canonical prtg represents the corpus perfectly. Let G = (N, , I, R) be an rtg, M = {P | P = (G, , ) is a consistent prtg}, and c a corpus over T . Then we also write mle G (c) instead of mle M (c). Let G = (N, , I, R) be a bottom-up deter- ministic rtg and c a corpus over T such that supp(c) G. Note that there is exactly one derivation tree d t G of G for every tree t supp(c). Based on G, we derive three corpora from c: c R G : R R 0 : r tsupp(c) c(t) |{w pos(t) | r = d t G (w)}|, c N G : N R 0 : A tsupp(c) c(t) |{w pos(t) | A = lhs(d t G (w))}|, c I G : N R 0 : A tsupp(c) : A=lhs(d t G ()) c(t). Now mle G (c) = (G, , ) where for every A I and r R (Prescher, 2004, Theorem 10): (A) = c I G (A) |c| and (r) = c R G (r) c N G (lhs(r)) . Note that c N G (A) = rR : A=lhs(r) c R G (r). 3 Count-Based State Merging Algorithm 1 summarizes the idea of our approach. We detail the used notions in what follows. Let G = (N, , I, R) be an rtg and () an equivalence relation on N . The G-merger w.r.t. () is the overloaded expression for nonterminals: (A) = [A] for every A N , rules: : A 0 , . . . , A rk() N : (A 0 (A 1 , . . . , A rk() )) Algorithm 1 Count-Based State Merging Input: corpus c over T Output: sequence of bottom-up deterministic prtg P 0 , . . . , P n , some n N, such that supp(P 0 ) . . . supp(P n ) 1: P 0 = (G 0 , 0 , 0 ) canonical prtg of c 2: i 0 3: while there exists a non-trivial G i -merger do 4: BESTMERGER(G i , c) 5: i i + 1 6: G i (G i1 ) 7: let P i be prtg such that mle G i (c) = P i = [A 0 ] ([A 1 ], . . . , [A rk() ]), sets of nonterminals or rules by applying elementwise, and rtg: (G) = (N/, , (I), (R)). Note that G (G), because by replacing each rule r in a derivation tree of G by (r) we get a derivation tree of (G). We call non- trivial, if () is not the identity relation. We say merges A 1 and A 2 , iff A 1 A 2 . We carry over the lattice structure of the set of equivalence relations over N to the set of G-mergers in order to identify minimal and least G-mergers with certain properties. To deal with prtg, we fix a corpus c over T such that supp(c) G. We repeatedly use the maximum likelihood estimate to assign weights to an rtg. That is, the weights are not manipulated during merging itself, but they are used to choose a G-merger: Let be a set of G-mergers. The best G-merger from w.r.t. c is defined as argmax L(c | mle (G) (c)). (1) So far, the presented notions are defined for gen- eral rtg. Now let G = (N, , I, R) be a bottom-up deterministic rtg. Assuming 0 0 = 1, we then have the following, which is proven in Appendix A: L(c | mle G (c)) = AN c I G (A) c I G (A) |c| |c| rR c R G (r) c R G (r) AN c N G (A) c N G (A) . (2) This can be used to make Expression 1 more man- ageable: Let G = (N, , I, R) be a bottom-up deterministic rtg and let be the set of all non- trivial G-mergers such that (G) is bottom-up deterministic. Then Equation 2 gives a more di- rect way of computing the likelihood in Expres- sion 1 without explicitly calculating mle (G) (c). This is the reason for calling our approach count- based, and this is the idea of BESTMERGER in Algorithm 1. Improving efficiency Let and G = (N , , I , R ) = (G). We need c R G , c N G , and c I G to calculate L(c | mle G (c)). Bottom-up determinism allows us to derive d t G for every t supp(c) by replacing every rule r in d t G by (r). Hence, x X : c X G (x ) = xX : x =(x) c X G (x), (3) where X is any of R, N , or I (with or without prime, italic or roman). So we can reuse the cor- pora related to G to calculate L(c | mle G (c)). We may rewrite Expression 1 by dividing the likelihood by L(c | mle G (c)). Then, for many instantiations of many factors in the fraction cancel out. In detail: Let () be the equivalence relation underlying , and N = {A N | |[A]| &gt; 1}, N = (N ), R = {r R | |[r]| &gt; 1}, and R = (R), where () is extended to rules such that r 1 r 2 iff (r 1 ) = (r 2 ) for every r 1 , r 2 R. Then, with Equation 2 and G = (G): L(c | mle G (c)) L(c | mle G (c)) = AN c I G (A) c I G (A) AN c I G (A) c I G (A) rR c R G (r) c R G (r) rR c R G (r) c R G (r) AN c N G (A) c N G (A) AN c N G (A) c N G (A) . (4) Yet, finding the maximum is still expensive. Heuristics Assume N = {A 1 , A 2 }, R = and N I = . Then, using Equation 3, the right-hand side of Equation 4 is equal to f (c N G (A 1 ), c N G (A 2 )) where f (x, y) = x x y y (x + y) x+y . (5) For positive x and y, f (x, y) is monotonically de- creasing (cf. Appendix B). Hence, with our as- sumption, it is best to merge nonterminals with the least counts w.r.t. c N G . This may be used to guide the search for the best merger. Recall that we want to generalize the canonical (p)rtg step by step. We want to take the smallest steps possible w.r.t. loss of likelihood Algorithm 2 Input: rtg G = (N, , I, R) and equivalence re- lation ( 0 ) on N Output: the least G-merger such that (G) is bottom-up deterministic and ( 0 ) () 1: () ( 0 ) 2: while (G) not bottom-up deterministic do 3: find rules r 1 and r 2 in G such that rhs( (r 1 )) = rhs( (r 2 )), but lhs( (r 1 )) = lhs( (r 2 )) 4: let A 1 = lhs(r 1 ) and A 2 = lhs(r 2 ) 5: let ( ) equivalence relation s.t. N/ = N/ \ {[A 1 ], [A 2 ]} {[A 1 ] [A 2 ]} 6: replace () by ( ) (cf. Expression 1), so we consider only minimal non-trivial mergers, i.e., mergers that merge ex- actly two nonterminals. Note that the application of larger mergers may be decomposed into a se- quential application of several minimal non-trivial mergers. We can easily sort the minimal non- trivial mergers by the value of f for the counts of the merged nonterminals. Note that the merger which merges the nonterminals with the lowest counts comes first. We choose a beam width n &gt; 0 and select the n first mergers for further investiga- tion assuming that the best merger is among them. Unfortunately, a minimal non-trivial merger does not necessarily result in a bottom-up deter- ministic rtg, but there is a least (i.e. unique mini- mal) merger such that () ( ) which does (cf. Lemma 2 in Appendix C). We use Algorithm 2 to calculate this merger for each of the n chosen mergers and determine the best merger from the results w.r.t. Equation 4. To restrict the number of considered mergers even more, it may be useful to only merge non- terminals which produce the same terminal. Note that Algorithm 2 preserves this property. </chunk></section><section><heading>4 Practical results and outlook </heading><chunk>So far, our implementation is not competitive. We are only able to train with small corpora. For example, training with 5 900 trees (consisting of 120 000 distinct subtrees) and a beam width of 1 000 takes about 8 days. For some inputs Algo- rithm 2 is very expensive. It remains to be seen whether we can avoid such inputs or reduce the effort by reusing results from previous iterations in Algorithm 1. There is generally a very large number of non- terminals with count 1 in the canonical rtg. Our current heuristics yields the same value for every merger which considers two of such nonterminals. Especially in the first iterations of Algorithm 1 the number of mergers with the same (lowest) heuris- tic value far exceeds the beam width. This means it is arbitrary which mergers lie within the beam. We hope to improve the heuristics by comparing the trees which are produced by the merged non- terminals. Using the generated grammars for parsing, we are currently only able to process sentences con- sisting of words seen in the training data. Even for this limited subset of sentences we are not able to improve precision or recall of brackets (Sekine and Collins, 1997) in comparison to the probabilistic context-free grammar straightforwardly obtained from the corpus. We hope this will improve with a better heuristics. We have restricted our attention to bottom-up deterministic regular tree grammars. Thanks to this, the conceptual framework could remain rel- atively simple. What is unclear at this time is whether the bottom-up determinism per se re- stricts the potential accuracy of the models, in re- lation to the split-merge framework, which allows nondeterministic regular tree grammars. </chunk></section><section><heading>References </heading><chunk>Rafael C. Carrasco and Jose Oncina. 1994. Learn- ing stochastic regular grammars by means of a state merging method. In Rafael C. Carrasco and Jose Oncina, editors, Grammatical Inference and Appli- cations, volume 862 of Lecture Notes in Computer Science, page 139152. Springer Berlin Heidelberg. Rafael C. Carrasco and Jose Oncina. 1999. Learn- ing deterministic regular grammars from stochastic samples in polynomial time. RAIRO Theoretical Informatics and Applications, 33(1):119. Rafael C. Carrasco, Jose Oncina, and Jorge Calera- Rubio. 2001. Stochastic inference of regular tree languages. Machine Learning, 44(1-2):185197. Eugene Charniak. 1996. Tree-bank grammars. In Proc. of AAAI/IAAI 1996, volume 2, pages 1031 1036. A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical So- ciety. Series B (Methodological), 39(1):138. Henning Fernau. 2002. Learning tree languages from text. In Jyrki Kivinen and Robert H. Sloan, editors, Computational Learning Theory, volume 2375 of Lecture Notes in Computer Science, page 153168. Springer Berlin Heidelberg. Ferenc Gecseg and Magnus Steinby. 1984. Tree Au- tomata. Akademiai Kiado, Budapest, Hungary. Dan Klein and Christopher D. Manning. 2003. Ac- curate unlexicalized parsing. In Proc. of ACL 2003, volume 1, pages 423430. Takuya Matsuzaki, Yusuke Miyao, and Junichi Tsujii. 2005. Probabilistic CFG with latent annotations. In Proc. of ACL 2005, pages 7582. Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and inter- pretable tree annotation. In Proc. of COLING/ACL 2006, pages 433440. Detlef Prescher. 2004. A tutorial on the expectation- maximization algorithm including maximum- likelihood estimation and EM training of probabilis- tic context-free grammars. CoRR, abs/cs/0412015. Juan Ramon Rico-Juan, Jorge Calera-Rubio, and Rafael C. Carrasco. 2000. Probabilistic k-testable tree languages. In Arlindo L. Oliveira, editor, Gram- matical Inference: Algorithms and Applications, volume 1891 of Lecture Notes in Computer Science, page 221228. Springer Berlin Heidelberg. Juan Ramon Rico-Juan, Jorge Calera-Rubio, and Rafael C. Carrasco. 2002. Stochastic k-testable tree languages and applications. In Pieter Adriaans, Henning Fernau, and Menno van Zaanen, editors, Grammatical Inference: Algorithms and Applica- tions, volume 2484 of Lecture Notes in Computer Science, page 199212. Springer Berlin Heidelberg. Satoshi Sekine and Michael John Collins. 1997. Evalb. http://nlp.cs.nyu.edu/evalb/. Accessed 2015-03-19. A The likelihood of the maximum likelihood estimate in the bottom-up deterministic case (Equation 2) Let G = (N, , I, R) be a bottom-up deterministic rtg and c a corpus over T such that supp(c) G. Let P = (G, , ) such that P = mle G (c). We can transform L(c | P ) as follows, assuming 0 0 = 1: L(c | P ) = tsupp(c) P (t) c(t) (def. of L) = tsupp(c) dD P (t) P (d) c(t) (def. of P ) = tsupp(c) P (d t G ) c(t) (assumptions for G and c) = tsupp(c) (lhs(d t G ())) wpos(t) (d t G (w)) c(t) (def. of P ) = tsupp(c) (lhs(d t G ())) c(t) wpos(t) (d t G (w)) c(t) (distributivity) = tsupp(c) (lhs(d t G ())) c(t) tsupp(c) wpos(t) (d t G (w)) c(t) (commutativity) = AN tsupp(c) : A=lhs(d t G ()) (A) c(t) rR tsupp(c) wpos(t) : r=d t G (w) (r) c(t) (commutativity) = AN (A) c I G (A) rR (r) tsupp(c) wpos(t) : r=d t G (w) c(t) (b c b d = b c+d , 0 0 = 1, def. of c I G ) = AN (A) c I G (A) rR (r) tsupp(c) c(t)|{wpos(t)|r=d t G (w)}| (distributivity) = AN (A) c I G (A) rR (r) c R G (r) (def. of c R G ) = AN c I G (A) c I G (A) AN |c| c I G (A) rR c R G (r) c R G (r) rR c N G (lhs(r)) c R G (r) (def. of and , comm., distr.) = AN c I G (A) c I G (A) |c| |c| rR c R G (r) c R G (r) AN c N G (A) c N G (A) (b c b d = b c+d , def. of c I G and c N G , comm.) This proves Equation 2. B The function in Equation 5 is monotonically decreasing We may transform Equation 5 as follows: f (x, y) = x x y y (x + y) x+y = x x + y x y x + y y . For positive arguments, the partial derivatives of f are f (x, y) x = ln x x + y x x + y x y x + y y , and f (x, y) y = ln y x + y x x + y x y x + y y . For x, y &gt; 0 the fractions are smaller than one. This means the logarithms are negative, hence the whole terms. So f is monotonically decreasing. C Properties of mergers regarding bottom-up determinism Lemma 1. Let G = (N, , I, R) be an rtg, and let ( 1 ) and ( 2 ) be equivalence relations over N such that 1 (G) and 2 (G) are bottom-up deterministic. Let () = ( 1 ) ( 2 ). Then also (G) is bottom-up deterministic. Proof. Assume (G) is not bottom-up deterministic. Then there are two rules A 0 (A 1 , . . . , A rk() ) and B 0 (B 1 , . . . , B rk() ) in R such that A i B i for every 1 i rk(), but A 0 B 0 . Hence, A i 1 B i and A i 2 B i for every 1 i rk(), and therefore A 0 1 B 0 and A 0 2 B 0 . This implies A 0 B 0 , which is a contradiction, so (G) is bottom-up deterministic. q.e.d. Lemma 2. Let G = (N, , I, R) be an rtg, and let () be an equivalence relation over N . Then there is a least (i.e. unique minimal) ( ) such that () ( ) and (G) is bottom-up deterministic. Proof. Existence: Consider ( ) such that A 1 , A 2 N : A 1 A 2 . Then ( ) satisfies the conditions. Uniqueness: Assume there is a minimal ( ) = ( ) satisfying the conditions. Then, by Lemma 1, ( ) ( ) would also satisfy the conditions, which contradicts that ( ) and ( ) are minimal. Hence, ( ) is unique. q.e.d. </chunk></section></sec_map>