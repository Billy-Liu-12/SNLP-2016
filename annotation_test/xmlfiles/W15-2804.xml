<sec_map><section><chunk>Proceedings of the 2015 Workshop on Vision and Language (VL15), pages 89, Lisbon, Portugal, 18 September 2015. c 2015 Association for Computational Linguistics. Lingusitic Analysis of multi-modal Recurrent Neural Networks Akos K  ad  ar Tilburg University a.kadar@uvt.nl Grzegorz Chrupaa Tilburg University g.a.chrupala@uvt.nl Afra Alishahi Tilburg University a.alishahi@uvt.nl 1 Introduction Recurrent neural networks (RNN) have gained a reputation for beating state-of-the-art results on many NLP benchmarks and for learning repre- sentations of words and larger linguistic units that encode complex syntactic and semantic struc- tures. However, it is not straight-forward to un- derstand how exactly these models make their de- cisions. Recently Li et al. (2015) developed meth- ods to provide linguistically motivated analysis for RNNs trained for sentiment analysis. Here we fo- cus on the analysis of a multi-modal Gated Recur- rent Neural Network (GRU) architecture trained to predict image-vectors - extracted from images us- ing a CNN trained on ImageNet - from their cor- responding descriptions. We propose two meth- ods to explore the importance of grammatical cat- egories with respect to the model and the task. We observe that the model pays most attention to head-words, noun subjects and adjectival modi- fiers and least to determiners and coordinations. </chunk></section><section><heading>2 Method </heading><chunk>We used the IMAGINET model from Chrupaa et al. (2015), trained on the MSCOCO dataset (Lin et al., 2014). It learns visually grounded mean- ing representations from textual and visual input and consists of two GRU pathways, TEXTUAL and VISUAL, with a shared word-embedding ma- trix. The inputs to the model are pairs of captions and their corresponding images. Each sentence is mapped to two sequences of hidden states: one by TEXTUAL and another by VISUAL. At each time- step TEXTUAL predicts the next word in the sen- tence from its current hidden state h T t , while VI- SUAL predicts the image vector from its last hid- den representation h V full . The model is trained us- ing a multi-task objective which combines cross- entropy loss for the word predictions and a mean squared error for the image predictions. We focus our analysis on the hidden states and update-gate activations of VISUAL to assess the impact of syntactic structure on the learned mean- ing representations of sentences used to predict images. For each input sentence of length n, VI- SUAL produces n hidden activations h V 1 , ..., h V n and n update-gate activations z V 1 , ..., z V n . We associate each word in the input sentence with their part-of-speech (POS) and dependency rela- tion (DepRel) labels 1 , and assess the contribution of the (word, POS, DepRel) tuples by estimating the following two scores: 1. d red measures the distance reduction at each step by calculating the cosine distance be- tween the current h t and the last hidden state h full and subtracting it from the previous dis- tance: d t red = d t1 red cos(h t , h full ). The idea is to see how much each word brings the cur- rent state closer to, or further away from, the final interpretation. 2. z mean assigns the average activation of the update-gate z at time step t to the tuple at positions t. The activation function for z is sigmoid, therefore it has values between 0-1. High values of z mean indicate that the model places more importance on the previous tu- ples until t 1 than on the current one at t. </chunk></section><section><heading>3 Results </heading><chunk>We measure d red and z mean for every position in the first 5000 captions from the validation portion of MSCOCO and use them to analyze the importance of both POS and DepRel categories. We only re- port results on the grammatical categories that ap- pear at least 500 times. Figure 1 demonstrates the d red measurements for each word in an example sentence. A large distance between two adjacent 1 We used the dependency parser from Martins et al. (2013) for both the POS and DepRel tags. 8 Figure 1: An example of the impact of each word in the sentence Stop sign with words written on it with a black marker, measured by d red . Left: best retrieved image; middle: reduction of distance from h V full ; right: d red scores for each word. Figure 2: Importance of dependency relations as measured by z mean on the left chart. Contribution of POS categories (middle) and DepRel categories (right) measured by d red . words signals the arrival of a highly informative word. Figure 2 shows the impact of both measures for each grammatical category. The low z mean scores (left) for the roots, adjectival modifiers (amod), direct objects (dobj), noun compound modifiers (nn), noun subjects (nsubj), conjuncts (conj) and objects of prepositions (pobj) suggest that the model remembers words of these cate- gories, while prefers to forget determiners (det), coordinations (cc), prepositions (prep) and auxil- iaries. As indicated by the high d red scores (mid- dle graph), nouns (N), adjectives (JJ) verbs (V) and prepositions (PRP) provide the largest contri- bution to the meaning representations of the sen- tences, while determiners (DET) and conjunctions (C) provide the least. The d red scores for DepRels are in line with the z mean scores; they highlight the importance of nsubj, nn, amod, pobj and dobj. </chunk></section><section><heading>4 Conclusions </heading><chunk>We propose two measures to assess the impact of grammatical categories on sentence representa- tions learned for predicting images. The observed patterns likely reflect the visual salience and in- formativeness of the lexical items associated with each category. They also provide insights into the details of the task e.g.: nouns came out signifi- cantly more important then other content word cat- egories, indicating that predicting the correct enti- ties is the most important aspect of the task. </chunk></section><section><heading>References </heading><chunk>[Chrupaa et al.2015] Grzegorz Chrupaa, Akos K  ad  ar, and Afra Alishahi. 2015. Learning language through pictures. arXiv preprint arXiv:1506.03694. [Li et al.2015] Jiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky. 2015. Visualizing and un- derstanding neural models in nlp. arXiv preprint arXiv:1506.01066. [Lin et al.2014] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ra- manan, Piotr Doll  ar, and C Lawrence Zitnick. 2014. Microsoft coco: Common objects in context. In Computer VisionECCV 2014, pages 740755. Springer. [Martins et al.2013] Andr  e FT Martins, Miguel Almeida, and Noah A Smith. 2013. Turning on the turbo: Fast third-order non-projective turbo parsers. In ACL (2), pages 617622. Citeseer. 9 </chunk></section></sec_map>