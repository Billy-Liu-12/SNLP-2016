<sec_map><section><chunk>Proceedings of CMCL 2015, pages 6878, Denver, Colorado, June 4, 2015. c 2015 Association for Computational Linguistics Utility-based evaluation metrics for models of language acquisition: A look at speech segmentation Lawrence Phillips &amp; Lisa Pearl University of California, Irvine 3151 Social Sciences Plaza Irvine, CA 92697 USA [lawphill, lpearl]@uci.edu Abstract Models of language acquisition are typically evaluated against a gold standard meant to represent adult linguistic knowledge, such as orthographic words for the task of speech seg- mentation. Yet adult knowledge is rarely the target knowledge for the stage of acquisition being modeled, making the gold standard an imperfect evaluation metric. To supplement the gold standard evaluation metric, we pro- pose an alternative utility-based metric that measures whether the acquired knowledge fa- cilitates future learning. We take the task of speech segmentation as a case study, assess- ing previously proposed models of segmen- tation on their ability to generate output that (i) enables creation of language-specific seg- mentation cues that rely on stress patterns, and (ii) assists the subsequent acquisition task of learning word meanings. We find that behav- ior that maximizes gold standard performance does not necessarily maximize the utility of the acquired knowledge, highlighting the ben- efit of multiple evaluation metrics. </chunk></section><section><heading>1 The problem with model evaluation </heading><chunk>Over the past decades, computational modeling has become an increasingly useful tool for studying the ways children acquire their native language. Model- ing allows researchers to explicitly evaluate learning strategies by whether these strategies would enable acquisition success. But how do researchers deter- mine if a particular learning strategy is successful? Traditionally, models have been evaluated against adult linguistic knowledge, typically captured in an explicit gold standard. If the modeled learner suc- ceeds at acquiring this adult linguistic knowledge, then it is said to have succeeded and the learning strategy is held up as a viable option for how the ac- quisition process might work. Gold standard evaluation has two key benefits. First, it provides a uniform measure of evaluation, especially when gold standards are relatively simi- lar across corpora (e.g. orthographic segmentation for speech). Second, this kind of evaluation is typ- ically straightforward to implement for labeled cor- pora, and so is easy to use for model comparison. Still, there are several potential disadvantages to gold standard evaluation. First, the choice of an appropriate gold standard is non-trivial for many linguistic tasks since there is disagreement about what the adult knowledge actually is (e.g., speech segmentation, grammatical categorization, syntac- tic parsing). Second, implementation may require a large amount of time-consuming manual annota- tion (e.g. visual scene labeling for word-object map- ping). Third, and perhaps most importantly, it is un- clear that adult knowledge is the appropriate output for some modeled learning strategies, particularly those that are meant to occur early in acquisition. For example, consider the early stages of speech segmentation that rely only on probabilistic cues. The earliest evidence of speech segmentation comes at six months (Bortfeld, Morgan, Golinkoff, &amp; Rath- bun, 2005) and it appears that probabilistic cues to segmentation, which are language-independent because their implementation does not depend on the specific language being acquired, give way to language-dependent cues between eight and nine 68 months (Johnson &amp; Jusczyk, 2001; Thiessen &amp; Saf- fran, 2003). So, accurate models of this early stage of speech segmentation should output the knowl- edge that a nine-month-old has, and this may differ quite significantly from the knowledge an adult has about how to segment speech. Unfortunately, addressing this last issue with gold standard evaluation is non-trivial. One strategy might be to create a gold standard representing age- appropriate knowledge. However, without empirical data that can identify exactly what childrens knowl- edge at a particular age is, this is difficult. Because of this, few (if any) age-specific gold standards exist for the many acquisition tasks that we wish to evalu- ate learning strategies for. An alternative is to com- pare model results against qualitative patterns that have been reported in the developmental literature. For instance, Lignos (2012) compares his segmen- tation model results against qualitative patterns of over- and undersegmentation reported in diary data (Brown, 1973; Peters, 1983). Still, such compar- isons are often difficult to make since the behavioral data may come from children of different ages than the modeled learners (e.g., the segmentation patterns mentioned above come from two- and three-year- olds while the modeled learners are at most nine months old). So, the essence of the evaluation problem is this: the true target for model output is potentially un- known, but we still wish to evaluate different mod- els. Fortunately for language acquisition modelers, this is exactly the problem faced in computer sci- ence when unsupervised learning algorithms are ap- plied and a gold standard does not exist. There are two main ways a model without a gold standard can be explicitly evaluated (Theodoridis &amp; Koutroubas, 1999; von Luxburg, Williamson, &amp; Guyon, 2011): </chunk></section><section /><section><heading>2. Measure the utility of the output. </heading><chunk>Adding these two evaluation approaches to a lan- guage acquisition modelers toolbox can help allevi- ate the issues surrounding gold standards. Still, the first option of applying expert knowledge is often time intensive, since this typically involves query- ing human knowledge. Moreover, given the key concern about what the output of language acquisi- tion models ought to look like anyway, it is unclear that querying linguistic experts is appropriate. Given this, we focus on measuring the utility of the models output (Mercier, 1912; von Luxburg et al., 2011) to supplement a gold standard analysis. This means we must be more precise about util- ity. Because children acquire linguistic knowledge and then apply that acquired knowledge to learn more of their native language system (Landau &amp; Gleitman, 1985; Morgan &amp; Demuth, 1996), one def- inition of utility for language acquisition is for the model output to facilitate further knowledge acqui- sition. Importantly, determining what future knowl- edge is acquired is often much easier than determin- ing the exact state of that knowledge, as with a gold standard. This is because we often have empiri- cal data about the order in which linguistic knowl- edge is acquired (e.g., language-independent cues to speech segmentation are used to identify language- dependent cues, which are then used to facilitate further segmentation). We can use these empirical data to identify what a models output should be used for, and assess if the acquired knowledge helps the learner acquire the appropriate additional knowl- edge. Then, if a modeled strategy yields this kind of useful knowledge, the modeled strategy should be counted as successful; in contrast, if the acquired knowledge isnt useful (or is actively harmful), then this is a mark of failure. Under this view, a strat- egys utility is equivalent to its ability to prepare the learner for subsequent acquisition tasks. As we will see when we apply this utility-based evaluation to speech segmentation strategies, we may still encounter some familiar evaluation issues. In particular, to evaluate whether a models output prepares a learner for subsequent acquisition tasks, we must have some idea as to what counts as good enough preparation for those subsequent tasks. The simplest answer seems to be that good enough for the subsequent task means that the output for that task is good enough for the next task after that. In some sense then, the best indicator of util- ity would be that the modeled strategy yields adult level knowledge once the entire acquisition process is complete. However, it is currently impractical to model the entire language acquisition process. In- stead, we have to restrict ourselves to smaller seg- 69 ments of the entire process here, two sequential stages. Given the available empirical data, it may be that we have a better idea about what childrens knowledge is for the second stage than we do for the first stage. That is, an age-appropriate gold stan- dard may be available for the subsequent acquisition task. For both utility evaluations we do here, we have something like this for each subsequent task, though it is likely still an imperfect approximation of young childrens knowledge. We note that this utility-based approach differs from a joint inference approach, where two tasks occur simultaneously and information from one task helpfully informs the other (Jones, Johnson, &amp; Frank, 2010; Feldman, Griffiths, Goldwater, &amp; Mor- gan, 2013; Dillon, Dunbar, &amp; Idsardi, 2013; Doyle &amp; Levy, 2013; B  orschinger &amp; Johnson, 2014). Joint inference is appropriate when we have empirical ev- idence that children accomplish both tasks at the same time. In contrast, the utility-based evaluation approach is appropriate when empirical evidence suggests children accomplish tasks sequentially. In this paper, we consider the task of speech segmentation and investigate different ways of as- sessing the utility of previously proposed strate- gies. Notably, these strategies have generally suc- ceeded when evaluated against some version of a gold standard (Phillips &amp; Pearl, in press, 2014a, 2014b). We first briefly review speech segmenta- tion in infants, and then describe the segmentation strategies previously investigated: a Bayesian seg- mentation strategy (Goldwater, Griffiths, &amp; John- son, 2009; Pearl, Goldwater, &amp; Steyvers, 2011) and a subtractive segmentation strategy (Lignos, 2011, 2012). We then evaluate each modeled strategy on two utility measures relating to (i) the creation of language-dependent segmentation cues relying on stress, and (ii) the subsequent acquisition task of learning word meanings. We find that the strategies differ significantly in their ability to identify stress segmentation cues and facilitate word meaning acquisition, with the Bayesian strategy yielding more useful output than the subtractive segmentation strategy. We discuss how these utility results relate to other qualitative patterns, such as oversegmentation, noting that be- havior that maximizes performance against a gold standard does not necessarily maximize the utility of the acquired knowledge for subsequent learning. 2 Speech segmentation strategies One of the first acquisition tasks infants solve is identifying useful units in fluent speech, and the useful units are typically thought of as words. While word boundaries are inconsistently marked by pauses (Cole &amp; Jakimik, 1980), there are sev- eral linguistic cues that infants can leverage (Morgan &amp; Saffran, 1995; Jusczyk, Houston, &amp; Newsome, 1999; Mattys, Jusczyk, &amp; Luce, 1999; Jusczyk, Hohne, &amp; Baumann, 1999; Johnson &amp; Jusczyk, 2001). However, many of these cues are spe- cific to the language being acquired (e.g., whether words of the language generally begin or end with a stressed syllable), and so require infants to iden- tify some words in the language before the language- specific cue can be instantiated. Fortunately, exper- imental evidence suggests that infants can leverage language-independent probabilistic cues to identify that initial seed pool of words (Saffran, Aslin, &amp; Newport, 1996; Aslin, Saffran, &amp; Newport, 1998; Thiessen &amp; Saffran, 2003; Pelucchi, Hay, &amp; Saf- fran, 2009). This had led to significant interest in the early probabilistic segmentation strategies in- fants use (Brent, 1999; Batchelder, 2002; Goldwater et al., 2009; Blanchard, Heinz, &amp; Golinkoff, 2010; Pearl et al., 2011; Lignos, 2011). The two strategies we examine here, a Bayesian strategy (Goldwater et al., 2009; Pearl et al., 2011; Phillips &amp; Pearl, 2014a, 2014b, in press) and a subtractive segmentation strategy (Lignos, 2011, 2012), have two attractive properties. First, they can be implemented so that the modeled learner perceives the input as a sequence of syllables, in accord with the infant speech perception ex- perimental literature (Jusczyk and Derrah (1987); Bertonicini, Bijeljac-Babic, Jusczyk, Kennedy, and Mehler (1988); Bijeljac-Babic, Bertoncini, and Mehler (1993); Eimas (1999) and see Phillips and Pearl (in press) for more detailed discussion). Sec- ond, their syllable-based implementations perform well on English child-directed speech when com- pared against a gold standard (Phillips &amp; Pearl, in press; Lignos, 2011). 70 2.1 Bayesian segmentation The Bayesian strategy 1 has two variants, using either a unigram or bigram generative assumption for how words are generated in fluent speech. The model as- sumes utterances are produced via a Dirichlet pro- cess (Ferguson, 1973). In the unigram case, the identity of the i th word is chosen according to (1): P (w i |w 1 . . . w i1 ) = n i1 (w) + P 0 (w) i 1 + (1) where n i1 is the number of times w appears in the previous i 1 words, is a free parameter, and P 0 is a base distribution specifying the probability that a novel word will consist of the perceptual units x 1 . . . x m (which are syllables here): P 0 (w = x 1 . . . x m ) = j P (x j ) (2) In the bigram case, the model assumes a hierar- chical Dirichlet Process (Teh, Jordan, Beal, &amp; Blei, 2006) and additionally tracks the frequencies of two- word sequences: P (w i |w i1 = w , w 1 . . . w i2 ) = n i1 (w , w) + P 1 (w) n(w ) 1 + (3) P 1 (w i = w) = b i1 (w) + P 0 (w) b 1 + (4) where n i1 (w , w) is the number of times the bi- gram (w , w) has occurred in the first i 1 words, n(w ) is the number of bigrams beginning with word w , b i1 (w) is the number of times w has occurred as the second word of a bigram, b is the total number of bigrams, and and are free parameters. 2 In both the unigram and bigram variants, this gen- erative model implicitly incorporates preferences for smaller lexicons by preferring words that appear fre- quently (due to equations 1, 3, and 4) and prefer- ring shorter words in the lexicon (due to equation </chunk></section><section><heading>1 Called DPSEG by Goldwater et al. (2009). </heading><chunk>2 , , and for all modeled learners were chosen, as in previous work, to maximize the gold standard word token F- score of the unigram and bigram Batch learner: = 1, = 1, = 90. 2). These can be thought of as domain-general par- simony biases. The ideal (Batch) learner for this model is taken from Goldwater et al. (2009) and utilizes Gibbs sampling (Geman &amp; Geman, 1984) to batch pro- cess the entire input corpus, sampling every poten- tial word boundary 20,000 times. This represents the most idealized learner, since Gibbs sampling is guaranteed to converge on the segmentation which best fits the underlying generative model. Because this learner does not include cognitive processing or memory constraints, we also implement one of the constrained learners developed by Pearl et al. (2011) that better approximates actual human in- ference. In addition, that constrained learner was shown to be very successful on English (Phillips &amp; Pearl, in press). The constrained (Online) learner processes data incrementally, but uses a Decayed Markov Chain Monte Carlo algorithm (Marthi, Pasula, Russell, &amp; Peres, 2002) to implement a kind of limited short- term memory. This learner is similar to the Batch learner in that it uses something like Gibbs sam- pling. However, the Online learner does not sample all potential boundaries; instead, it samples s previ- ous boundaries using the decay function b d to se- lect the boundary to sample, where b is the number of potential boundary locations between the bound- ary under consideration b c and the end of the current utterance, while d is the decay rate. Thus, the fur- ther b c is from the end of the current utterance, the less likely it is to be sampled. Larger values of d in- dicate a stricter memory constraint. All results pre- sented here use a set, non-optimized value for d of 1.5, which was chosen to implement a heavy mem- ory constraint (e.g., 90% of samples come from the current utterance, while 96% are in the current or previous utterance). Having sampled a set of bound- aries 3 , the learner can then update its beliefs about those boundaries and subsequently update its lexi- con before moving on to the next utterance. 3 The Online learner samples s = 20, 000 boundaries per ut- terance. For a syllable-based learner, this works out to approx- imately 74% less processing than the Batch learner (Phillips &amp; Pearl, in press). 71 </chunk></section><section><heading>2.2 Subtractive segmentation </heading><chunk>The subtractive segmentation strategy (Lignos, 2011) processes the corpus one utterance a time. It begins by assuming that every utterance is a sin- gle word and then, as it adds vocabulary to its lexicon, it segments out those words when possi- ble. The specific variant we investigate is the beam search subtractive segmenter without stress informa- tion, which is allowed the same segmentation cues as the Bayesian strategy. In cases where there is ambiguity with respect to a particular word boundary, the model considers the two possible segmentations (the one with the bound- ary and the one without) and chooses the one with the higher score. A segmentations score is the ge- ometric mean of the score of each word in the po- tential segmentation. A words score is determined by two factors: (i) its frequency in previous inferred segmentations, and (ii) how often it has been part of potential segmentation that was previously rejected. </chunk></section><section><heading>2.3 Baseline comparison: Random oracle </heading><chunk>We additionally examine a random oracle base- line (Lignos, 2012). This strategy makes guesses about word boundaries as a series of Bernoulli trials, where the probability of a boundary p b is set to the true probability according to the gold standard. Al- though this is unrealistic as an actual strategy infants use because it assumes knowledge of word bound- ary frequency, this strategy serves as a best-case sce- nario for what random guessing might achieve. </chunk></section><section><heading>3 Previous results with the gold standard </heading><chunk>These strategies were evaluated against a gold stan- dard in English by using the UCI Brent Syllables corpus of English child-directed speech (Phillips &amp; Pearl, in press) available through CHILDES (MacWhinney, 2000), which contains 28,391 utter- ances of speech directed to American English chil- dren between six and nine months old. Word token F-scores (shown in Table 1) provide a convenient summary statistic for segmentation model evalua- tion, where the F-score is the harmonic mean of precision and recall. So, the F-score balances how accurate the set of identified words is (precision = # correctly identif ied # identif ied ) with how complete the set of identified words is (recall = #correctly identif ied # true ). Word Token F-scores Batch (Uni) 0.531 Online (Uni) 0.551 Batch (Bi) 0.771 Online (Bi) 0.863 Subtractive Seg 0.879 Random 0.588 Table 1: Word token F-score results on the UCI Brent Syllables corpus as reported by Phillips and Pearl (in press) for the Bayesian learners (Batch vs. Online, Un- igram vs. Bigram), the subtractive segmenter, and the random oracle baseline. Based on this evaluation metric, the subtractive segmenter performs the best, though the Bayesian Online bigram learner does nearly as well. Notably, the Bayesian unigram learners suffer significantly in comparison, doing worse than even the random ora- cle baseline. This suggests the unigram assumption is harmful if the goal is to generate the adult knowl- edge represented in the gold standard. </chunk></section><section><heading>4 Stress cue identification </heading><chunk>A language-dependent segmentation cue that in- fants use fairly early is their native languages pre- dominant stress pattern (Jusczyk, Houston, &amp; New- some, 1999; Morgan &amp; Saffran, 1995). In particu- lar, while seven-month-olds rely more on probabilis- tic cues, nine-month-olds rely more on stress-based cues (Johnson &amp; Jusczyk, 2001; Thiessen &amp; Saf- fran, 2003). So, while probabilistic cues and stress- based cues may be used jointly (Lignos, 2012; Doyle &amp; Levy, 2013), infants likely use probabilistic cues only until enough evidence has been accumulated to identify the language-dependent stress cue. In par- ticular, infants want to identify whether words tend to begin with stressed syllables or end with stressed syllables, since that can provide a convenient heuris- tic for identifying word boundaries. For example, if words begin with stressed syllables, then a stressed syllable signals that the previous word has ended and a new word has begun. Given this, a measure of the utility of a segmenta- tion strategys output is whether the generated lex- icon yields the appropriate stress cue. To deter- mine this, we must first identify where stressed syl- lables are in the English child-directed data. Be- cause the UCI Brent Syllables corpus does not mark stress, we make use of the English Callhome Lexi- con (Kingsbury, Strassel, McLemore, &amp; MacIntyre, 72 1997) to identify the main stress in words. For child-register words not found in standard dictio- naries (like moosha), we manually coded the stress when the words were familiar enough to us to de- duce the stress pattern. If a word was not familiar enough for us to be confident about its stress pattern (e.g., bonino), we ignored it for the purposes of this analysis. All words in the analyses presented below were given their dictionary stress patterns. In order to better approximate the stress of actual utterances, monosyllabic words were left unstressed. Table 2 presents the stress pattern of the bisyl- labic word types in each learners lexicon. 4 Our corpus of English child-directed speech has 1344 unique bisyllabic words with 89.9% beginning with a stressed syllable (SW: b  aby) and 10.1% ending with a stressed syllable (WS: ball  oon), as shown by the Adult Seg row. For the learner to correctly in- fer that English words tend to be stress-initial, the inferred lexicon should have more words with the stress-initial pattern. This serves as an approximate age-appropriate gold standard, since the goal is to match the qualitative stress distribution pattern that would yield the stress cue English nine-month-olds use (i.e., stressed syllables begin words). SW WS Adult Seg 89.9% 10.1% Batch (Uni) 80.0% 20.0% Online (Uni) 80.8% 19.2% Batch (Bi) 80.4% 19.6% Online (Bi) 79.6% 20.4% Subtractive Seg 59.4% 40.6% Random 68.5% 31.5% Table 2: Stress pattern results for all learners on bisyl- labic word types. Percentages are calculated out of all bisyllabic words identified by the model. All Bayesian learners capture the qualitative stress pattern, and come fairly close to capturing the quantitative distribution, with 79.6% - 80.8% of the bisyllabic word types having word-initial stress (SW). The subtractive segmenter weakly shows the same pattern, identifying more bisyllabic word types 4 We note that we calculate this over word types rather than word tokens, since learners may ignore frequency when de- ciding how far to extend generalizations (Yang, 2005; Perfors, Ransom, &amp; Navarro, 2014). with word-initial stress (59.4% SW). The random oracle baseline actually produces a stronger word- initial bias than the subtractive segmenter (68.5% SW). This suggests an advantage for the Bayesian strategy when it comes to inferring the English stress segmentation cue from the bisyllabic words in the inferred lexicon. When we turn to trisyllabic words, however, the Bayesian strategy no longer does better both strate- gies fail to capture the qualitative stress pattern (as does the random oracle baseline). Table 3 shows the results across the 345 trisyllabic word types. The qualitative pattern in the true distribution is similar to the bisyllabic words (though the distribution is less pronounced), with the majority (69.2%) having initial stress. However, all strategies yield a pref- erence for word-medial stress in trisyllabic words (37.4% - 50.1%). Interestingly, if a learner was at- tempting to infer a segmentation cue, word-medial stress actually doesnt yield an obvious cue there is no word boundary either immediately before or immediately after the stressed syllable. So, even if the inferred stress pattern is incorrect for trisyllabic words, it may not actually harm a learner who is looking for segmentation cues it just fails to help. SWW WSW WWS Adult Seg 69.2% 2.2% 28.6% Batch (Uni) 22.7% 50.1% 27.2% Online (Uni) 22.8% 49.2% 28.0% Batch (Bi) 22.0% 46.6% 31.4% Online (Bi) 23.7% 47.7% 28.6% Subtractive Seg 19.1% 48.7% 32.2% Random 28.6% 37.4% 34.0% Table 3: Stress pattern results for all learners on trisyl- labic word types. Percentages are calculated out of all trisyllabic words identified by the model. More generally, these results suggest that the word token F-score is not necessarily correlated with knowledge utility, at least when it comes to inferring language-dependent stress-based cues to segmen- tation. For instance, the Online Bayesian bigram learner and the subtractive segmenter have similar word token F-scores (0.863 vs. 0.879), but gener- ate quantitatively different predictions for the En- glish stress-based segmentation cue. Similarly, the Bayesian unigram learners have far lower word to- 73 ken F-scores (0.531-0.551), yet yield correct predic- tions for the English stress cue, based on bisyllabic word types. If any of these strategies are the ones infants use, then we would predict that infants in the early stages of segmentation have different expectations about the prevalent stress pattern for bisyllabic vs. trisyl- labic words in English. This is something that can be verified experimentally. However, we do note that the current analyses leading to this prediction are based on particular assumptions about how ac- curately infants perceive stress in their input (here, perfectly accurately), and so future analyses should consider other cognitively plausible instantiations of infant stress perception. In addition, while this stress analysis was only applied to English here, it is worthwhile to do so for other languages that vary in how their stress system operates. </chunk></section><section><heading>5 Word meaning </heading><chunk>A task that infants tackle after they are somewhat able to segment the speech stream is learning word meaning. In particular, word meaning learning be- gins as early as six months (Tincoff &amp; Jusczyk, 1999, 2012; Bergelson &amp; Swingley, 2012), focusing on concrete items in the learners environment like apple and hand. So, another test of a segmentation strategys utility is whether the lexicon it generates facilitates this kind of early word-object mapping. 5.1 A model of early word-object mapping Drawing on the intuition that early word-object mapping could leverage cross-situational learning, Frank, Goodman, and Tenenbaum (2009) developed a Bayesian learning strategy for early word-object mapping. The modeled learner infers a referential lexicon of word-object mappings based on the utter- ances spoken and the set of objects visually salient in the environment. In the generative model shown in the plate diagram in Figure 1, the learner assumes there are some objects (O) in the environment, and the speaker intends to refer to some subset of them (I) using words. The speaker draws words from the referential lexicon (L) to refer to those intended ob- jects, with non-referential words also occurring in the utterances with some probability. So, based on a set of situations (S) containing observable utterances comprised of words (W) and sets of visually salient objects (O), the modeled learner can infer the refer- ential lexicon L of word-object mappings as well as the specific intended objects (I). Figure 1: Plate diagram of the Frank et al. (2009) word- object mapping generative model. This model vastly outperformed other word- object mapping strategies on a sample of English child-directed speech, yielding a referential lexicon that was significantly more accurate (higher preci- sion) compared to other strategies. High lexicon pre- cision is likely more important than high lexical re- call for early word-object mapping because this is only the first stage of word meaning learning. So, it is better to have a small set of reliable word-object mappings than a large set of unreliable word-object mappings if the learner is using these mappings to bootstrap future word meaning acquisition. Notably, the model assumes the utterances are al- ready segmented into words. So, a natural evalu- ation measure for segmentation strategies is to use the inferred segmentation of the utterances, rather than the adult orthographic segmentation used in the original Frank et al. (2009) demonstration. We can then see if the mapping strategy is still able to iden- tify a reliable referential lexicon. As with the pre- vious utility evaluation, the desired output (a lexi- con of word-object mappings) is a gold standard, in this case based on how adults construct word-object mappings. However, because the inferred mappings focus on concrete objects infants are known to learn mappings for, we believe it is at least an approxima- tion of an age-appropriate gold standard. </chunk></section><section><heading>5.2 Segmentation strategy evaluation </heading><chunk>Originally, the word-object model was evaluated on a small subset of 700 utterances from the Rollins 74 corpus from CHILDES (MacWhinney, 2000) which was labeled with visually salient objects (O in the Figure 1). We used this corpus to evaluate the seg- mentation strategies. We first trained the segmenta- tion strategies on the 28,391 utterances of the UCI Brent Syllables corpus (Phillips &amp; Pearl, in press) so that the modeled learners using those strategies could infer a lexicon of word forms with associated probabilities of occurrence. We then applied the re- sulting knowledge to the Rollins corpus subset, let- ting each strategy segment those utterances as best it could, given the knowledge it had inferred from the training set. The word-object mapping model was then applied with the inferred segmentations as part of the observed input (W). Due to the stochas- tic nature of the inference process, we repeated this process five times and present averaged results. We present lexical precision scores due to the im- portance of inferring high quality mappings during early word meaning learning. However, to mea- sure precision we need to identify what constitutes a correct mapping. Frank et al. (2009) created a gold standard referential lexicon by hand and we fol- low their basic guidelines in creating our own. One consideration when dealing with non-adult segmentation is the possibility of legitimate map- pings between non-words and objects. For instance, the undersegmenation abunny might reasonably be mapped onto the object BUNNY. Our gold standard referential lexicon allows these combinations of de- terminers and content words as legitimate words for an object to be mapped to, unlike the original Frank et al. (2009) study. In contrast, an oversge- mentation like du or ckie for duckie was not allowed as a correct word for the object DUCK. This is because neither unit (du or ckie) captures the true word form. For instance, it isnt good if the child thinks every instance of /ki/ key, ckie, etc. refers to DUCK. Given this, oversegmention errors are worse than undersegmentations, since they damage the ability to form a reasonable word-object map- ping. </chunk></section><section><heading>5.3 Results </heading><chunk>Table 4 presents the evaluation results for all mod- eled learners, including the segmentation word to- ken F-scores, the rate of oversegmentation errors, and the referential lexicon precision scores. We additionally show the word-object mapping results based on the adult orthographic segmentation as an upper-bound comparison. Segmentation Mapping F-score Overseg. Lex. prec. Adult Seg 1.000 0.0% 0.583 Batch (Uni) 0.514 1.7% 0.427 Online (Uni) 0.524 9.0% 0.458 Batch (Bi) 0.746 13.8% 0.544 Online (Bi) 0.813 44.8% 0.347 Subtractive Seg 0.833 90.7% 0.336 Random 0.576 53.2% 0.406 Table 4: Average results over five runs from all modeled learners, showing word token F-score segmentation per- formance, the rate of oversegmentation errors, and the precision of the inferred referential lexicon. First, we can see that using the adult segmenta- tion yields a referential lexicon with precision 0.583. While this may not seem very high, it is far more precise than other competing word-object mapping strategies investigated by Frank et al. (2009), which had precision scores between 0.06-0.15. When we turn to the segmentation performance of the learners, we see similar results on the Rollins corpus as we found before. The Bayesian unigram learners have F-scores around 50% (0.514-0.524), which is worse than the random oracle guesser (0.576). In contrast, the Bayesian bigram learners fare much better (0.746-0.813), with almost as good token F-score performance as the subtractive seg- menter (0.833). Interestingly, we see vast differences in the rate of oversegmentation errors. The subtractive seg- menters errors are nearly always oversegmentations (90.7%). The Online Bayesian bigram learner and the random oracle guesser have about half their er- rors as oversegmentations (44.8%, 53.2%), while the remaining Bayesian learners have very few overseg- mentation errors (1.7%-13.8%). Given how damag- ing oversegmentation errors can be for word-object mapping, we might expect high oversegmentation rates to take their toll despite highly accurate word segmentation. This is precisely what we find for the subtractive segmenter: it has the highest token F-score for seg- mentation but the worst lexical precision for word- 75 object mappings (0.336). The Online Bayesian bi- gram learner suffers in lexical precision for a simi- lar reason (0.347), though its oversegmentation bias is lower. Notably, both these learners generate ref- erential lexicons that are worse than what can be achieved by best-case random guessing (0.406). In contrast, the Bayesian learners with very few over- segmentations fare better (0.427-0.544). Given that the best possible performance for lexical precision was 0.583, 0.544 seems quite respectable. When we examine the mapping errors made by each modeled learner (samples shown in Table 5), the detrimental impact of oversegmentation is more apparent. Notably, many words in English child- directed speech are made up of two syllables (e.g. birdie, bunny, piggy). If these words are overseg- mented, the model cannot create a lexical mapping from birdie to its object and instead tends to map both bir and die to the same object. The Bayesian unigram learners never produce these types of over- segmentations for the concrete nouns which the model is attempting to learn (they do, however, pro- duce oversegmentations such as hip-hop segmented as hip and hop). In contrast, the Bayesian bigram learners, the subtractive segmenter, and random or- acle learner generate these errors for words that oth- erwise might have been learned correctly (between 6.4% - 10.2% of all inferred mappings). Word Object % Over Err Batch (Bi) bu(nnies) RABBIT 6.4% (bu)nnies RABBIT Online (Bi) (bir)die DUCK 10.2% bir(die) DUCK Subtr. Seg bu(nnies) RABBIT 8.1% bir(die) DUCK Random pi(ggy) PIG 8.5% bir(die) DUCK Table 5: Example oversegmentation errors from the four learners that make them for items in the referential lexi- con. Oversegmented lexical items are shown in bold with the remainder of the correct word in parentheses. The percentage of all lexical mappings that were incorrect be- cause of oversegmentation is also given. More generally, similar to the stress utility eval- uation, this word-object mapping utility evaluation reveals that segmentations which are more cor- rect are not necessarily more useful. In particular, having a non-detrimental segmentation error pattern (i.e., preferring undersegmentation to oversegmen- tation) may matter more than having a more accu- rate segmentation for the early stages of both speech segmentation and word-object mapping. However, these results do not necessarily indicate that the on- line bigram Bayesian or subtractive segmentation strategies are not used by infants. It simply means that if they are, oversegmentations may need to be corrected before word-object mapping can success- fully get off the ground. We note that the particular parameters used for the Bayesian strategy can influ- ence the rates of over- and undersegmentation. Be- cause we selected parameters that optimized word token F-score performance, it may be that parame- ters can be optimized for word-object mapping (and also stress cue induction). </chunk></section><section><heading>6 Conclusion </heading><chunk>We have presented two concrete suggestions for evaluating the utility of speech segmentation strate- gies, capitalizing on the bootstrapping nature of lan- guage acquisition. This utility-focused evaluation approach demonstrates that a more accurate seg- mentation when compared to a gold standard does not equate to a more useful segmentation for subse- quent language acquisition processes. Notably, the types of errors made may significantly impact the utility of the inferred lexicon, so it is worthwhile to analyze not just what is right about a models output but also exactly what is wrong. This is a specific demonstration of a larger methodological point about how to evaluate unsupervised models of language acquisition. While gold standard evalua- tion can tell us whether a strategy reproduces adult knowledge, measuring model output utility can indi- cate what strategies are actually useful for learners. Acknowledgments We would like to thank Michael Frank, Ulrike von Luxburg, Sharon Goldwater, Stella Frank, and the members of the Computation of Language Labora- tory at UCI for their helpful discussion and com- ments. This work was supported by a Jean-Claude Falmagne Research Award to the first author from the department of Cognitive Sciences at UCI. 76 References Aslin, R., Saffran, J., &amp; Newport, E. (1998). Com- putation of conditional probability statistics by 8-month-old infants. Psychological Sci- ence, 9, 321324. Batchelder, E. (2002). Bootstrapping the lexicon: A computational model of infant speech seg- mentation. Cognition, 83(2), 167206. Bergelson, E., &amp; Swingley, D. (2012). At 69 months, human infants know the meanings of many common nouns. Proceedings of the National Academy of Sciences, 109(9), 3253 3258. Bertonicini, J., Bijeljac-Babic, R., Jusczyk, P., Kennedy, L., &amp; Mehler, J. (1988). An in- vestigation of young infants perceptual rep- resentations of speech sounds. Journal of Ex- perimental Psychology, 117(1), 2133. Bijeljac-Babic, R., Bertoncini, J., &amp; Mehler, J. (1993). How do 4-day-old infants categorize multisyllabic utterances? Developmental Psy- chology, 29(4), 711721. Blanchard, D., Heinz, J., &amp; Golinkoff, R. (2010). Modeling the contribution of phonotactic cues to the problem of word segmentation. Journal of child language, 37, 487511. B  orschinger, B., &amp; Johnson, M. (2014). Exploring the role of stress in Bayesian word segmen- tation using adaptor grammars. Transactions of the Association for Computational Linguis- tics, 2(1), 93-104. Bortfeld, H., Morgan, J., Golinkoff, R., &amp; Rath- bun, K. (2005). Mommy and me: Familiar names help launch babies into speech-stream segmentation. Psychological Science, 16(4), 298304. Brent, M. (1999). An efficient, probabilistically sound algorithm for segmentation and word discovery. Machine Learning, 34, 71105. Brown, R. (1973). A first language: The early stages. Harvard University Press. Cole, R., &amp; Jakimik, J. (1980). Perception and pro- duction of fluent speech. In R. Cole (Ed.), (pp. 133163). Hillsdale, NJ: Erlbaum. Dillon, B., Dunbar, E., &amp; Idsardi, W. (2013). A single-stage approach to learning phonologi- cal categories: Insights from inuktitut. Cogni- tive Science, 37(2), 344377. Doyle, G., &amp; Levy, R. (2013). Combining multiple information types in bayesian word segmen- tation. In Proceedings of naacl-hlt 2013 (pp. 117126). Eimas, P. (1999). Segmental and syllabic represen- tations in the perception of speech by young infants. Journal of the Acoustical Society of America, 105(3), 19011911. Feldman, N., Griffiths, T., Goldwater, S., &amp; Morgan, J. (2013). A role for the developing lexicon in phonetic category acquisition. Psychological Review, 120(4), 751778. Ferguson, T. (1973). A bayesian analysis of some nonparametric problems. Annals of Statistics, 1(2), 209230. Frank, M., Goodman, N., &amp; Tenenbaum, J. (2009). Using speakers referential intentions to model early cross-situational word learn- ing. Psychological Science, 20, 579585. Geman, S., &amp; Geman, D. (1984). Stochastic re- laxation, gibbs distributions, and the bayesian restoration of images. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 6, 721741. Goldwater, S., Griffiths, T., &amp; Johnson, M. (2009). A bayesian framework for word segmenta- tion. Cognition, 112(1), 2154. Johnson, E., &amp; Jusczyk, P. (2001). Word segmen- tation by 8-month-olds: When speech cues count more than statistics. Journal of Mem- ory and Language, 44, 548567. Jones, B., Johnson, M., &amp; Frank, M. (2010). Learn- ing words and their meanings from unseg- mented child-directed speech. In Human lan- guage technologies: The 2010 annual con- ference of the north american chapter of the association for computational linguistics (pp. 501509). Jusczyk, P., &amp; Derrah, C. (1987). Representation of speech sounds by young infants. Develop- mental Psychology, 23(5), 648654. Jusczyk, P., Hohne, E., &amp; Baumann, A. (1999). In- fants sensitivity to allphonic cues for word segmentation. Perception and Psychophysics, 61, 14651476. Jusczyk, P., Houston, D., &amp; Newsome, M. (1999). The beginnings of word segmentation in 77 english-learning infants. Cognitive Psychol- ogy, 39, 159207. Kingsbury, P., Strassel, S., McLemore, C., &amp; Mac- Intyre, R. (1997). Callhome american en- glish lexicon (pronlex). Linguistic Data Con- sortium. Landau, B., &amp; Gleitman, L. (1985). Language and experience. Cambridge, MA: Harvard Univer- sity Press. Lignos, C. (2011). Modeling infant word segmen- tation. In Proceedings of the Fifteenth Con- ference on Computational Natural Language Learning (pp. 2938). Lignos, C. (2012). Infant word segmentation: An in- cremental, integrated model. In Proceedings of the 30th west coast conference on formal linguistics (pp. 237247). MacWhinney, B. (2000). The childes project: Tools for analyzing talk (3rd ed.). Mahwah, NJ: Lawrence Erlbaum Associates. Marthi, B., Pasula, H., Russell, S., &amp; Peres, Y. (2002). Decayed mcmc filtering. In Proceed- ings of 18th uai (pp. 319326). Mattys, S., Jusczyk, P., &amp; Luce, P. (1999). Phono- tactic and prosodic effects on word segmen- tation in infants. Cognitive Psychology, 38, 465494. Mercier, C. (1912). A new logic. London: William Heineman. Morgan, J., &amp; Demuth, K. (1996). Signal to syn- tax: Bootstrapping from speech to grammar in early acquisition. Lawrence Erlbaum As- sociates, Inc. Morgan, J., &amp; Saffran, J. (1995). Emerging inte- gration of sequential and suprasegmental in- formation in preverbal speech segmentation. Child Development, 66(4), 911936. Pearl, L., Goldwater, S., &amp; Steyvers, M. (2011). On- line learning mechanisms for bayesian models of word segmentation. Research on Language and Computation, 8(2), 107132. (special is- sue on computational models of language ac- quisition) Pelucchi, B., Hay, J., &amp; Saffran, J. (2009). Learn- ing in reverse: Eight-month-old infants track backward transitional probabilities. Cogni- tion, 113, 244247. Perfors, A., Ransom, K., &amp; Navarro, D. (2014). People ignore token frequency when deciding how widely to generalize. In Proceedings of the 36th Annual Conference of the Cognitive Science Society (pp. 27592764). Peters, A. (1983). The units of language acquisition. New York: Cambridge University Press. Phillips, L., &amp; Pearl, L. (2014a). Bayesian infer- ence as a cross-linguistic word segmentation strategy: Always learning useful things. In Proceedings of the Computational and Cogni- tive Models of Language Acquisition and Lan- guage Processing Workshop. Phillips, L., &amp; Pearl, L. (2014b). Bayesian inference as a viable cross-linguistic word segmentation strategy: Its about whats useful. In Proceed- ings of the 36th annual conference of the cog- nitive science society. Phillips, L., &amp; Pearl, L. (in press). The utility of cognitive plausibility in language acquisition modeling: Evidence from word segmentation. Cognitive Science. Saffran, J., Aslin, R., &amp; Newport, E. (1996). Statisti- cal learning by 8-month-old infants. Science, 274, 19261928. Teh, Y., Jordan, M., Beal, M., &amp; Blei, D. (2006). Heirarchical dirichlet processes. Jour- nal of the American Statistical Association, 101(476), 15661581. Theodoridis, S., &amp; Koutroubas, K. (1999). Pattern recognition. Academic Press. Thiessen, E., &amp; Saffran, J. (2003). When cues col- lide: Use of stress and statistical cues to word boundaries by 7- to 9-month-old infants. De- velopmental Psychology, 39(4), 706716. Tincoff, R., &amp; Jusczyk, P. W. (1999). Some be- ginnings of word comprehension in 6-month- olds. Psychological Science, 10(2), 172175. Tincoff, R., &amp; Jusczyk, P. W. (2012). Six-month- olds comprehend words that refer to parts of the body. Infancy, 17(4), 432444. von Luxburg, U., Williamson, R., &amp; Guyon, I. (2011). Clustering: Science or art? In JMLR Workshop and Conference Proceedings 27 (pp. 6579). (Workshop on Unsupervised Learning and Transfer Learning) Yang, C. (2005). On productivity. Linguistic varia- tion yearbook, 5(1), 265302. 78 </chunk></section></sec_map>