<sec_map><section><chunk>Proceedings of the SIGDIAL 2015 Conference, pages 144148, Prague, Czech Republic, 2-4 September 2015. c 2015 Association for Computational Linguistics Information Theoretical and Statistical Features for Intrinsic Plagiarism Detection Rashedur Rahman IRT-SystemX &amp; LIMSI-CNRS Paris-Sud University rashedur.rahman@limsi.fr Abstract In this paper we present some information theoretical and statistical features includ- ing function word skip n-grams for detect- ing plagiarism intrinsically. We train a bi- nary classifier with different feature sets and observe their performances. Basically, we propose a set of 36 features for clas- sifying plagiarized and non-plagiarized texts in suspicious documents. Our ex- periment finds that entropy, relative en- tropy and correlation coefficient of func- tion word skip n-gram frequency profiles are very effective features. The proposed feature set achieves F-Score of 85.10%. </chunk></section><section><heading>1 Introduction </heading><chunk>Extrinsic plagiarism detection attempts to detect whether a document is plagiarised relative to refer- ence documents. IPD (intrinsic plagiarism detec- tion), which is relatively new, detects the plagia- rised section(s) in a suspicious document without using any reference document. The basic hypoth- esis behind IPD is different writers have their own styles and they maintain these in their writings consciously or subconsciously. Sometimes it is very difficult to define the reference set for the task of external plagiarism detection. Additionally, the source of the plagiarized text may not be available in digitized format. Therefore, researchers are try- ing to answer whether it is possible to detect pla- giarism without using any reference. In this paper, we investigate some information theoretical and statistical measurements for IPD as a binary classification task. A set of 36 features has been proposed for classifying plagiarized and non-plagiarized segments in the suspicious docu- ments. We use the PAN-PC-11 (Potthast et al., 2010) corpus compiled for IPD task. The PAN corpus is artificially plagiarised and it provides a meta-file mentioning the offsets of plagiarised and non-plagiarized parts for each suspicious doc- ument. We consider that each suspicious docu- ment is written by single author and it is either partially plagiarised or not plagiarised and we try to identify the text-segments that differ in writing style compared to the whole document. We train an SMO (Platt, 1998) classifier in Weka3.6 (Hall et al., 2009) by using 10 fold cross-validation. Then the classification performances are observed with different feature sets according to the stan- dard precision, recall and F-score. The next sections are organized as follows: sec- tion 2 discusses related works and section 3 briefly describes information theoretical and statistical features. The text segmentation and windowing process is summarized in section 4 while the ex- perimental framework and baseline feature sets are discussed in section 5. Section 6 compares the classification performances with different feature sets and finally, the paper concludes in section 7. </chunk></section><section><heading>2 Related Work </heading><chunk>A series of regular studies on plagiarism detec- tion were started following the first international competition for plagiarism detection, the PAN 1 workshop in 2009. Potthast et al. (2009) pro- vides an overview on PAN09 including the cor- pus design for plagiarism detection, quality mea- surements and the methods of plagiarism detection developed by the participants. Zu Eissen and Stein (2006) proposed the first method for IPD and presented a taxonomy of pla- giarism with methods for analysis. They also pro- posed some features including average sentence length, part-of-speech features, average stopword number and averaged word frequency class for quantifying the writing style. Some researchers used character n-gram profiles for the task of IPD 1 http://pan.webis.de/ 144 (Stamatatos, 2009; Kestemont et al., 2011). Ober- reuter et al. (2011) proposed word n-gram based method and they assumed that different writers use different sets of words that they repeat fre- quently. Tschuggnall and Specht (2012) proposed the Plag-Inn algorithm that finds plagiarized sen- tences in a suspicious document by comparing grammar trees of the sentences. Stamatatos (2009) introduced sliding window and proposed a distance function for calculating the dissimilarity between two texts based on a charac- ter tri-gram profile. Stamatatos (2011) employed n-grams of function word sequence with different lengths and found significant impact to distinguish between plagiarised and non-plagiarized texts. We employ function words differently as skip n-gram profiles for measuring entropy, relative entropy and correlation coefficient as discussed in Section 5.2. Stein et al. (2011) employed unmasking tech- nique and proposed a set of features of different types for example POS, function words etc for in- trinsic plagiarism analysis. Seaward and Matwin (2009) and Chuda and Uhlik (2011) proposed compression based meth- ods for IPD. They measured the Kolmogorov com- plexity of the distributions of different parts-of- speech and word classes in the sentences. For calculating the complexity a binary string is gen- erated for each distribution and later the string is compressed by a compression algorithm. </chunk></section><section><heading>3 Information Theoretical and Statistical Features </heading><chunk>Shannon Entropy (Shannon, 1948) has a great im- pact on communication theory or theory of infor- mation transmission, it measures the uncertainty of a random variable. Mathematically, entropy is defined as in equation (1). H(X) = n i=1 p(x i ) log 2 (p(x i )) (1) KLD (p||q) = xX p(x) log 2 p(x) q(x) (2) r = 1 n 1 n i=1 X i X s X Y i Y s Y (3) We measure entropy of n-gram frequency profile generated from each text-window (X) for quan- tifying the writing style. Manning and Schutze (1999) measured the distance between two prob- ability distributions by using Relative entropy or Kullback-Leibler divergence (KLD) which is cal- culated by using the equation (2). The Pearson correlation coefficient (Pearson, 1920) or simply correlation coefficient measures the linear corre- lation between two samples that is calculated by the equation (3). Since the task of IPD does not use any reference document we require a robust method for comparing small sections of the docu- ment relative to the whole document under ques- tion. Measuring the relative entropy and correla- tion coefficient between a small section and the rest of the document are possible methods. We use the frequency profiles of n-grams generated from the individual text-window (X) and the com- plete suspicious document (Y) separately for cal- culating relative entropy and correlation coeffi- cient. The probability distributions of n-gram fre- quencies (P and Q) is calculated from n-gram fre- quency profiles (from X and Y) for measuring the relative entropy. </chunk></section><section><heading>4 Text Segmentation and windowing </heading><chunk>To define the small sections of text for comparison to the rest of the document, we experiment with window of different lengths (1000, 2000, 5000 characters). To prepare the corpus for training and testing to support this additional experimenta- tion, we separate plagiarised and non-plagiarized sections of the documents in the corpus accord- ing to the offsets (as indicated in the meta-file). By doing this we can guarantee that the smaller texts we generate are still accurately annotated as to whether the content is plagiarised or not. The whole procedure is illustrated in figure 1. xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz 1 2 m 1 2 n xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz xyz ... ... ... ... ... ... ... ... Figure 1: Text segmentation and windowing 145 5 Experimental Framework and Feature Sets This section illustrates the experimental frame- work of IPD task by combining the preprocessing and classification tools, the framework is graphi- cally described in figure 2. After extracting and windowing the corpus, we calculate different fea- ture values for generating the feature vectors. Be- fore calculating the features, several text prepro- cessing tasks, for example, tokenizing, sentence detection and POS-tagging are employed. We gen- Figure 2: Experimental framework erate several feature vectors for different baseline feature sets and proposed feature set. Then a clas- sifier model is trained with the feature sets, we train SMO classifier with 10 fold cross valida- tion in Weka 3.6 explorer interface. Equal number of plagiarized and non-plagiarized text samples are trained with the classifier. We train the clas- sifier with 8, 100 text segments from each class where each segment initially contains 5, 000 char- acters. Finally, the classification performances are observed for different feature sets. </chunk></section><section><heading>5.1 Baseline feature sets </heading><chunk>We used three different baseline feature sets for the experiment which are listed below: Baseline-1 (feature set used by Stein et al. (2011)): used 30 features that includes lex- ical and syntactical features, surface fea- tures, vocabulary richness and readability measurement-based features, n-gram-based features, POS-based features etc. Baseline-2 (feature set used by Seaward and Matwin (2009)): calculated the Kolmogorov complexity of function words and different parts-of-speech. Baseline-3 (distance function proposed by Stamatatos (2009)): measured distance func- tion or style-change score of the text- windows with respect to the whole suspicious document by using their character tri-gram profiles. </chunk></section><section><heading>5.2 Proposed feature set </heading><chunk>We propose 36 features for IPD including en- tropy, relative entropy, correlation coefficient, skip n-grams of function words etc. Lavergne et al. (2008) and Zhao et al. (2006) used relative en- tropy for fake content detection and authorship at- tribution accordingly. Islam et al. (2012) classified readability levels of texts by using both entropy and relative entropy. Stamatatos (2011) used func- tion word n-grams for exterinsic plagiarism detec- tion but here we generate several skip n-grams of function words instead of simple n-grams. Guthrie et al. (2006) used 1 to 4 skip n-grams for mod- elling unseen sequences of words in the text. Here we summarize the proposed feature set: Character tri-gram frequency profile: we measure entropy for text windows and rela- tive entropy and the correlation coefficient of the character tri-gram frequency profile for the text windows and documents. Addition- ally, we calculate average n-gram frequency class by using the equation of average word frequency class proposed by Zu Eissen and Stein (2006). Here we have 4 features: en- tropy, relative entropy, correlation coefficient and n-gram frequency class calculated from character tri-gram frequency profiles of text- windows and complete document. bi-gram and tri-gram frequency profile with 1, 2, 3 and 4 skips : we measure entropy, relative entropy, correlation coeffi- cient of function-word bi-gram and tri-gram frequency profile with 1, 2, 3 and 4 skips. Additionally, we calculate the style change scores with these frequency profiles using the distance function proposed by Stamatatos (2009). For generating the skip n-gram pro- files of function-words we extract the func- tion words sequentially from each sentence. 146 We generate function-word skip n-gram pro- files of the text segments by considering only the function words at sentence level instead of passage level as Stamatatos (2011) used. Here we have 32 features: entropy, rela- tive entropy, correlation coefficient and style- change score calculated from 8 function- word skip n-gram frequency profiles. </chunk></section><section><heading>6 Experimental Results </heading><chunk>We observe that the proposed feature set achieves the highest F-Score compared to the baseline fea- ture sets as illustrated in figure 3. All the fea- ture sets together obtain a promising F-Score of 91% while the three baselines combined result in an F-Score around 89%. The proposed feature set achieves an 85% F-Score which is the high- est compared to the three baseline feature sets. Baseline-1 and baseline-2 obtain F-Score around 68% and 62% while baseline-3 surprisingly results in an 84% F-Score as a single feature. We pair fea- ture sets and observe their performances, figure 4 shows that the proposed feature set increases the F-Score with the combination of baseline feature sets. Figure 5 depicts separate observations of en- tropy, relative entropy, correlation coefficient and distance function of function word skip n-gram frequency profiles. Here we notice that relative entropy achieves a very good F-Score of 72%, en- tropy and correlation coefficient also obtain better F-Scores than the distance function. Though dis- tance function results in very good F-Score with the character tri-gram frequency profile it does not perform good enough with the function word skip n-gram frequency profile. Distance function with function word skip n-gram frequency profile ob- tains around a 35% F-Score which is the lowest compared to other functions with function word skip n-gram frequency profile. We also observe the effect of different window lengths (discussed in section 4) on classification performance, the classification performance increases for each fea- ture set if the window length is increased. All the feature sets combined result in F-Score of 82% and 87% for window lengths of 1000 and 2000 characters accordingly while a 91% F-Score is achieved with the window length of 5000 charac- ters. BL-1 BL-2 BL-3 Proposed BL-1 + BL-2 + BL-3 All Together 50 60 70 80 90 100 69.1 61.6 84.1 85.1 89.4 91 68.6 61.5 84.1 85.1 89.4 91 68.4 61.4 84.1 85.1 89.4 91 68.59 61.49 84.08 85.1 89.38 90.94 Precision Recall F-score Accuracy Figure 3: Performance observation of the baseline and proposed feature sets BL-1 + BL-2 BL-1 + BL-3 BL-2 + BL-3 BL-1 + Proposed BL-2 + Proposed BL-3 + Proposed 50 60 70 80 90 100 69.5 88.4 86.8 88.7 87.1 86.8 69 88.4 86.8 88.7 87 86.8 68.8 88.4 86.8 88.7 87 86.8 68.97 88.36 86.78 86.75 86.8 87.02 Precision Recall F-score Accuracy Figure 4: Performance observation of the coupled feature sets Distance Function Entropy Relative Entropy Correlation Coefficient All Together 40 60 80 100 66.7 58.4 73.2 66.1 74.6 50.8 57.7 72.5 66.1 74.4 34.8 57.7 72.4 66.1 74.4 50.84 57.73 72.55 66.07 74.41 Precision Recall F-score Accuracy Figure 5: Performance observation of function word skip n-gram based features 7 Conclusion In this paper we proposed a set of new features for intrinsic plagiarism detection that support ar- guments for continued research on IPD. In the fu- ture we would like to evaluate these features on human-plagiarised and different domain corpora. We are also interested in expanding the IPD task by considering the case that a suspicious document is written by multiple authors. 147 Acknowledgement This paper is a part of my master thesis work while studied at Frankfurt University of Applied Sci- ences. I am very thankful to my thesis supervisor Dr. Alexander Mehler and my especial thanks to IRT-SystemX for ensuring me to attend at SIGdial conference. I also thank my SIGdial mentor and reviewers for their feedback and guidance. </chunk></section><section><heading>References </heading><chunk>Daniela Chuda and Martin Uhlik. The plagia- rism detection by compression method. In Pro- ceedings of the 12th International Conference on Computer Systems and Technologies, pages 429434. ACM, 2011. David Guthrie, Ben Allison, Wei Liu, Louise Guthrie, and Yorick Wilks. A closer look at skip-gram modelling. In Proceedings of the 5th international Conference on Language Re- sources and Evaluation (LREC-2006), pages 1 4, 2006. Mark Hall, Eibe Frank, Geoffrey Holmes, Bern- hard Pfahringer, Peter Reutemann, and Ian H Witten. The weka data mining software: an up- date. ACM SIGKDD Explorations Newsletter, 11(1):1018, 2009. Zahurul Islam, Alexander Mehler, Rashedur Rah- man, and AG Texttechnology. Text readabil- ity classification of textbooks of a low-resource language. In Proceedings of the 26th Pa- cific Asia Conference on Language, Informa- tion, and Computation.(Accepted), 2012. Mike Kestemont, Kim Luyckx, and Walter Daele- mans. Intrinsic plagiarism detection using char- acter trigram distance scores. Proceedings of the PAN, 2011. Thomas Lavergne, Tanguy Urvoy, and Francois Yvon. Detecting fake content with relative en- tropy scoring. In PAN, 2008. Christopher D Manning and Hinrich Schutze. Foundations of statistical natural language pro- cessing, volume 999. MIT Press, 1999. Gabriel Oberreuter, Gaston La A ZHuillier, Se- bastian A Rios, and Juan D Velasquez. Ap- proaches for intrinsic and external plagiarism detection. Proceedings of the PAN, 2011. Karl Pearson. Notes on the history of correlation. Biometrika, 13(1):2545, 1920. John C. Platt. Sequential minimal optimization: A fast algorithm for training support vector machines. Technical report, ADVANCES IN KERNEL METHODS - SUPPORT VECTOR LEARNING, 1998. Martin Potthast, Benno Stein, Andreas Eiselt, Alberto Barron-Cedeno, and Paolo Rosso. Overview of the 1st international competition on plagiarism detection. In 3rd PAN WORK- SHOP. UNCOVERING PLAGIARISM, AU- THORSHIP AND SOCIAL SOFTWARE MIS- USE, 2009. Martin Potthast, Benno Stein, Alberto Barron- Cedeno, and Paolo Rosso. An Evaluation Framework for Plagiarism Detection. In Pro- ceedings of the 23rd International Conference on Computational Linguistics (COLING 2010), Beijing, China, August 2010. Association for Computational Linguistics. Leanne Seaward and Stan Matwin. Intrinsic pla- giarism detection using complexity analysis. In Proc. SEPLN, pages 5661, 2009. Claude Elwood Shannon. A mathematical theory of communication. ACM SIGMOBILE Mobile Computing and Communications Review, 5(1): 355, 1948. Efstathios Stamatatos. Intrinsic plagiarism detec- tion using character n-gram profiles. Proceed- ings of the PAN, pages 3846, 2009. Efstathios Stamatatos. Plagiarism detection based on structural information. In Proceedings of the 20th ACM international conference on Informa- tion and knowledge management, pages 1221 1230. ACM, 2011. Benno Stein, Nedim Lipka, and Peter Pretten- hofer. Intrinsic plagiarism analysis. Language Resources and Evaluation, 45(1):6382, 2011. Michael Tschuggnall and Gunther Specht. Plag- inn: intrinsic plagiarism detection using gram- mar trees. In Natural Language Processing and Information Systems, pages 284289. Springer, 2012. Ying Zhao, Justin Zobel, and Phil Vines. Using relative entropy for authorship attribution. In In- formation Retrieval Technology, pages 92105. Springer, 2006. Sven Meyer Zu Eissen and Benno Stein. Intrinsic plagiarism detection. In Advances in Informa- tion Retrieval, pages 565569. Springer, 2006. 148 </chunk></section></sec_map>