<sec_map><section><chunk>Proceedings of the Eighth SIGHAN Workshop on Chinese Language Processing (SIGHAN-8), pages 110119, Beijing, China, July 30-31, 2015. c 2015 Association for Computational Linguistics and Asian Federation of Natural Language Processing KWB: An Automated Quick News System for Chinese Readers Yiqi Bai 1 , Wenjing Yang 1 , Hao Zhang 1 , Jingwen Wang 1 , Ming Jia 1 , Roland Tong 2 , Jie Wang 1 1. University of Massachusetts Lowell 2. Wantology Abstract We present an automated quick news sys- tem called KWB. KWB crawls and col- lects around the clock news items from over 120 news websites in mainland China, eliminates duplicates, and retrieves a summary of up to 600 characters for each news article using a proprietary sum- mary engine. It then uses a Labeled-LDA classifier to classify the remaining news items into 19 categories, computes popu- larity ranks called PopuRank of the newly collected news items in each category, and displays the summaries of news items in each category sorted according to Popu- Rank together with a picture, if there is any, on http://www.kuaiwenbao.com and mobile apps. We will describe in this pa- per the system architecture of KWB, the data crawler structure, the functionalities of the central database, and the definition of PopuRank. We will show, through ex- periments, the running time of obtaining PopuRank. We will also demonstrate the use of KWB. </chunk></section><section><heading>1 Introduction </heading><chunk>We are living in the era of information explosion. To help people obtain information quickly, we would want to construct an automated system that collects information and provides accurate sum- marization to the user in a timely fashion. This would be a system that integrates advanced tech- nologies and current research results on text au- tomation, including data collection, storage, clas- sification, ranking, summarization, web display- ing, and app development. KWB is such a system that collects news items from the Internet and pro- vides to the reader summarization and PopuRank This work was supported in part by a grant from Wan- tology. Correspondence: wang@cs.uml.edu. of each news item, making it easier for people to obtain critical information quickly. In this paper we will describe the data collec- tion, data storage, and popular ranking of news items for KWB. Descriptions of the other com- ponents will be reported in separate papers, in- cluding Labeled-LDA classifier and content ex- tractions. KWB uses a proprietary summary en- gine to retrieve a summary of up to 600 characters for each news item. This paper is organized as follows. In Section 2 we will describe related work. We will describe the architecture of KWB in Section 3, the KWB Crawler Framework for collecting news items in Section 4, and the KWB central database in Sec- tion 5. We will present the PopuRank formula in Section 6. In Section 7 we will describe KWB and we will conclude the paper in Section 8. </chunk></section><section><heading>2 Related Work </heading></section><section><heading>2.1 Web crawling </heading><chunk>Web-crawling technologies are important mecha- nisms for collecting data from the Internet (see, e.g., (Emamdadi et al., 2014; Lin and Bilmes, 2011; Li et al., 2011; Li et al., 2009; Li et al., 2009; Li and Teng, 2010; Zheng et al., 2008)). The gen- eral framework of a crawling is given below: </chunk></section><section><heading>1. Provide the crawler a seed URL. </heading></section><section><heading>2. The crawler grabs and stores the target pages content. </heading><chunk>3. Enter the URLs contained in the target page in a waiting queue. 4. Process one URL at a time in the queue. </chunk></section><section><heading>5. Repeat Steps 2 to 4. </heading><chunk>A crawler is responsible for the following tasks: 1. URL fetching. There are three approaches to grabbing URLs at the target site (initially the 110 target site is the seed URL): (1) Grab all the URLs in the target site. This approach may waste computing resources of the crawler machines on materials that are not useful for the applications at hand. (2) Grab a portion of the URLs and ignore certain URLs. (3) Grab only what is needed for the current applica- tion. 2. Content extraction. Parse the webpage to get the content for the given application. There are two ways to parse a page. One way is to write specific rules for each web- site, then use a web parsing tool such as Jsoup to extract content. The other way is to write common rules for all websites, such as Googles content extractor. 3. Visit frequency. If a crawler visits a target website very frequently in a short period of time, then the website may consider it hostile and block the crawlers IP to stop it. Thus, it is important to not to visit the target website too often in a short period of time to avoid being blocked. 4. Crawler monitoring. We should monitor if the target website blocks a crawlers request and if the website changes the structures of the webpages. </chunk></section><section><heading>2.2 Ranking of importance and popularity </heading><chunk>There are a number of methods to measure the im- portance and popularity of an object or a person in a network. For example, the Pagerank mechanism measures the influence and popularity of a web- page (Page et al., 1999) and the Erd  os collabo- ration network (Erd  os Number Project, 2010) may be used to measure the impact of collaborators (di- rect and indirect) of Erd  os. These measures, how- ever, do not explicitly consider the effect of time in their ranking. To measure the importance and pop- ularity of news items, we need to consider time ex- plicitly. This calls for a new measure and we will present PopuRank to fill this gap. </chunk></section><section><heading>3 KWB Architecture </heading><chunk>KWB consists of five components (see Fig. 1): (1) crawlers, (2) central DB, (3) summary engine, (4) core processing unit, and (5) web display. Given below are brief descriptions of each of these components: 1. The crawler component is responsible for collecting news items around the clock from over 120 news websites in mainland China. 2. The central DB is responsible for processing the raw data collected from the crawlers, in- cluding removing duplicated news items and fetching summaries for each news article. 3. The summary engine is responsible for re- turning summaries for each new article with different lengths required by applications. This is preparatory technology. 4. The core processing unit consists of three parts: (1) Chinese text fragmentation. (2) News article classifications. (3) Ranking each document according to PopuRank. 5. The web display component is responsible for displaying on a website the news items in each category according to their Popu- Ranks in each day, their summaries, pictures (if there is any), and links to the original news items. Fig. 2 describes the data flow in KWB system in which each module will operate data and save new attributes. 4 KWB Crawler Framework The KWB crawler in our system follows the framework of vertical crawling. It can be reused and customized according to the specific layout of a webpage. We observe that news websites tend to have the same structure: an index page and a num- ber of content pages for news items. When grab- bing the index page, we may want to set the crawl- ing depth to 1 to stop the crawler from grabbing the URLs contained in the content page. Mean- time, we also want to remove repeating URLs in the URL queue. The KWB crawler framework uses both specific rules and common rules, de- pending on the individual crawler for a given web- site. The KWB crawler framework consists of the following modules (see Fig. 3): 1. Visual input module: This module allows the user to specify the patten of the target webpages layout. The user may specify two kinds of patterns. The first kind is a regu- lar expression representing what the content the user wants to extract. For example, the regular expression matches the opening and 111 Figure 1: The architecture of KWB Figure 2: The data flow diagram of KWB closing pair of a specific HTML tag , within which is content the user wants to extract. The second kind is an XPath structure of the content that the user wants to extract. For ex- ample, Suppose that the user wants to select the content enclosed in all the tags. Then the user can specify an XPath query as . 2. Webpage rule management. It manages the webpage rules entered by users, including the following operations: deleting, checking, and updating. </chunk></section><section><chunk>3. The core crawler cluster. This cluster con- sists of the following components:  (1) Thread pool. It is the set of threads in a multitask system. (2) URL pool. It is the database with all the pending URL information when a URL was grabbed. We use Bloom filter to de- tect duplicate URLs and remove them. The crawler will visit and remove a URL one at a time from the remaining URLs in this pool. (a) Pattern pool. It is the database of all the webpage rules entered by users. (b) DAO module. DAO (data access object) contains the interface for further opera- tions, including data export and data in- terface. (c) Duplicate removal. It removes duplicate URLs in the URL pool and the patterns in the pattern pool. 112 Figure 3: Architecture of the KWB crawler framework </chunk></section><section><chunk>4. The crawler task module. This module con- sists of the following submodules:  (1) Priority processing. Some websites are updated more frequency than the oth- ers. This module determines which sites need more frequent visits. (2) Temp grab. Sometimes the user just wants to fetch a website once without paying a return visit. This component handles this type of crawling. (3) Regular grab. For most websites, the user sets up a schedule to grab them pe- riodically. This component handles this type of crawling. </chunk></section><section><chunk>5. The supervision module. This module con- sists of the following submodules:  (1) Resource control (proxy/account). It is a pool containing all the proxy infor- mation and account information. The proxy is used to avoid IP blocking prob- lems, and the account is used to log on certain websites that require signing in, such as twitter and facebook. (2) Monitoring. It monitors if the crawler functions normally. For example, it monitors whether the target website has blocked the crawler. (3) Anti-blocking. When the monitoring submodule detects that a crawler is blocked, it decides whether to restart the crawler, change the pattern, or change proxy to avoid blocking. (4) Managing anti-blocking, exception, and restore rules. This submodule allows the user to manage and change patterns of a website rules. It also determines how of- ten to test if a crawler is still functioning normally. 6. The program entrance. This component consists of a crawler controller/entrance sub- module, which is responsible for starting the entire system. We implemented the KWB crawler framework using Java. We use httpclient to connect to a web- site and get the DOM tree of the page. We use 113 CSS and Jsoup to parse and extract content. We implemented DAO using mysql and JDBC. 5 Central Database Data collected from the KWB crawler are raw data. Although duplicate URLs are eliminated by the crawler, the same news article may be col- lected from different URLs because the it may be reposted on different websites. For each news ar- ticle we need to retrieve its summary of different length (depending on applications) using a propri- etary Chinese text summary engine. These two processes are time consuming. To reduce com- putations, we create a new database called central DB (see Fig. 4) to remove duplicates and retrieve summaries for raw data collected in every hour. Figure 4: Central DB There are two different types of duplicates in the raw data: (1) exactly the same news items due to reposting; (2) different news items reporting the same news. We will keep the second type of news items, for they report the same event from differ- ent angles, which are useful. To identify the first type of duplicates we may compute cosine simi- larities for all the raw data collected by the KWB crawlers, but this approach is time consuming. In- stead, we take a greedy approach to reducing the number of news items that we need to retrieve summaries by eliminating duplicates posted in a small time window. We will further remove dupli- cates later before computing news classifications. The central DB retrieves article summaries and detects duplicates in a parallel fashion. In particu- lar, it sorts all the unprocessed raw data in increas- ing order according to their IDs. These are incre- mental IDs given to the news items based on the time they are fetched by the KWB crawler frame- work. Starting from the first news article, repeat the following: </chunk></section><section><chunk>1. Send a request to the summary engine to re- trieve summaries of required lengths.  2. Compute the cosine similarities of the article with the news items whose IDs fall in a small fixed time window after this article. If a du- plicate is found, remove the one whose ID is in the time window (i.e., with a larger ID), for it is likely a reposting and the news arti- cle with a smaller ID may have already had the summaries generated from the summary engine running on a different server. </chunk></section><section><chunk>3. Move to the next news article in the shorted list  The index of the news items stored in the cen- tral DB contains, among other things, the follow- ing four fields: news title, news URL, image URL, first and last sentence of the news content. We fur- ther remove news items that match any of these fields for all pairs of news items. In other words, for each pair of news items, if there is a match on any of these four fields, then remove the article with a larger ID. </chunk></section><section><heading>6 PopuRank </heading><chunk>KWB implements a Labeled-LDA classifier to classify all the news items stored in the central DB. To do so, it needs to segment each news ar- ticle into a sequence of words, where a word is a sequence of Chinese characters. We show that using Labeled-LDA achieves higher classification accuracy than SVM (Support Vector Machines) for Chinese news items, and we will report this work in a separate paper. KWB then determines the popularity ranking, called PopuRank, of news items. We observe that the news items that are popular during crawling are indeed the true popular news. In particular, in a given time period, breaking news will be fast reported and reposted online everywhere. In this case, the term frequency (TF) of certain words de- scribing this news will increase sharply. Mean- while, the document frequency (DF) of certain words describing the breaking news will also in- crease. We monitor each word (except stop words) in each time frame every day. By monitoring the TF and DF fluctuations of words, KWB calculates PopuRank of the news items collected in each time unit u. The news item with higher PopuRank is more popular. The time unit u may be changed ac- cording to the actual needs and user interests. For example, if we want to determine popular news items in each hour, then we may set u to be the 114 unit of hour. The PopuRank of each article re- mains valid for a fixed number of time frames. For example, we may let = 24 or 48, when u is hour. The value of may also be changed. Let t v denote the current time frame. Let D v = {D 1 , D 2 , , D N } denote the corpus of all news items collected in this time frame with duplicates removed, where D i is a news article and D i contains N i words in the model of bag of words, denoted by D i = (w 1 , w 2 , ..., w N i ) , where each word is a segment of two or more Chi- nese characters after segmentation. We define the following terms: </chunk></section><section><heading>1. Term frequency (TF). The term frequency of word w </heading><chunk>j in D i in time frame t v , denoted by tf (w j , D i , t v ), is the number of times it appears in D i , denoted by N ij , divided by N i . That is, tf (w j , D i , t v ) = N ij N i . Note that if w j D i , then tf (w j , D i , t v ) = 0. </chunk></section><section><heading>2. Document frequency (DF). The document frequency of word w </heading><chunk>j in the corpus D v , de- noted by df (w j , D v ), is defined as the total number of documents in D v that contain w j , denoted by N j , divided by the total number of words in D v , denoted by N . That is, df (w j , t v ) = N j N . </chunk></section><section><heading>3. Average term frequency (ATF). Let atf (w </heading><chunk>j , D v ) denote the average term fre- quency of word w j in corpus D v . That is, atf (w j , t v ) = N i=1 tf (w j , D i , t v ) N . </chunk></section><section><chunk>4. Term rank (TR). We define the term rank of word w  j in document D i in time frame t v , denoted by tr(w j , D i , t v ), as follows: tr(w j , D i , t v ) = tf (w j , D i , t v ) + df (w j , D v ), where 0, 0, and + = 1. For example, we may let = 0.6 and = 0.4 to indicate that we place more weight on term frequency over document frequency. For each word w j appearing in D v , compute df (w j , D v ) and atf (w j , D v ), and keep them for number of time frames. We now define PopuRank of a document. As- sume that word w j appears in the current time frame t v . Let T denote the following sequence of consecutive time frames, called a window: T = (t v+1 , t v+2 , , t v ) . At each time frame in this window, we monitor the DF and ATF values for each word. Let t v be the current time frame. For each word w j in D v , we have the following two cases: Case 1 : w j is a new word, that is, it did not appear in the previous time frames in the window T , then we compute the TF-IDF values of all the new words in this time frame and mark the top d percent of the new words as popular words. Case 2 : w j is not a new word. Compute atf (w j , t v ) and df (w j , t v ). If the ATF and DF values of word w j at time t v suddenly increase k 1 and k 2 times over the previous average ATF and DF values, respectively, for word w j , denoted by avgAT F (w j , t v ) and avgDF (w j , t v ), then we will consider the word w j a popular word, where avgAT F (w j , t v ) = AT F (w j , t v ) 1 , avgDF (w j , t v ) = DF (t v ) 1 , AT F (w j , t v ) = t i T {tv} atf (w j , t i ), DF (w j , t v ) = t i T {tv} df (w j , t i ). To specify the values of k 1 and k 2 , let ratAT F (w j , t v ) = atf (w j , t v ) avgAT F (w j , t v ) , ratDF (w j , t v ) = df (w j , t v ) avgDF (w j , t v ) . If ratAT F (w j , t v ) &gt; , ratDF (w j , t v ) &gt; , where and are threshold values, then we say that word w j is popular in time frame t v . Let H v denote the set of all popular words in time frame t v . We define the PopuRank of news article D i D v to be the sum of term rank of the popular words in D i in time frame t v . Namely, 115 PopuRank(D i , t v ) = wHvD i tr(w, D i , t v ). (1) Figure 5: The top 20 news items in all categories in a time frame Figure 6: The top 20 news items in the sports cat- egories in a time frame Fig. 5 depicts the top 20 news items in all categories within one time frame together with a timestamp when a news article becomes popular, while Fig. 6 depicts the top 20 news items in the category of sports in the time frame. The values of parameters for our PopuRank calculation are u = hour, = 24, d = 20%, = 0.6, = 0.4, = 1.5, and = 1.5. The time stamp 1430445600 is the Unix epoch time, which is equal to the to- tal number of seconds since 00:00:00, January 1, 1970 Greenwich time, corresponding to 22:00:00, April 30, 2015 Eastern Time. Parameters and is related to TR and Popu- Rank. The value of and are decided by which character, TF or DF, is regarded more important. Figure 7: Term Rank (TR) of a word with different values of Figure 8: PopuRank of one news item with differ- ent values of The Fig. 7 shows the TR of a particular word with different . Meanwhile, since TR varies, Popu- Rank of the news also varies, the Fig. 8 shows the different PopuRank of one news with different and in same time frame. Threshold and decide the numbers of popu- lar words, Fig. 9 shows that the numbers of popu- lar words decrease when and increase, and have same value in Fig. 9. The running time of calculating PopuRank on news items in each time frame depends on the numbers of news items waiting to be processed. Table 1 shows the number of news items in each time frame on an average day and the time to com- pute PopuRank of all news items in each time frame on a server running QEMU Virtual CPU version 1.2.0 with 2.6 GHz and 16 GB RAM. </chunk></section><section><heading>7 Web Displays of KWB </heading><chunk>KWB is an automated quick news system that collects news items real-time from all ma- jor Chinese news websites, classifies the news items into 19 categories, and displays on http://www.kuaiwenbao.com news items in each category with summaries and pictures, sorted ac- cording to their PopuRank values. We have also implemented KWB in mobile apps (An- 116 Figure 9: No. of popular words with different val- ues of / droid App may be downloaded by entering http://www.kuaiwenbao.com/kuaiwenbao.apk on a web browser of an Android phone). Fig. 10 depicts the web display of KWB, where the left- hand panel is a menu bar of news titles and picture thumbnails. The user simply points their mouse to a particular news title to see the original pic- ture and the summary of of the news items on the right-hand panel. The reader may also click the read the original button to the URL of the origi- nal news article and read it. KWB classifiers all news items into 19 cate- gories. Users may click the menu icon on the upper-left corner to display the menu of categories and select a particular category of interests. Fig. 11 depicts the category menu. </chunk></section><section><heading>8 Conclusion </heading><chunk>We described KWB, an automated quick news system for the Chinese reader. In particular, we described the architecture of KWB, the KWB crawler framework, the central DB, the PopuRank, and the use of KWB. Required by blind reviews, we have removed the URL information of KWB in this version. </chunk></section><section><heading>References </heading><chunk>Dasgupta, Anirban, Kumar Ravi, and Sujith Ravi. 2013. Summarization Through Submodularity and Dispersion. IBM Journal of research and develop- ment 2.2, pages 159165 Emamdadi, Reihaneh, Mohsen Kahani, and Fattane Zarrinkalam. 2014. A focused linked data crawler based on HTML link analysis. The 4th International eConference onComputer and Knowledge Engineer- ing (ICCKE), pp. 7479. IEEE, 2014. Erd  os Number Project (Oakland University). Table 1: Running time (seconds) for computing PopuRank for news items on an average day time frame no. news items running time 00:00 1238 13.671 01:00 11 0.119 02:00 16 0.116 03:00 5 0.088 04:00 4 0.076 05:00 2 0.070 06:00 3 0.082 07:00 15 0.249 08:00 3 0.196 09:00 7 0.203 10:00 841 6.343 11:00 602 4.735 12:00 6 0.283 13:00 1007 8.848 14:00 2089 38.700 15:00 1444 13.767 16:00 2100 25.918 17:00 2485 40.937 18:00 685 4.437 19:00 5 0.400 20:00 3 0.321 21:00 2 0.320 22:00 4 0.325 23:00 34 0.361 Facts about Erd  os numbers and the col- laboration graph. 2010. Retrieved from http://wwwp.oakland.edu/enp/trivia/. Lin, Hui and Jeff Bilmes. 2011. A class of submodular functions for document summarization. Proc. ACL, pages 510520. Li, Xueming, Minling Xing, and Jiapei Zhang. 2011. A Comprehensive Prediction Method of Visit Prior- ity for Focused Crawler. The 2nd International Sym- posium onIntelligence Information Processing and Trusted Computing (IPTC), pp. 2730. IEEE, 2011. Li, Wei-jiang, Ru Hua-suo, Zhao Tie-jun, and Zang Wen-mao. 2009. A New Algorithm of Topical Crawler. Second International Workshop on Com- puter Science and Engineering (WCSE09), vol. 1, pp. 443446. IEEE, 2009. Li, Wei-jiang, Ru Hua-suo, Hong Kun, and Luo Jia. 2009. A New Algorithm of Blog-Oriented Crawler. International Forum on Computer Science- Technology and Applications (IFCSTA09), vol. 1, pp. 428431. IEEE, 2009. Li, Peng, and Teng Wen-Da. 2010. A focused web crawler face stock information of financial field. 117 IEEE International Conference on Intelligent Com- puting and Intelligent Systems, vol. 2, pp. 512516. 2010. Page, Lawrence, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1999. The PageRank citation ranking: Bringing order to the web. Zheng, Xiaolin, Tao Zhou, Zukun Yu, and Deren Chen. 2008. URL Rule based focused crawler. IEEE In- ternational Conference on e-Business Engineering (ICEBE08). pp. 147154. IEEE, 2008. 118 Figure 10: Web display of KWB Figure 11: Web display of KWB with the menu of categories 119 </chunk></section></sec_map>