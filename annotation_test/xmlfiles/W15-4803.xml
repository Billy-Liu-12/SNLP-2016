<sec_map><section><chunk>A Chomsky-Schutzenberger Representation for Weighted Multiple Context-free Languages Tobias Denkinger Faculty of Computer Science Technische Universitat Dresden 01062 Dresden, Germany tobias.denkinger@tu-dresden.de Abstract We prove a Chomsky-Schutzenberger rep- resentation theorem for weighted multiple context-free languages. 1 Introduction Mildly context-sensitive languages receive much attention in the natural language processing com- munity (Kallmeyer, 2010). Many classes of mildly context-sensitive languages are subsumed by the multiple context-free languages, e.g. the languages of head grammars, linear context-free rewriting systems (Seki et al., 1991), combinatory catego- rial grammars (Vijay-Shanker et al., 1986; Weir and Joshi, 1988), linear indexed grammars (Vijay- Shanker, 1987), minimalist grammars, (Michaelis, 2001a; Michaelis, 2001b), and finite-copying lexi- cal functional grammars (Seki et al., 1993). The Chomsky-Schutzenberger (CS) represen- tation for context-free languages (Chomsky and Schutzenberger, 1963, Prop. 2) has recently been generalised to quantitative context-free languages (Droste and Vogler, 2013) and to (unweighted) multiple context-free languages (Yoshinaka et al., 2010). In order to obtain a CS representation for multiple context-free languages, Yoshinaka et al. (2010) introduce multiple Dyck languages. We give a more algebraic definition of multi- ple Dyck languages using congruence relations to- gether with a decision algorithm for membership that is strongly related to these congruence relations (Sec. 3). We then provide a CS representation for weighted multiple context-free languages (Sec. 4). </chunk></section><section><heading>2 Preliminaries </heading><chunk>In this section we briefly recall formalisms used in this paper and fix some notation. We denote by N the set of natural numbers (in- cluding zero). For every n N we abbreviate {1, . . . , n} by [n]. Let A be a set. The power set of A is denoted by P(A). Let B be a finite set. A partitioning of B is a set P P(B) where the el- ements of P are non-empty, pairwise disjoint, and pP p = B. Let S be a countable set (of sorts) and s S. An S-sorted set is a tuple (B, sort) where B is a set and sort is a function from B to S. We denote the preimage of s under sort by B s and abbreviate (B, sort) by B; sort will always be clear from the context. An S-ranked set is an (S S)-sorted set. Let A and B be sets. The set of functions from A to B is denoted by B A . Let f and g be functions. The domain and range of f are denoted by dom(f ) and rng(f ), respectively. We denote the function obtained by applying g after f by g f . Let F be a set of functions and B f F dom(f ). The set {f (B) | f F } P(rng(f )) is denoted by F (B). Let G and H be sets of functions. The set {h g | h H, g G} of functions is denoted by H G. We use the notion of nondeterministic finite au- tomata with extended transition function (short: FSA) from Hopcroft and Ullman (1979, Sec. 2.3). </chunk></section><section><heading>2.1 Weight algebras </heading><chunk>A monoid is an algebra (A, , 1) where is associa- tive and 1 is neutral with respect to . A bimonoid is an algebra (A, +, , 0, 1) where (A, +, 0) and (A, , 1) are monoids. We call a bimonoid strong if (A, +, 0) is commutative and for every a A we have 0 a = 0 = a 0. Intuitively, a strong bimonoid is a semiring without distributivity. A strong bimonoid is called commutative if (A, , 1) is commutative. A commutative strong bimonoid is complete if there is an infinitary sum operation that maps every indexed family of elements of A to A, extends +, and satisfies infinitary asso- ciativity and commutativity laws; cf. Droste and Vogler (2013, Sec. 2). For the rest of this paper let (A, +, , 0, 1), abbreviated by A, be a complete commutative strong bimonoid. Example 1. We provide a list of complete com- mutative strong bimonoids (cf. Droste et al. (2010, Ex. 1)) some of which are relevant for natural lan- guage processing: Any complete commutative semiring, e.g. the Boolean semiring B = {0, 1}, , , 0, 1 , the probability semiring Pr = R 0 , +, , 0, 1 , the Viterbi semiring [0, 1], max, , 0, 1 , the tropi- cal semiring R {}, min, +, , 0 , any complete lattice, the tropical bimonoid R 0 {}, +, min, 0, , and the algebra ([0, 1], , , 0, 1) with being de- fined for every a, b [0, 1] as either a b = a + b a b or a b = min{a + b, 1}, where R and R 0 denote the set of reals and the set of non-negative reals, respectively, and +, , max, min, , denote the usual operations. 2 An A-weighted language (over ) is a func- tion L : A. The support of L, denoted by supp(L), is {w | L(w) = 0}. If |supp(L)| 1, we call L a monomial. We write .w for L if L(w) = and for every w \ {w} we have L(w ) = 0. </chunk></section><section><heading>2.2 Weighted string homomorphisms </heading><chunk>Let and be alphabets and g : A such that g() is a monomial for every . We define g : A where for every k N, w 1 , . . . , w k , and u we have g(w 1 w k )(u) = u 1 ,...,u k u=u 1 u k k i=1 g w i u i . We call g an A-weighted (string) homomorphism. An A-weighted homomorphism h : A is alphabetic if there is a function h : A {} with h = h . Now assume that A = B and for every we have |supp(g())| = 1. Then g can be construed as a function from to and g can be construed as a function from to . In this case we call g a (string) homomorphism. If moreover, g is a function from to {}, we call g alphabetic. The sets of all A-weighted homomorphisms, A-weighted alphabetic homomorphisms, homo- morphisms, and alphabetic homomorphisms are denoted by HOM(A), HOM(A), HOM, and HOM, respectively. </chunk></section><section><heading>2.3 Weighted multiple context-free languages </heading><chunk>We fix a set X = {x j i | i, j N} of variables. Variables serve as placeholders for strings. The set of string functions over is the N-ranked set F where for every , s 1 , . . . , s , s N we have that (F ) (s 1 s ,s) is the set of functions f : ( ) s 1 ( ) s ( ) s that are defined by some equation of the form f x 1 , . . . , x = u 1 , . . . , u s where x i = (x 1 i , . . . , x s i i ) for every i [], X f = {x j i | i [], j [s i ]}, and u 1 , . . . , u s ( X f ) . In this situation, we define the rank of f , de- noted by rank(f ), and the fan-out of f , denoted by fan-out(f ), as and s, respectively. The string function f is called linear if in u 1 u s every element of X f occurs at most once, f is called non-deleting if in u 1 u s every element of X f occurs at least once, and f is called terminal-free if u 1 , . . . , u s X f . If f is non-deleting, it is uniquely determined by the string [u 1 , . . . , u s ]. We may therefore write [u 1 , . . . , u s ] for f . Note that for every s N N, the set of linear terminal-free string functions of sort s is finite. Definition 2. A multiple context-free gram- mar (MCFG) is a tuple (N, , I, P ) where N is a finite N-sorted set (non-terminals), I N 1 (initial non-terminals), and P fin (A, f, A 1 A ) N F N | sort(f ) = (sort(A 1 ) sort(A ), sort(A)), f is linear, N (productions). We construe P as an N-ranked set where for every = (A, f, A 1 A ) P we have sort() = sort(f ). 2 Let G = (N, , I, P ) be an MCFG and w . A production (A, f, A 1 A ) P is usually written as A f (A 1 , . . . , A ); it inherits rank and fan-out from f . Also, rank(G) = max P rank() and fan-out(G) = max P fan-out(). MCFGs of fan-out at most k are called k-MCFGs. The productions of G form a context-free grammar G with the elements of F and (, ), and , as terminal symbols, N as the set of non-terminals, and I as the set of initial non- terminals. A word in the language of G is a term over F and can be evaluated to a word in . The set of derivations of w in G, denoted by D G (w), is the set of abstract syntax trees in G whose cor- responding words are evaluated to w. The lan- guage of G is L(G) = {w | D G (w) = }. A language L is multiple context-free if there is an MCFG G with L = L(G). The set of multi- ple context-free languages (for which a k-MCFG exists) is denoted by MCFL (k-MCFL, respec- tively). Let k N. The class k-MCFL is a substitution- closed full abstract family of languages (Seki et al., 1991, Thm. 3.9). In particular, k-MCFL is closed under intersection with regular languages and under homomorphisms. Definition 3. An A-weighted MCFG is a tuple (N, , I, P, ) where (N, , I, P ) is an MCFG and : P A (weight function). 2 Let G = (N, , I, P, ) be an A-weighted MCFG and w . The set of derivations of w in G is the set of derivations of w in (N, , I, P ). G inherits fan-out from (N, , I, P ); A-weighted MCFGs of fan-out at most k are called A-weighted k-MCFGs. We apply to derivations by applying it at every position (of the derivation) and then mul- tiplying the resulting values (in any order, since is commutative). The A-weighted language induced by G is the function G : A where for every w we have G(w) = dD G (w) (d). Two (A- weighted) MCFGs are equivalent if they induce the same (A-weighted) language. An A-weighted language L is multiple context-free and of fan-out k if there is an A-weighted k-MCFG G such that L = G; k-MCFL(A) denotes the set of multiple context-free A-weighted languages of fan-out k. Example 4. Consider the Pr-weighted MCFG G = {S, A, B}, , {S}, { 1 , . . . , 5 }, where = {a, b, c, d}, sort(S) = 1, sort(A) = sort(B) = 2, and 1 : S [x 1 1 x 1 2 x 2 1 x 2 2 ](A, B) ( 1 ) = 1 2 : A [ax 1 1 , cx 2 1 ](A) ( 2 ) = 1/2 3 : B [bx 1 1 , dx 2 1 ](B) ( 3 ) = 1/3 4 : A [, ]() ( 4 ) = 1/2 5 : B [, ]() ( 5 ) = 2/3 . We observe that supp(G) = {a m b n c m d n | m, n N} and for every m, n N we have G(a m b n c m d n ) = ( 1 ) ( 2 ) m ( 4 ) ( 3 ) m ( 5 ) = 1/(2 m 3 n+1 ). The only deriva- tion of a 2 bc 2 d in G is shown in Fig. 1. 2 Non-deleting normal form An (A-weighted) MCFG is called non-deleting if the string func- tion in every production is linear and non-deleting. Seki et al. (1991, Lem. 2.2) proved that for ev- ery k-MCFG there is an equivalent non-deleting k- MCFG. We generalise this to A-weighted MCFGs. S [x 1 1 x 1 2 x 2 1 x 2 2 ](A, B) A [ax 1 1 , cx 2 1 ](A) A [ax 1 1 , cx 2 1 ](A) A [, ]() B [bx 1 1 , dx 2 1 ](B) B [, ]() Figure 1: Only derivation of a 2 bc 2 d in G (Ex. 4). Lemma 5. For every A-weighted k-MCFG there is an equivalent non-deleting A-weighted k-MCFG. Proof. Let G = (N, , I, P, ). When exam- ining the proof of Seki et al. (1991, Lem. 2.2), we notice that only step 2 of Procedure 1 deals with non-deletion. We construct N and P from (N, , I, P ) by step 2 of Procedure 1, but drop the restriction that = [sort(A)]. 1 Let g : P P assign to every P the production in G it has been constructed from. Furthermore, let I = {A[] | A I} and = g. Since the con- struction preserves the structure of derivations, we have for every w that g gives rise to a bijec- tion g between D G (w) and D G (w) with = g. Hence G = (N , , I , P , ). The fan-out is not increased by this construction. </chunk></section><section><heading>3 Multiple Dyck languages </heading><chunk>According to Kanazawa (2014, Sec. 1) there is no definition of multiple Dyck languages using congruence relations. We close this gap by giving such a definition (Def. 7). </chunk></section><section><heading>3.1 The definition </heading><chunk>We recall the definition of multiple Dyck languages (Yoshinaka et al., 2010, Def. 1): Let be a finite N-sorted set, 2 () be a bijection between and some alphabet , k = max sort(), and r k. The multiple Dyck grammar with respect to is the k-MCFG G = {A 1 , . . . , A k }, , {A 1 }, P where = { [i] , [i] | , i [sort()]}, sort(A i ) = i for every i [k], and P is the small- est set such that (i) for every linear non-deleting 3 terminal-free string function f (F ) (s 1 s ,s) with </chunk></section><section><heading>1 This construction may therefore create productions of fan-out 0. </heading><chunk>2 In Yoshinaka et al. (2010), N-sorted sets are called in- dexed sets and sort is denoted as dim. 3 We add the restriction non-deleting in comparison to the original definition since in Yoshinaka et al. (2010, Proof of Lem. 1) only non-deleting rules are used. [r], s 1 , . . . , s , s [k] we have A s f (A s 1 , . . . , A s ) P , (ii) for every with sort s we have A s [ [1] x 1 1 [1] , . . . , [s] x s 1 [s] ](A s ) P , and (iii) for every s [k] we have A s [u 1 , . . . , u s ](A s ) P where u i x i , x i [1] [1] , [1] [1] x i | 1 for every i [s]. The multiple Dyck language with respect to , denoted by mD(), is L(G ). We call max sort() the dimension of mD(). The set of multiple Dyck languages of dimension at most k is denoted by k-mDYCK. For the rest of this section let be an alphabet. Also let be a set (disjoint from ) and () be a bijection between and . Intuitively and are sets of opening and closing parentheses and () matches an opening to its closing parenthesis. We define as the smallest congruence rela- tion on the free monoid ( ) where for ev- ery the cancellation rule holds. The Dyck language with respect to , denoted by D(), is [] . The set of Dyck languages is de- noted by DYCK. Example 6. Let = {(, , [, }. We abbreviate (, , [, and by ), , ], and , respectively. Then we have for example ()() and ()() ()() () . 2 Let P be a partitioning of . We define ,P as the smallest congruence relation on the free monoid ( ) such that if v 1 v ,P with v 1 , . . . , v D(), then the cancellation rule u 0 1 v 1 1 u 1 v u ,P u 0 u holds for every { 1 , . . . , } P and u 0 , . . . , u D(). Intuitively, every element of P de- notes a set of linked opening parentheses, i.e. paren- theses that must be consumed simultaneously by ,P . Definition 7. The congruence multiple Dyck lan- guage with respect to and P, denoted by mD c (, P), is [] ,P . 2 Example 8. Let = {(, , [, } and P = {p 1 , p 2 } where p 1 = {(, } and p 2 = {[, }. We abbreviate (, , [, and by ), , ], and , respectively. Then we have for example ()[] ,P since p 2 = {[, } P, () ,P , and u 0 = u 1 = u 2 = . But ()[] ,P since when instantiating the cancellation rule with any of the two elements of P, we can not reduce ()[]: (i) If we choose { 1 , 2 } = {, [} then we would need to set u 1 = and u 2 =, but they are not in D(), also () ,P ; (ii) If we choose { 1 , 2 } = {(, } then we would need to set u 0 = and u 1 =, but they are not in D(), also [] ,P . Hence ()[], () mD c (, P) and ()[] mD c (, P). 2 Observation 9. From the definition of ,P it is easy to see that for every u 1 , . . . , u k D() and v 1 , . . . , v D() we have that u 1 u k , v 1 v mD c (, P) implies that every permutation of u 1 , . . . , u k , v 1 , . . . , v is in mD c (, P). The dimension of mD c (, P) is max pP |p|. The set of congruence multiple Dyck languages (of at most dimension k) is denoted by mDYCK c (k-mDYCK c , respectively). Note that the dimension of P is 1 if and only if P = {{} | }. In this situation we have = ,P and therefore also D() = mD c (, P). Hence DYCK = 1-mDYCK c . Proposition 10. k-mDYCK k-mDYCK c Idea of the proof. We show the property () that implies our claim. The we prove by induction on the structure of derivations in G . For we construct derivations in G by induction on the number of applications of the cancellation rule. Proof. Let mD k-mDYCK. Then there is an N-sorted set such that mD = mD() and k max sort(). We define p = { [i] | i [sort()]} for every , = p , and P = {p | }. Clearly max pP |p| k. Thus mD c (, P) k-mDYCK. Let Tup(G , A) de- note the set of tuples generated in G when starting with non-terminal A where A is not necessarily ini- tial. In the following we show that for every m [max sort()] and w 1 , . . . , w m ( ) : (w 1 , . . . , w m ) Tup(G , A m ) w 1 w m mD c (, P) () w 1 , . . . , w m D() . We show the by induction on the struc- ture of derivations in G : From the definitions of Tup and G we have that (w 1 , . . . , w m ) Tup(G , A m ) implies that there are a rule A m f (A m 1 , . . . , A m ) in G and a tuple u i = (u 1 i , . . . , u m i i ) Tup(G , A m i ) for every i [] such that f ( u 1 , . . . , u ) = (w 1 , . . . , w m ). By ap- plying the induction hypothesis times, we also have that u 1 1 , . . . , u m 1 1 , . . . , u 1 , . . . , u m D() and u 1 1 u m 1 1 , . . . , u 1 u m mD(, P). We distinguish three cases (each corresponding to one type of rule in G ): (i) f is linear, non-deleting, and terminal-free. Then we have for every i [m] that w i {u 1 1 , . . . , u m 1 1 , . . . , u 1 , . . . , u m } and there- fore also w i D(). Furthermore, by ap- plying Obs. 9 ( 1) times, we have that w 1 w m mD c (, P). (ii) f = [ [1] x 1 1 [1] , . . . , [m] x m 1 [m] ]; then = 1, m 1 = m, and for every i [m] we have w i = [i] u i 1 [i] and since u i 1 D() also w i D(). Furthermore, w 1 w m = [1] u 1 1 [1] [m] u m 1 [m] mD c (, P) due to the cancellation rule. (iii) f = [u 1 , . . . , u m ] where for every i [m] : u i x i , x i [1] [1] , [1] [1] x i | 1 ; then = 1, m 1 = m, and w i u 1 i , x 1 i [1] [1] , [1] [1] x 1 i | 1 . Since is a congruence relation, we have that w 1 , . . . , w m D(). By applying Obs. 9 m times, we have that w 1 w m mD c (, P). We show the by induction on the number of applications of the cancellation rule (including the number of applications to reduce the word v 1 v from the definition on the cancellation rule to ): If the cancellation rule is applied zero times in order to reduce w 1 w m to then w 1 = . . . = w m = . The rule A m [, . . . , ]() clearly derives (w 1 , . . . , w m ). If the cancellation rule is applied i + 1 times in order to reduce w 1 w m to then w 1 w m has the form u 0 1 v 1 1 u 1 v u for some u 0 , . . . , u D(), v 1 , . . . , v D(), and { 1 , . . . , } P with v 1 v ,P . Then we need to apply the cancellation rule at most i times to reduce v 1 v to , hence, by induction hypothesis, there is some d D G that derives (v 1 , . . . , v ). We use an appropriate rule of type (ii) such that (d) derives ( 1 v 1 1 , . . . , v ). Also, we need to apply the cancellation rule at most i times in order to reduce u 0 u to , hence, by induction hypothesis, there are deriva- tions d 1 , . . . , d n that derive tuples containing ex- actly u 0 , . . . , u as components. Then there is a rule such that ((d), d 1 , . . . , d n ) D G de- rives the tuple (w 1 , . . . , w m ). From () with m = 1 and the fact w 1 mD c (, P) implies w 1 D() we get that mD c (, P) = mD. Lemma 11. k-mDYCK c k-MCFL Proof idea. For any congruence multiple Dyck lan- guage we construct a multiple Dyck grammar that is equivalent up to a homomorphism. We then use the closure of k-MCFL under homomorphisms. Proof. Let L k-mDYCK c . Then there are an alphabet and a partitioning P of such that mD c (, P) = L. Consider P as an N-sorted set where the sort of an element is its cardinality. Then = {p [i] , p [i] | p P, i [|p|]}. For every p P assume some fixed enumeration of the ele- ments of p. We define a bijection g : such that every p [i] (for some p and i) is assigned the i-th element of p and g( p [i] ) = g(p [i] ). Then g(L(G P )) = L, where G P is the multiple Dyck grammar with respect to P. Since k-MCFLs are closed under homomorphisms (Seki et al., 1991, Thm. 3.9), L k-MCFL. 4 Observation 12. Examining the definition of mul- tiple Dyck grammars, we observe that some pro- duction in item (ii) has fan-out k for at least one . Then, using Seki et al. (1991, Thm. 3.4), we have for every k 1 that (k + 1)-mDYCK c \ k-MCFL = . Proposition 13. 1-mDYCK c 2-mDYCK c . . . Proof. We get from the definition of k-mDYCK c and = from Obs. 12. </chunk></section><section><heading>3.2 Membership in a congruence multiple Dyck language </heading><chunk>We provide a recursive algorithm (Alg. 1) to decide whether a word w is in a given congruence multi- ple Dyck language mD c (, P). This amounts to checking whether w ,P , and it suffices to only apply the cancellation rule from left to right. Outline of Alg. 1 We check that w is in D(), e.g. with the context-free grammar in (7.6) in Sa- lomaa (1973). If w is not in D(), it is also not in mD c (, P) and we return 0. Otherwise, we split w into shortest strings u 1 , . . . , u D() \ {} such that w = u 1 u (in line 5) with the function SPLIT. Then every u i (for i []) necessarily starts 4 This construction shows that Def. 7 is equivalent to Def. 1 in Yoshinaka et al. (2010) modulo the application of g. Algorithm 1 Membership in mD c (, P) Input: , P, and w ( ) Output: 1 if w mD c (, P), 0 otherwise 1: function MAIN(, P, w) 2: if w D() then 3: return 0 4: end if 5: (u 1 , . . . , u ) SPLIT(, w) 6: Rel 7: for i [] do 8: let u i s.t. u i = u i for some 9: P {p dom(Rel ) | p } 10: if P = then 11: p a minimal element of P 12: {I} {I | (p, I ) Rel } 13: Rel Rel \ {(p, I)} 14: Rel Rel {(p \ {}, I {i})} 15: else 16: {p} {p P | p} 17: Rel Rel {(p \ {}, {i})} 18: end if 19: end for 20: if (,J)Rel J = [] then 21: return 0 22: end if 23: for {j 1 , . . . , j k } {J | (, J) Rel } do 24: if MAIN(, P, u j 1 u j 2 u j k ) = 0 then 25: return 0 26: end if 27: end for 28: return 1 29: end function 30: function SPLIT(, w) 31: (u 1 , . . . , u ) sequence of shortest words u 1 , . . . , u D() \ {} with w = u 1 u 32: return (u 1 , . . . , u ) 33: end function with some symbol and ends with ; we use this property on line 8. Note that SPLIT is bijec- tive (the inverse function is concatenation). We therefore say that w and (u 1 , . . . , u ) correspond to each other, and for every operation on either of them there is a corresponding operation on the other. In particular the empty string corresponds to the empty tuple. In lines 619 we find sets of indices {i 1 , . . . , i k } such that the set of first symbols of u i 1 , . . . , u i k has cardinality k and is an element of P. In order to do this, we use a relation Rel P() P([]) Table 1: Run of Alg. 1 on the word ()[], cf. Exs. 8 and 14. line variable assignment 5: (u 1 , . . . , u ) = (), [] 18: i = 1 Rel : {[}, {1} 18: i = 2 Rel : , {1, 2} 23: u 1 = (), u 2 = 5: (u 1 , . . . , u ) = (), 18: i = 1 Rel : {{}, {1} 18: i = 2 Rel : , {1, 2} 23: u 1 = , u 2 = 5: (u 1 , . . . , u ) = that is initially empty (line 6). We modify Rel while traversing the list u 1 , . . . , u from left to right. Intuitively every element (l, r) Rel stands for an element p P where there is some previous (with respect to the traversal of u 1 , . . . , u ) u i that starts with some symbol p; l is the set of elements of p we have not yet seen in some pre- vious u i ; and r is the set of indices i such that some previous u i starts with an element of p. Rel is constructed in lines 719. Since in every itera- tion of the for-loop (lines 719) we add the cur- rent i to the set on the right side of some tuple in Rel , we have that (p,J)Rel J = []. Now since the left side of a tuple in Rel signifies the elements of p we have not yet seen, a tuple of the form (, {j 1 , . . . , j k }) Rel means that we can reduce the outer parentheses of u j 1 , . . . , u j k with one step of the cancellation rule. In order to reduce the whole string w, every element of [] has to appear in the right side of (exactly) one tuple of the form (, J) Rel ; we check this property in line 20 and return 0 (line 21) if it is not satisfied. If it is satisfied, we remove the first and last symbol from all u j 1 , . . . , u j k (obtaining u j 1 , . . . , u j k ) where (, {j 1 , . . . , j k }) Rel and call MAIN recursively with the string u j 1 u j k (lines 2327); this corresponds to the condition that v 1 v k ,P in the definition of ,P . Example 14 (Ex. 8 continued). Tab. 1 shows a run of Alg. 1 on the word ()[] where we report a subset of the variable assignment whenever we reach the end of lines 5, 18, or 23. Different calls to MAIN are separated by horizontal lines. 2 Proof of termination for Alg. 1. If w is not in D(), the algorithm terminates at line 3. If w is the empty string, then = 0, therefore the in- dex set of the for-loop on lines 719 is empty, the condition on line 20 is false, the index set of the for-loop on lines 2327 is empty, there are no re- cursive calls to MAIN, and the algorithm terminates on line 28. If w is in D() and w = then there remain two possible situations: (i) There is some i [] that does not occur in the right side of any tuple in Rel . Then the algorithm terminates on line 21. (ii) Every i [] occurs in the right side of some tuple in Rel . Then the combined length of the third arguments of all recursive calls in the for- loop on lines 2327 is strictly smaller then |w| since the outermost parentheses are removed from u 1 , . . . , u . Since w has finitely many symbols, this process can only be repeated finitely often and the algorithm eventually ter- minates. In light of the close link between Alg. 1 and the relation ,P we omit the proof of correctness. 4 CS theorem for weighted MCFLs In this section we generalise the CS representation of (unweighted) MCFLs (Yoshinaka et al., 2010, Thm. 3) to the weighted case. We prove that an A-weighted MCFL L can be decomposed into an A-weighted alphabetic homomorphism h, a regu- lar language R and a congruence multiple Dyck language mD c such that L = h(R mD c ). To show this, we use the proof idea from Droste and Vogler (2013). The outline of our proof is as follows: (i) We separate the weights from L (Lem. 15), ob- taining an MCFL L and a weighted alphabetic homomorphism. (ii) We use a corollary of the CS representation of (unweighted) MCFLs (Cor. 16) to obtain a CS representation of L . (iii) Using the two previous points and an ob- servation for the composition of weighted and unweighted alphabetic homomorphisms (Lem. 18), we obtain a CS representation of L (Thm. 19). Lemma 15. k-MCFL(A) = HOM(A) k-MCFL Proof. () Let L k-MCFL(A). By Lem. 5 there is a non-deleting A-weighted k-MCFG G = (N, , I, P, ) such that G = L. We define a non-deleting k-MCFG G = (N, , I, P ) where = { i | P, i [fan-out()]} and P is the smallest set such that for every production = A [u 1 , . . . , u s ](A 1 , . . . , A m ) P there is a production A [ 1 u 1 , . . . , s u s ](A 1 , . . . , A m ) P . We define an A-weighted alphabetic homo- morphism h : ( ) A where h() = 1. for every , h( 1 ) = (). for every P , and h( i ) = 1. for every P and i {2, . . . , fan-out()}. Since 1 is neutral in multiplication, is commutative, and G is non- deleting, we have L = h(L(G )). () Let L k-MCFL and h : A an A-weighted alphabetic homomorphism. By Seki et al. (1991, Lem. 2.2) there is a non- deleting k-MCFG G = (N, , I, P ) such that L(G) = L. We construct the A-weighted k-MCFG G = (N, , I, P , ) as follows: We extend h to h : ( X) A (X) where h (x) = 1.x for every x X and h () = h() for every . We define P as the smallest set such that for every = A [u 1 , . . . , u s ](A 1 , . . . , A m ) P (s 1 sm,s) and (u 1 , . . . , u s ) supp(h (u 1 )) . . . supp(h (u s )) we have that P contains the production = A [u 1 , . . . , u s ](A 1 , . . . , A m ) and ( ) = h (u 1 )(u 1 ) . . . h (u s )(u s ). Since is commutative and G non-deleting, we have that G = h(G). 5 By setting k = 1 in the above lemma we reobtain the equivalence of 1 and 3 in Thm. 2 of Droste and Vogler (2013) for complete commutative strong bimonoids. The following is a corollary to Yoshinaka et al. (2010, Thm. 3) where the homomorphism is re- placed by an alphabetic homomorphism and the multiple Dyck language is replaced by a congru- ence multiple Dyck language. Corollary 16. Let L be a language and k N. Then the following are equivalent: (i) L k-MCFL (ii) there are an alphabetic homomorphism h, a regular language R, and a congruence multiple Dyck language mD c of at most dimension k with L = h(R mD c ). 5 The same two constructions also work to show that k-MCFL(A) = HOM(A) k-MCFL . S [1] start A [1] [1] 1 1 1 1 1 [1] 1 ,1 [1] 2 1 2 1 2 a a [1] 2 ,1 T [1] 4 1 4 1 4 [1] 4 [2] 1 ,2 [2] 1 , [1] 2 ,1 [1] 2 , [2] 2 ,1 [2] 2 , [1] 3 ,1 [1] 3 , [2] 3 ,1 [2] 3 A [2] [2] 4 2 4 2 4 [2] 4 [1] 1 ,2 [2] 1 ,1 [2] 2 2 2 2 2 c c [2] 2 ,1 B [1] [1] 5 1 5 1 5 [1] 5 [1] 1 ,1 [1] 1 ,2 [1] 3 1 3 1 3 b b [1] 3 ,1 B [2] [2] 5 2 5 2 5 [2] 5 [2] 1 ,1 [2] 1 ,2 [2] 3 2 3 2 3 d d [2] 3 ,1 Figure 2: Automaton R obtained from G (cf. Exs. 4 and 17) by Lem. 15 and Cor. 16. Proof. The construction of h in Yoshinaka et al. (2010, Sec. 3.2) already satisfies the definition of an alphabetic homomorphism. We may use a congruence multiple Dyck language instead of a multiple Dyck language since, for (i) (ii), k-mDYCK k-mDYCK c and, for (ii) (i), k-mDYCK c k-MCFL and k-MCFL is closed under intersection with regular languages and un- der homomorphisms. We give an example to show how Lem. 15 and Yoshinaka et al. (2010, Sec. 3.2) can be employed to construct a regular language for the CS represen- tation of weighted MCFLs. The regular language is represented by an FSA. Example 17 (Ex. 4 continued). We construct an MCFG G from G as described in the proof of () in Lem. 15. Fig. 2 shows the FSA R obtained from G by the construction in Yoshinaka et al. (2010, Sec. 3.2). An edge labelled with a set L of words denotes a set of transitions each reading a word in L. Note that the language of R is not finite. 2 Lemma 18. HOM(A) HOM = HOM(A) Proof. () Let h 1 : A HOM(A) and h 2 : HOM. By the defini- tions of HOM(A) and HOM there must exist h 1 : A {} and h 2 : {} such that h 1 = h 1 and h 2 = h 2 . Since h 1 (rng(h 2 )) A {} there is some h HOM(A) such that h = h 1 h 2 ; hence h 1 h 2 HOM(A). () Follows from the fact that HOM contains the identity. Theorem 19. Let L be an A-weighted language and k N. The following are equivalent: (i) L k-MCFL(A) (ii) there are an A-weighted alphabetic homomor- phism h, a regular language R, and a congru- ence multiple Dyck language mD c of dimen- sion at most k with L = h(R mD c ). Proof. For (i) (ii): There are some L k-MCFL, h, h 1 HOM(A), h 2 HOM, mD c k-mDYCK c , and R REG such that L = h 1 (L ) (by Lem. 15) = h 1 (h 2 (R mD c )) (by Cor. 16) = h(R mD c ) (by Lem. 18) For (ii) (i): We use Lems. 11 and 15, and the closure of k-MCFG under intersection with regular languages and application of homomorphisms. 5 Conclusion and outlook We defined multiple Dyck languages using con- gruence relations (Def. 7), gave an algorithm to decide whether a word is in a given multiple Dyck language (Alg. 1), and established that multiple Dyck languages with increasing maximal dimen- sion form a hierarchy (Prop. 13). We obtained a weighted version of the CS rep- resentation of MCFLs for complete commutative strong bimonoids (Thm. 19) by separating the weights from the weighted MCFG and using Yoshi- naka et al. (2010, Thm. 3) for the unweighted part. Thm. 19 may be used to develop a parsing algo- rithm for weighted multiple context-free grammars in the spirit of Hulden (2011). </chunk></section><section><heading>References </heading><chunk>Noam Chomsky and Marcel-Paul Schutzenberger. 1963. The algebraic theory of context-free languages. Com- puter Programming and Formal Systems, Studies in Logic, pages 118161. Manfred Droste and Heiko Vogler. 2013. The Chomsky- Schutzenberger theorem for quantitative context-free languages. In Marie-Pierre Beal and Olivier Carton, editors, Developments in Language Theory, volume 7907 of Lecture Notes in Computer Science, pages 203214. Springer. Manfred Droste, Torsten Stuber, and Heiko Vogler. 2010. Weighted finite automata over strong bi- monoids. Information Sciences, 180(1):156166. John E. Hopcroft and Jeffrey D. Ullman. 1979. Intro- duction to automata theory, languages and computa- tion. Addison-Wesley, 1st edition. Mans Hulden. 2011. Parsing CFGs and PCFGs with a Chomsky-Schutzenberger representation. In Zyg- munt Vetulani, editor, Human Language Technology. Challenges for Computer Science and Linguistics, volume 6562 of Lecture Notes in Computer Science, pages 151160. Laura Kallmeyer. 2010. Parsing beyond context-free grammars. Springer. Makoto Kanazawa. 2014. Multidimensional trees and a Chomsky-Schutzenberger-Weir representation theo- rem for simple context-free tree grammars. Journal of Logic and Computation. Jens Michaelis. 2001a. Derivational minimalism is mildly contextsensitive. In Michael Moortgat, ed- itor, Logical Aspects of Computational Linguistics, volume 2014 of Lecture Notes in Computer Science, pages 179198. Springer. Jens Michaelis. 2001b. Transforming linear context- free rewriting systems into minimalist grammars. In Philippe Groote, Glyn Morrill, and Christian Retore, editors, Logical Aspects of Computational Linguis- tics, volume 2099 of Lecture Notes in Computer Sci- ence, pages 228244. Springer. Arto Salomaa. 1973. Formal languages. Academic Press. Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii, and Tadao Kasami. 1991. On multiple context-free gram- mars. Theoretical Computer Science, 88(2):191229. Hiroyuki Seki, Ryuichi Nakanishi, Yuichi Kaji, Sachiko Ando, and Tadao Kasami. 1993. Parallel multiple context-free grammars, finite-state translation sys- tems, and polynomial-time recognizable subclasses of lexical-functional grammars. In Proceedings of the 31st Annual Meeting on Association for Compu- tational Linguistics, pages 130139. Krishnamurti Vijay-Shanker, David Jeremy Weir, and Aravind K Joshi. 1986. Tree adjoining and head wrapping. In Proceedings of the 11th International Conference on Computational Linguistics, pages 202 207. Krishnamurti Vijay-Shanker. 1987. A study of tree adjoining grammars. Ph.D. thesis. David Jeremy Weir and Arvind K. Joshi. 1988. Combi- natory categorial grammars: Generative power and relationship to linear context-free rewriting systems. In Proceedings of the 26th annual meeting on Associ- ation for Computational Linguistics, pages 278285. Ryo Yoshinaka, Yuichi Kaji, and Hiroyuki Seki. 2010. Chomsky-Schutzenberger-type characterization of multiple context-free languages. In Adrian-Horia Dediu, Henning Fernau, and Carlos Martin-Vide, edi- tors, Proceedings of the 4th International Conference on Language and Automata Theory and Applications, volume 6031 of Lecture Notes in Computer Science, pages 596607. Springer. </chunk></section><section><heading>A Supplemental definitions </heading><chunk>Let G be an MCFG and A a non-terminal in G. A subderivation in G is a derivation in the un- derlying context-free grammar of G that does not necessarily start with an initial non-terminal. The set of subderivations in G from A, denoted by D G (A), is the set of subderivations in G that start with the non-terminal A. </chunk></section><section><heading>B Supplementals to the proof of Lem. 5 </heading><chunk>Observation 20. g, obtained by position-wise ap- plication of g, is a tree homomorphism. Claim 21. g is a bijection. Proof. We show that g is bijective by induction on the structure of subderivations: Induction hypothesis: For every A N and M (A) : g is a bijection between D G (A[ ]) and D G (A). Induction step: Let M (A) and d = (d 1 , . . . , d k ) D G (A) with d 1 D G (A 1 ), . . . , d k D G (A k ). The construction defines 1 M (A 1 ), . . . , k M (A k ) and a production which is unique for every and . By the induction hypothesis, we know that there are derivations d 1 , . . . , d k which are unique for (d 1 , 1 ), . . . , (d k , k ), respectively. Therefore, d = (d 1 , . . . , d k ) is unique for d and . Hence for every : g induces a bijection on D G (A[ ]) and D G (A). By construction, all elements of I have the form A[] for some A I; hence for every element of D G we set = and by the induction hypothesis we obtain Cl. 21. Claim 22. = g Proof. Since g is a tree homomorphism (cf. Obs. 20), it preserves the tree structure. By the definition of we obtain Cl. 22. C Supplementals to the proof of Lem. 11 Claim 23. g(L(G P )) = L. Proof. Let Tup g (G P ) be the set of tuples that are obtained by interpreting the terms corresponding to every subderivation in G P and then applying g to every component. We show the following equivalence by induction: w mD(P) N, u 0 , . . . , u , w 1 , . . . , w D() with w = u 0 w 1 u 1 w u : (u 0 , w 1 , u 1 , , w , u ) Tup g (G P ) (IH) Note that in the following the indices in p = { 1 , . . . , } are chosen such that for every i [] : g(p [i] ) = i . We derive w mD(P) N, u 0 , . . . , u , v 1 , . . . , v D(), p = { 1 , . . . , } P with w = u 0 1 v 1 1 u 1 v u : u 0 v 1 u 1 v u mD(P) (by def. of mD(P)) N, u 0 , . . . , u , v 1 , . . . , v D(), p = { 1 , . . . , } P with w = u 0 1 v 1 1 u 1 v u : (u 0 , v 1 , u 1 , . . . , v , u ) Tup g (G P ) (by (IH)) N, u 0 , . . . , u , v 1 , . . . , v D(), p = { 1 , . . . , } P with w = u 0 1 v 1 1 u 1 v u : (u 0 , 1 v 1 1 , u 1 , . . . , v , u ) Tup g (G P ) (by def. of G P ) N, u 0 , . . . , u , w 1 , . . . , w D() with w = u 0 w 1 u 1 w u : (u 0 , w 1 , u 0 , . . . , w , u ) Tup g (G P ) (using permuting productions in G P ) Cl. 23 follows by instantiating (IH) for = 0 and discovering that {t | (t) Tup g (G P )} = g(L(G P )). D Supplementals to Alg. 1 Alg. 2 implements SPLIT from Alg. 1 (lines 3033) in an explicit manner. For this purpose we define a data structure push- down as a string over some alphabet and two func- tions with side-effects on pushdowns. Let be an alphabet, , and pd be a pushdown. pop(pd ) returns the left-most symbol of pd and removes it from pd . push(pd , ) prepends to pd . Note that pop() is only a partial function, it is undefined for pd = . But since the input word w is in D(), the expression on line 8 is always defined. Algorithm 2 Algorithm to split a word in D() into shortest non-empty strings from D() Input: alphabet , w D() Output: sequence (u 1 , . . . , u ) of shortest words u 1 , . . . , u D() \ {} with w = u 1 u 1: function SPLIT(, w) 2: pd 3: j 1 4: u j 5: for 0 i |w| do 6: u j u j w i 7: if w i then 8: pop(pd ) 9: if pd = then 10: j j + 1 11: u j 12: end if 13: else 14: push(pd , w i ) 15: end if 16: end for 17: return (u 1 , . . . , u j1 ) 18: end function </chunk></section><section><heading>E Supplementals to the proof of Lem. 15 </heading><chunk>Claim 24. There are bijections f : D G D G and g : D G L(G ). Proof. Let f be the function that is obtained by applying the construction position-wise to a deriva- tion in D G . The function f only inserts symbols into the functions in the productions; by removing these elements, we get the original function, hence f is bijective. Let g : D G L(G ) be the function that as- signs for every derivation d D G the word in L(G ) obtained by interpreting the term cor- responding to d. For every w L(G ) we can calculate the corresponding derivation (as a tree with domain dom(t) and labelling function t) us- ing Alg. 3, hence g is bijective. Claim 25. For every d D G and w : (h g f )(d)(w) = (d) if d D G (w), 0 otherwise. Algorithm 3 Algorithm to calculate for every word in L(G ) the corresponding derivation in D G (cf. Cl. 24) Input: w ( ) Output: t : N P 1: procedure MAIN(w ( ) ) 2: let t be the empty function 3: DESCEND(, 1) 4: return t 5: end procedure 6: procedure DESCEND( N , j N) 7: j u w where P and u ( ) 8: t() 9: w u 10: A [u 1 , . . . , u s ](A 1 , . . . , A k ) 11: for every symbol in u j do 12: if then 13: remove from the beginning of w 14: else 15: x j i for some i and j 16: DESCEND(i, j ) 17: end if 18: end for 19: end procedure Proof. Follows directly from the definitions of f , g, and h. Claim 26. G = h(L(G )). Proof. For every w : L(w) = G(w) = dD G (w) (d) = dD G (h g f )(d)(w) (by Cl. 25) = dD G ,uL(G ) u=(gf )(d) h(u)(w) = uL(G ) h(u)(w) (by Cl. 24) = h(L(G ))(w) </chunk></section></sec_map>