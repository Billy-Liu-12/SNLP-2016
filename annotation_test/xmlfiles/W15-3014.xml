<sec_map><section><chunk>Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 134140, Lisboa, Portugal, 17-18 September 2015. c 2015 Association for Computational Linguistics. Montreal Neural Machine Translation Systems for WMT15 S  ebastien Jean University of Montreal jeasebas@iro.umontreal.ca Orhan Firat Middle East Technical University, Turkey orhan.firat@ceng.metu.edu.tr Kyunghyun Cho University of Montreal Roland Memisevic University of Montreal firstname.lastname@umontreal.ca Yoshua Bengio University of Montreal CIFAR Senior Fellow Abstract Neural machine translation (NMT) systems have recently achieved re- sults comparable to the state of the art on a few translation tasks, including EnglishFrench and EnglishGerman. The main purpose of the Montreal In- stitute for Learning Algorithms (MILA) submission to WMT15 is to evaluate this new approach on a greater variety of language pairs. Furthermore, the human evaluation campaign may help us and the research community to better understand the behaviour of our systems. We use the RNNsearch architecture, which adds an attention mechanism to the encoder- decoder. We also leverage some of the recent developments in NMT, including the use of large vocabularies, unknown word replacement and, to a limited degree, the inclusion of monolingual language models. </chunk></section><section><heading>1 Introduction </heading><chunk>Neural machine translation (NMT) is a recently proposed approach for machine translation that re- lies only on neural networks. The NMT system is trained end-to-end to maximize the conditional probability of a correct translation given a source sentence (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015). Although NMT has only recently been introduced, its performance has been found to be comparable to the state-of-the-art statistical ma- chine translation (SMT) systems on a number of translation tasks (Luong et al., 2015; Jean et al., 2015). The main purpose of our submission to WMT15 is to test the NMT system on a greater equal contribution variety of language pairs. As such, we trained sys- tems on CzechEnglish, GermanEnglish and FinnishEnglish. Furthermore, the human evalu- ation campaign of WMT15 will help us better un- derstand the quality of NMT systems which have mainly been evaluated using the automatic evalu- ation metric such as BLEU (Papineni et al., 2002). Most NMT systems are based on the encoder- decoder architecture (Cho et al., 2014; Sutskever et al., 2014; Kalchbrenner and Blunsom, 2013). The source sentence is first read by the encoder, which compresses it into a real-valued vector. From this vector representation the decoder may then generate a translation word-by-word. One limitation of this approach is that a source sen- tence of any length must be encoded into a fixed- length vector. To address this issue, our systems for WMT15 use the RNNsearch architecture from (Bahdanau et al., 2015). In this case, the encoder assigns a context-dependent vector, or annotation, to every source word. The decoder then selectively combines the most relevant annotations to gener- ate each target word. NMT systems often use a limited vocabu- lary of approximately 30, 000 to 80, 000 target words, which leads them to generate many out- of-vocabulary tokens (UNK). This may easily lead to the degraded quality of the translations. To sidestep this problem, we employ a variant of importance sampling to help increase the target vocabulary size (Jean et al., 2015). Even with a larger vocabulary, there will almost assuredly be words in the test set that were unseen during training. As such, we replace generated out-of- vocaulbary tokens with the corresponding source words with a technique similar to those proposed by (Luong et al., 2015). Most NMT systems rely only on parallel data, ignoring the wealth of information found in large monolingual corpora. On FinnishEnglish, we combine our systems with a recurrent neural net- 134 work (RNN) language model by recently proposed deep fusion (G  ulc  ehre et al., 2015). For the other language pairs, we tried reranking n-best lists with 5-gram language models (Chen and Goodman., 1998). 2 System Description In this section, we describe the RNNsearch ar- chitecture as well as the additional techniques we used. Mathematical Notations Capital letters are used for matrices, and lower-case letters for vectors and scalars. x and y are used for a word in source and target sentences, respectively. We boldface them into x, y and y to denote their continuous-space representation (word em- beddings). </chunk></section><section><heading>2.1 Bidirectional Encoder </heading><chunk>To encode a source sentence (x 1 , . . . , x Tx ) of length T x into a sequence of annotations, we use a bidirectional recurrent neural network (Schus- ter and Paliwal, 1997). The bidirectional recur- rent neural network (BiRNN) consists of two re- current neural networks (RNN) that read the sen- tence either forward (from left to right) or back- ward. These RNNs respectively compute the sequences of hidden states ( h 1 , . . . , h Tx ) and ( h 1 , . . . , h Tx ). These two sequences are con- catenated at each time step to form the annota- tions (h 1 , . . . , h Tx ). Each annotation h i summa- rizes the entire sentence, albeit with more empha- sis on word x i and the neighbouring words. We built the BiRNN with gated recurrent units (GRU, (Cho et al., 2014)), although long short-term memory (LSTM) units could also be used (Hochreiter and Schmidhuber, 1997), as in (Sutskever et al., 2014). More precisely, for the forward RNN, the hidden state at the i-th word is computed as h i = (1 z i ) h i1 + z i h i , if i &gt; 0 0 , if i = 0 where h i = tanh W x i + U r i h i1 + b z i = W z x i + U z h i1 r i = W r x i + U r h i1 . To form the new hidden state, the network first computes a proposal h i . This is then additively combined with the previous hidden state h i1 , and this combination is controlled by the update gate z i . Such gated units facilitate capturing long-term dependencies. </chunk></section><section><heading>2.2 Attentive Decoder </heading><chunk>After computing the initial hidden state s 0 = tanh W s h 1 + b s , the RNNsearch decoder alternates between three steps: Look, Generate and Update. During the Look phase, the network determines which parts of the source sentence are most rele- vant. Given the previous hidden state s i1 of the decoder recurrent neural network (RNN), each an- notation h j is assigned a score e ij : e ij = v a tanh (W a s i1 + U a h j ) . Although a more complex scoring function can potentially learn more non-trivial alignments, we observed that this single-hidden-layer function is enough for most of the language pairs we consid- ered. These scores e ij are then normalized to sum to 1: ij = exp (e ij ) Tx k=1 exp (e ik ) , (1) which we call alignment weights. The context vector c i is computed as a weighted sum of the annotations (h 1 , ..., h Tx ) according to the alignment weights: c i = Tx j=1 ij h j . This formulation allows the annotations with higher alignment weights to be more represented in the context vector c i . In the Generate phase, the decoder predicts the next target word. We first combine the previous hidden state s i1 , the previous word y i1 and the current context vector c i into a vector t i : t i = U o s i1 + V o y i1 + C o c i + b o . We then transform t i into a hidden state m i with an arbitrary feedforward network. In our submission, we apply the maxout non-linearity (Goodfellow et al., 2013) to t i , followed by an affine transforma- tion. 135 Phase Output Input Look c i s i1 , (h 1 , ..., h Tx ) Generate y i s i1 , y i1 , c i Update s i s i1 , y i , c i Table 1: Summary of RNNsearch decoder phases For a target vocabulary V , the probability of word y i is then p(y i |s i1 , y i1 , c i ) = exp y i m i + b y i yV exp ( y m i + b y ) . (2) Finally, in the Update phase, the decoder com- putes the next recurrent hidden state s i from the context c i , the generated word y i and the previ- ous hidden state s i1 . As with the encoder we use gated recurrent units (GRU). Table 1 summarizes this three-step procedure. We observed that it is important to have Update to follow Generate. Otherwise, the next steps Look would not be able to resolve the uncertainty em- bedded in the previous hidden state about the pre- viously generated word. </chunk></section><section><heading>2.3 Very Large Target Vocabulary Extension </heading><chunk>Training an RNNsearch model with hundreds of thousands of target words easily becomes pro- hibitively time-consuming due to the normaliza- tion constant in the softmax output (see Eq. (2).) To address this problem, we use the approach pre- sented in (Jean et al., 2015), which is based on importance sampling (Bengio and S  en  ecal, 2008). During training, we choose a smaller vocabulary size and divide the training set into partitions, each of which contains approximately unique target words. For each partition, we train the model as if only the unique words within it existed, leaving the embeddings of all the other words fixed. At test time, the corresponding subset of target words for each source sentence is not known in advance, yet we still want to keep computational complexity manageable. To overcome this, we run an existing word alignment tool on the train- ing corpus in advance to obtain word-based con- ditional probabilities (Brown et al., 1993). During decoding, we start with an initial target vocabu- lary containing the K most frequent words. Then, reading a few sentences at once, we arbitrarily re- place some of these initial words by the K most likely ones for each source word. 1 No matter how large the target vocabulary is, there will almost always be those words, such as proper names or numbers, that will appear only in the development or test set, but not during train- ing. To handle this difficulty, we replace un- known words in a manner similar to (Luong et al., 2015). More precisely, for every predicted out-of-vocabulary token (UNK), we determine its most likely origin by choosing the source word with the largest alignment weight ij (see Eq. (1).) We may then replace UNK by either the most likely word according to a dictionary, or simply by the source word itself. Depending on the lan- guage pairs, we used different heuristics according to performance on the development set. </chunk></section><section><heading>2.4 Integrating Language Models </heading><chunk>Unlike some data-rich language pairs, most of the translation tasks do not have enough paral- lel text to train end-to-end machine translation systems. To overcome with this issue of low- resource language pairs, external monolingual cor- pora is exploited by using the method of deep fu- sion (G  ulc  ehre et al., 2015). In addition to the RNNsearch model, we train a separate language model (LM) with a large mono- lingual corpus. Then, the trained LM is plugged into the decoder of the trained RNNsearch with an additional controller network which modulates the contributions from the RNNsearch and LM. The controller network takes as input the hidden state of the LM, and optionally RNNsearchs hid- den state, and outputs a scalar value in the range [0, 1]. This value is multiplied to the LMs hid- den state, controlling the amount of information coming from the LM. The combined model, the RNNsearch, the LM and the controller network, is jointly tuned as the final translation model for a low-resource pair. In our submission, we used recurrent neural net- work language model (RNNLM). More specif- ically, let s LM i be the hidden state of a pre- trained RNNLM and s TM i be that of a pre-trained RNNsearch at time i. The controller network is defined as g t = V g s LM t + W g s TM t + b g , 1 This step differs very slightly from (Jean et al., 2015), where the sentence-specific words were added on top of the K common ones instead of replacing them. 136 where is a logistic sigmoid function, v g , w g and b g are model parameters. The output of the con- troller network is multiplied to the LMs hidden state s LM i : p LM t = s LM t g t . The Generate phase in Sec. 2.2 is updated as, t i = U TM o s TM i1 + U LM o p LM t1 + V o y i1 + C o c i + b o . This lets the decoder fully use the signal from the translation model, while the the signal from the LM is modulated by the controller output. Among all the pairs of languages in WMT15, FinnishEnglish translation has the least amount of parallel text, having approximately 2M aligned sentences only. Thus, we use the deep fusion for the Fi-En in the official submission. However, we further experimented GermanEnglish, having the second least parallel text, and CzechEnglish, which has comparably larger data. We include the results from these two language pairs here for completeness. </chunk></section><section><heading>3 Experimental Details </heading><chunk>We now describe the settings of our experiments. Except for minor differences, all the settings were similar across all the considered language pairs. </chunk></section><section><heading>3.1 Data </heading><chunk>All the systems, except for the EnglishGerman (EnDe) system, were built using all the data made available for WMT15. The EnDe sys- tem, which was showcased in (Jean et al., 2015), was built earlier than the others, using only the data from the last years workshop (WMT14.) Each corpus was tokenized, but neither lower- cased nor truecased. We avoided badly aligned sentence pairs by removing any source-target sen- tence pair with a large mismatch between their lengths. Furthermore, we removed sentences that were likely written in an incorrect language, ei- ther with a simple heuristic for EnDe, or with a publicly available toolkit for the other language pairs (Shuyo, 2010). In order to limit the memory use during training, we only trained the systems with sentences of length up to 50 words only. Fi- nally, for some but not all models, we reshuffled the data a few times and concatenated the differ- ent segments before training. In the case of German (De) source, we performed compound splitting (Koehn and Knight, 2003), as implemented in the Moses toolkit (Koehn et al., 2007). For Finnish (Fi), we used Morfessor 2.0 for morpheme segmenta- tion (Virpioja et al., 2013) by using the default parameters. An Issue with Apostrophes In the training data, apostrophes appear in many forms, such as a straight vertical line (U+0027) or as a right sin- gle quotation mark (U+0019). The use of, for instance, the normalize-punctuation script 2 could have helped, but we did not use it in our exper- iments. Consequently, we encountered an issue of the tokenizer from the Moses toolkit not apply- ing the same rule for both kinds of apostrophes. We fixed this issue in time for CzechEnglish (CsEn), but all the other systems were affected to some degree, in particular, the system for DeEn. </chunk></section><section><heading>3.2 Settings </heading><chunk>We used the RNNsearch models of size identi- cal to those presented in (Bahdanau et al., 2015; Jean et al., 2015). More specifically, all the words in both target and source vocabularies were projected into a 620-dimensional vector space. Each recurrent neural network (RNN) had a 1000- dimensional hidden state. The models were trained with Adadelta (Zeiler, 2012), and the norm of the gradient at each update was rescaled (Pas- canu et al., 2013). For the language pairs other than CsEn and FiEn, we held the word em- beddings fixed near the end of training, as de- scribed in (Jean et al., 2015). With the very large target vocabulary technique in Sec. 2.3, we used 500K source and target words for the EnDe system, while 200K source and target words were used for the DeEn and CsEn systems. 3 During training we set be- tween 15K and 50K, depending on the hardware availability. As for decoding, we mostly used K = 30, 000 and K = 10. Given the small sizes of the FiEn corpora, we simply used a fixed vocabulary size of 40K to- kens to avoid any adverse effect of including ev- ery unique target word in the vocabulary. The in- clusion of every unique word would prevent the network from decoding out UNK at all, even if 2 http://www.statmt.org/wmt11/ normalize-punctuation.perl 3 This choice was made mainly to cope with the limited storage availability. 137 Language pair BLEU-c BLEU-c ranking Human ranking single ensemble constrained unconstrained EnCs 15.7 18.3 1/6 2/7 4/8 EnDe 22.4 24.8 1/11 1/13 1-2/16 CsEn 20.2 23.3 3/6 3/6 3-4/7 DeEn 25.6 27.6 6/9 6/10 6-7/13 FiEn 10.1 13.6 7/9 9/12 10/14 Table 2: Results on the official WMT15 test sets for single models and primary ensemble submissions. All our own systems are constrained. When ranking by BLEU, we only count one system from each submitter. Human rankings include all primary and online systems, but exclude those used in the CsEn tuning task. out-of-vocabulary words will assuredly appear in the test set. For each language pair, we trained a total of four independent models that differed in parameter ini- tialization and data shuffling, monitoring the train- ing progress on either newstest2012+2013, new- stest2013 or newsdevs2015. 4 Translations were generated by beam search, with a beam width of 20, trying to find the sentence with the highest log- probability (single model), or highest average log- probability over all models (ensemble), divided by the sentence length (Boulanger-Lewandowski et al., 2013). This length normalization addresses the tendency of the recurrent neural network to output shorter sentences. For FiEn, we augmented models by deep fu- sion with an RNN-LM. The RNN-LM, which was built using the LSTM units, was trained on the En- glish Gigaword corpus using the vocabulary com- prising of the 42K most frequent words in the En- glish side of the intersection of the parallel cor- pora of FiEn, DeEn and CsEn. Impor- tantly, we use the same RNN-LM for both FiEn, CsEn and DeEn. In the experiments with deep fusion, we used the randomly selected 2/3 of newsdev2015 as a validation set and the rest as a held-out set. In the case of DeEn, we used newstest2013 for validation and newstest2014 for test. For all language pairs except FiEn, we also simply built 5-gram language models, this time on all appropriate provided data, with the exception of the English Gigaword (Heafield, 2011). In our contrastive submissions only, we re-ranked our 20- best lists with the LM log-probabilities, once again divided by sentence length. The relative weight of the language model was manually chosen to max- 4 For EnDe, we created eight semi-independent models. See (Jean et al., 2015) for more details. imize BLEU on the development set. 4 Results Results for single systems and primary ensem- ble submissions are presented in Table 2. 5 When translating from English to another language, neu- ral machine translation works particularly well, achieving the best BLEU-c scores among all the constrained systems. On the other hand, NMT is generally competitive even in the case of trans- lating to English, but it not yet as good as well as the best SMT systems according to BLEU. If we rather rely on human judgement instead of au- tomated metrics, the NMT systems still perform quite well over many language pairs, although they are in some instances surpassed by other sta- tistical systems that have slightly lower BLEU scores. In our contrastive submissions for CsEn and DeEn where we re-ranked 20-best lists with a 5- gram language model, BLEU scores went up mod- estly by 0.1 to 0.5 BLEU, but interestingly transla- tion error rate (TER) always worsened. One possi- ble drawback about the manner we integrated lan- guage models here is the lack of translation mod- els in the reverse direction, meaning we do not implicitely leverage the Bayes rule as most other translation systems do. In our further experiments, which are not part of our WMT15 submission, for single models we observed the improvements of approximately 1.0/0.5 BLEU points for dev/test in {Cs,De}En tasks, when we employ deep fusion for incorporat- ing language models. 6 5 Also available at http://matrix.statmt.org/ matrix/ 6 Improvements are for single models only. See (G  ulc  ehre et al., 2015) for more details. 138 5 Conclusion We presented the MILA neural machine trans- lation (NMT) systems for WMT15, using the encoderdecoder model with the attention mech- anism (Bahdanau et al., 2015) and the recent de- velopments in NMT (Jean et al., 2015; G  ulc  ehre et al., 2015). We observed that the NMT sys- tems are now competitive against the conventional SMT systems, ranking first by BLEU among the constrained submission on both the EnCs and EnDe tasks. In the future, more analysis is needed on the influence of the source and target languages for neural machine translation. For in- stance, it would be interesting to better understand why performance relative to other approaches was somewhat weaker when translating into English, or how the amount of reordering influences the translation quality of neural MT systems. Acknowledgments The authors would like to thank the develop- ers of Theano (Bergstra et al., 2010; Bastien et al., 2012). We thank Kelvin Xu, Bart van M  errienboer, Dzmitry Bahdanau and Etienne Si- mon for their help. We also thank the two anony- mous reviewers and everyone who contributed to the manual evaluation campaign. We acknowl- edge the support of the following agencies for re- search funding and computing support: NSERC, Calcul Qu  ebec, Compute Canada, the Canada Re- search Chairs, CIFAR, Samsung and TUBITAK. </chunk></section><section><heading>References </heading><chunk>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2015. Neural machine translation by jointly learning to align and translate. In ICLR2015, arXiv:1409.0473. Fr  ed  eric Bastien, Pascal Lamblin, Razvan Pascanu, James Bergstra, Ian J. Goodfellow, Arnaud Berg- eron, Nicolas Bouchard, and Yoshua Bengio. 2012. Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop. Yoshua Bengio and Jean-S  ebastien S  en  ecal. 2008. Adaptive importance sampling to accelerate train- ing of a neural probabilistic language model. IEEE Trans. Neural Networks, 19(4):713722. James Bergstra, Olivier Breuleux, Fr  ed  eric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Des- jardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. 2010. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientific Computing Conference (SciPy), June. Oral Presentation. Nicolas Boulanger-Lewandowski, Yoshua Bengio, and Pascal Vincent. 2013. Audio chord recognition with recurrent neural networks. In ISMIR. Peter F Brown, Vincent J Della Pietra, Stephen A Della Pietra, and Robert L Mercer. 1993. The mathemat- ics of statistical machine translation: Parameter esti- mation. Computational linguistics, 19(2):263311. Stanley F. Chen and Joshua T. Goodman. 1998. An empirical study of smoothing techniques for lan- guage modeling. Technical Report TR-10-98, Com- puter Science Group, Harvard University. Kyunghyun Cho, Bart van Merrienboer, Caglar Gul- cehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using RNN encoder-decoder for statistical machine translation. In Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014), October. Ian Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. 2013. Max- out networks. In Proceedings of The 30th Inter- national Conference on Machine Learning, pages 13191327. C aglar G  ulc  ehre, Orhan Firat, Kelvin Xu, Kyunghyun Cho, Lo  c Barrault, Huei-Chi Lin, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2015. On using monolingual corpora in neural machine trans- lation. CoRR, abs/1503.03535. Kenneth Heafield. 2011. KenLM: faster and smaller language model queries. In Proceedings of the EMNLP 2011 Sixth Workshop on Statistical Ma- chine Translation, pages 187197, Edinburgh, Scot- land, United Kingdom, July. S. Hochreiter and J. Schmidhuber. 1997. Long short- term memory. Neural Computation, 9(8):1735 1780. S  ebastien Jean, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. 2015. On using very large tar- get vocabulary for neural machine translation. In Proceedings of ACL. Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent continuous translation models. In Proceedings of the ACL Conference on Empirical Methods in Nat- ural Language Processing (EMNLP), pages 1700 1709. Association for Computational Linguistics. Philipp Koehn and Kevin Knight. 2003. Empirical methods for compound splitting. In Proceedings of the tenth conference on European chapter of the As- sociation for Computational Linguistics-Volume 1, pages 187193. Association for Computational Lin- guistics. 139 Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ond rej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL 07, pages 177180, Stroudsburg, PA, USA. Association for Computational Linguistics. Minh-Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, and Wojciech Zaremba. 2015. Addressing the rare word problem in neural machine translation. In Proceedings of ACL. Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Com- putational Linguistics, ACL 02, pages 311318, Stroudsburg, PA, USA. Association for Computa- tional Linguistics. Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. 2013. On the difficulty of training recurrent neural networks. Mike Schuster and Kuldip K Paliwal. 1997. Bidirec- tional recurrent neural networks. Signal Processing, IEEE Transactions on, 45(11):26732681. Nakatani Shuyo. 2010. Language detection library for java. Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to sequence learning with neural net- works. In NIPS2014. Sami Virpioja, Peter Smit, Stig-Arne Gr  onroos, Mikko Kurimo, et al. 2013. Morfessor 2.0: Python imple- mentation and extensions for morfessor baseline. Matthew D Zeiler. 2012. ADADELTA: An adap- tive learning rate method. arXiv:1212.5701 [cs.LG]. 140 </chunk></section></sec_map>