<sec_map><section><chunk>Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 112119, Lisboa, Portugal, 17-18 September 2015. c 2015 Association for Computational Linguistics. The AFRL-MITLL WMT15 System: Theres More than One Way to Decode It! Jeremy Gwinnup , Timothy Anderson, Grant Erdmann, Katherine Young , Christina May Air Force Research Laboratory jeremy.gwinnup.ctr,timothy.anderson.20, grant.erdmann,katherine.young.1.ctr, christina.may.3.ctr@us.af.mil Michaeel Kazi , Elizabeth Salesky , Brian Thompson MIT Lincoln Laboratory michaeel.kazi,elizabeth.salesky, brian.thompson@ll.mit.edu </chunk></section><section><heading>Abstract </heading><chunk>This paper describes the AFRL-MITLL statistical MT systems and the improve- ments that were developed during the WMT15 evaluation campaign. As part of these efforts we experimented with a num- ber of extensions to the standard phrase- based model that improve performance on the Russian to English translation task cre- ating three submission systems with differ- ent decoding strategies. Out of vocabulary words were addressed with named entity postprocessing. </chunk></section><section><heading>1 Introduction </heading><chunk>As part of the 2015 Workshop on Machine Translation (WMT15) shared translation task, the MITLL and AFRL human language technol- ogy teams participated in the RussianEnglish translation task. Our machine translation sys- tems represent enhancements to both our sys- tems from IWSLT2014 (Kazi et al., 2014) and WMT14 (Schwartz et al., 2014), the addition of hierarchical decoding systems (Hoang and Koehn, 2008), neural network joint models (Devlin et al., 2014) and the utilization of Drem (Erdmann and Gwinnup, 2015), a method of scaled derivative- free trust-region optimization, during the system tuning process. </chunk></section><section><heading>2 System Description </heading><chunk>We submitted systems for the Russian-to-English machine translation shared task. In all submitted systems, we used either phrase-based or hierarchi- cal variants of the moses decoder (Koehn et al., This work is sponsored by the Air Force Research Labo- ratory under Air Force contract FA-8650-09-D-6939-029. This work is sponsored by the Air Force Research Labo- ratory under Air Force contract FA-8721-05-C-0002. 2007). As in previous years, our submitted sys- tems used only the constrained data supplied when training. </chunk></section><section><heading>2.1 Data Usage </heading><chunk>In training our RussianEnglish systems we uti- lized the following corpora to train translation and language models: Yandex 1 , Commoncrawl (Smith et al., 2013), LDC Gigaword English v5 (Parker et al., 2011) and News Commentary. The Wikipedia Headlines corpus 2 was reserved to train named en- tity recognizers. </chunk></section><section><heading>2.2 Data Preprocessing </heading><chunk>As with our WMT14 submission systems, prepro- cessing to address issues with the training data was required to ensure optimal system performance. Unicode characters in the private use, control char- acter(C0, C1, zero-width, non-breaking, joiner, directionality and paragraph markers), and unal- located ranges were removed. Punctuation nor- malization and tokenization using Moses prepro- cessing scripts were then applied before lower- casing the data. The Commoncrawl corpus was further processed as in Schwartz et al. (2014) to exclude wrong-language text and to normalize mixed-alphabet spellings. </chunk></section><section><heading>2.3 Factored Data Generation </heading><chunk>We generated a class-factored version of the paral- lel RussianEnglish training data by using mkcls to produce 600 word classes for each side of the data. The factored data was then used to create a factored translation model and an in-domain class language model (Brown et al., 1993) for the En- glish portion. 1 https://translate.yandex.ru/corpus?lang=en 2 http://statmt.org/wmt15/wiki-titles.tgz 112 </chunk></section><section><heading>2.4 Phrase and Rule Table Training </heading><chunk>Phrase tables and rule tables were trained on the preprocessed data using scripts provided with the moses distribution. Both rule tables and phrase tables utilized Good-Turing discounting (Gale, 1995). Hierarchical lexicalized reordering mod- els (Galley and Manning, 2008) were also trained for use in the phrase-based systems. An additional phrase table was trained on the lemmatized forms of the Russian training data. These lemmatized forms were generated by the mystem 3 tool. </chunk></section><section><heading>2.5 Language Model Training </heading><chunk>The English data sources listed in 2.1 were used to train a very large 6-gram language model (BigLM15). The English portion of the parallel data was processed into class form as outlined in 2.3 to generate an in-domain 600 class language model. kenlm (Heafield, 2011) was used to train these 6-gram models. These models were then bi- narized and stored on local solid-state disks for each machine in our cluster to improve load time and reduce fileserver traffic. </chunk></section><section><heading>2.6 Operation Sequence Models </heading><chunk>Using both the Russian and English data generated in 2.3, we trained order-5 Operation Sequence models (Durrani et al., 2011) for both the surface and class-factored forms of the data. These models improve translation quality by introducing infor- mation on the sequence of operations occuring at both the surface and class factor level. These mod- els were then used in our factored phrase-based system. </chunk></section><section><heading>2.7 Neural Network Joint Models </heading><chunk>Neural network joint models (Devlin et al., 2014) are neural network based language models with a source window context. We trained these mod- els on the alignments produced by mgiza (Gao and Vogel, 2008) over the parallel training data and then used them to rescore n-best lists. As in (Devlin et al., 2014), we trained four different models. The standard model is source-to-target, left-to-right, (s2t, ltr) which evaluates p(t i |T, S) with target window T = (t i1 , t i2 , . . . , t in ) and S = (s km , . . . s k , . . . , s k+m ), where s k is word- aligned to t i . The four permutations of this are de- fined by (a) whether to count upwards from i, in- 3 https://tech.yandex.ru/mystem stead of downwards (this is left-to-right vs right-to- left), and (b) whether to swap the sources and tar- gets entirely (source-to-target vs target-to-source). We experimented with NNJM decoding (via a simple feature function in Moses). We achieved some benefit (+0.48 BLEU) with this approach but rescoring a single NNJM source-to-target on 200- best lists produced better results in this case (+0.90 BLEU). This was on a single system tuned on newstest2013, tested on newstest2014 (base- line 29.07 BLEU). In testing, 2-hidden layer rescoring models outperformed the 1-hidden layer decoding model. The vocabulary for the NNJMs were created by using all words that appeared at least a certain number of times in the training data. We experi- mented with minimum counts of 20 and 25. Us- ing 20, our vocabulary was approximately 80,000 Russian words and 40,000 English; with 25, it was 70,000 and 34,000, respectively. We compared rescoring with a single, standard model (s2t, l2r) to rescoring with all directions with results listed in Table 1. Baseline 1 NNJM 4 NNJMs 20 25 20 25 max 27.71 27.90 28.05 27.90 28.07 mean 27.48 27.61 27.81 27.67 27.60 Table 1: NNJM Rescoring on newstest2015, optimizing on newstest2014, case-insensitive BLEU. </chunk></section><section><heading>2.8 Processing of Unknown Words </heading><chunk>In our submission systems, we allowed words unknown to the decoder to be passed through to the translated output. We developed three post-processing techniques to address unknown words: named entity (NE) tagging and transla- tion (2.8.2), permissive NE translation (2.8.3), and selective transliteration of the remaining OOV words (2.8.4). The first two techniques rely on our in-house transliteration mining of NE pairs, which is described in 2.8.1. We applied all three post-processing steps to the output of our factored phrase-based submis- sion system; due to time constraints, only the last two steps were applied to the output of our phrase- based and hierarchical submission systems. Score improvements in uncased BLEU are re- ported in Table 2. We see that application of 113 permissive lookup and selective transliteration yielded an improvement of +0.48 BLEU versus a baseline system, while the application of named entity tagging and translation, permissive lookup and selective transliteration yielded a +0.57 BLEU gain. 2.8.1 Transliteration Mining Both NE processing steps (2.8.2 and 2.8.3) make use of a NE pairs list that we developed through transliteration mining of the Russian- English CommonCrawl. In transliteration mining (Kumaran et al., 2010; Zhang et al., 2012), we use transliteration as a tool to detect similar-sounding words in the parallel text that may correspond to names. Our process for detecting transliterated NE is generative and rule-based. We used mystem to tag NE in the Russian text, and then used capital- ization and transliteration as clues to find match- ing NE in the parallel English sentences. English words were considered candidate matches if they were capitalized, but not sentence-initial; we ex- cluded all-caps words, since acronyms often do not transliterate well. We also required the English candidate words to match the initial sound of the Russian NE. We checked the initial sound match by translit- erating the Russian words according to the text- book values of the Russian letters, and then check- ing for matches with the English spellings, allow- ing certain spelling variations. These variations include instances where Russian lacks an English sound, and substitutes a similar sound (e.g., En- glish h written in Russian with the letters for x or g, and English w written with the Russian letters for v or u), as well as common English spelling al- ternations like n/kn, s/c, c/k, etc. An iterative process of refining spelling alter- nations was applied by manual observation of known NE pairs that were not matched via exist- ing rules; notably, this introduced spelling varia- tions for words originating from a third language. For example, English j typically represents [d] but may also indicate [h] in words of Spanish ori- gin, so we need to allow the spelling alternation x/j . Similarly, the letters gi may represent [d] in Italian names like Giovanni, so we need to allow transliterated Russian dzh to match English gi. At this point in the transliteration mining pro- cess, we have derived a list of capitalized English words that have initial spellings potentially match- ing the initial sound of the Russian NE word. If the English sentence contains more than one such can- didate, we select the word with the smallest edit distance from the Russian transliteration, using a length-normalized Levenshtein distance. For this calculation, any spelling variation counts as an edit distance change, so we penalize variations such as k for c. For NE tagging and translation (2.8.2), we re- turn only the NE pairs with zero edit distance. For permissive NE translation, we allow some varia- tion, as described in 2.8.3. 2.8.2 Named Entity Tagging and Translation The named entity post-process uses Russian English pairs in the combined names and titles lists from the Wikipedia Headlines corpus (the Wiki pairs list) and the transliteration-mined list (2.8.1) to replace unknown words with En- glish equivalents. We began by stemming each list to remove Russian noun and adjective end- ings. To the Wiki pairs list, we added additional pairs yielded by replacing word-internal punctu- ation marks in existing Wiki pairs with spaces. We used giza++ (Och and Ney, 2003) to align RussianEnglish phrases from the Wiki list. We then used these alignments to start a generated list of pairs with only one Russian word and one En- glish word in a pair. Of the aligned pairs, we only included pairs that were aligned with one another three or more times. Only one-to-one alignments would count toward the three alignment rule. We also removed entries where the English word in the pair occurred in a list of stop-words as well as where the English word consisted of only digits. To the generated list, we also added pairs directly from the Wiki list with both single Russian words and single English words. Finally, we also added the highest quality pairs from the transliteration- mined list. Upon encountering a single word without word- internal punctuation, the system first searches through the generated list, and returns a list of found guesses. If no items are found in the gen- erated list, the Wiki list is then searched. If still no guesses are found, then the transliteration-mined list is searched. The same process occurs for a word containing word-internal punctuation, but af- ter a failed iteration of the search process, the punc- tuation is replaced with a space and the Wiki lists are searched. Finally if that iteration fails, then the search process occurs on each individual word and a concatenation of English definitions is added 114 to the guess list for every possible combination of guesses for each component word. An English lan- guage model is used to choose among the guesses. 2.8.3 Permissive Named Entity Translation Permissive NE look-up is applied to translate OOV words that remain untranslated after NE tagging and translation (2.8.2), or when the NE tagging and translation step is unavailable. In this second step, we expand the NE pairs list to include pairs with greater edit distance when they are validated by repeat occurrence. While the NE tagging and translation step only uses transliteration-mined NE pairs which match exactly, the permissive step allows NE pairs that have some spelling variation. We apply two addi- tional restrictions to ensure good quality matches, length disparity and instance ratio. We restrict the output to words which come from sentences that do not differ too much in length. A large length disparity suggests a sentence alignment error in the parallel text, which would make the NE match un- reliable. We also restrict the output to words which are fairly frequent among other matches for the same Russian words, calculating an instance ratio as the number of times we see this English word with this Russian word, divided by the total number of English matches we record for this Russian word. Rare instances may be mistakes or spelling vari- ants that we would prefer to exclude. For example, we found the Russian name matched with English Constantine 117 times, and matched with the spelling Konstantine only 1 time, so we do not want to collect /Konstantine as a NE pair. We keep the NE pairs if: </chunk></section><section><heading>1. The length-normalized edit distance &lt; 0.2 </heading><chunk>2. The length-normalized edit distance falls be- tween 0.2 and 0.5, inclusive, and sentence length disparity &lt; 2 and instance ratio &gt; 0.01 With these restrictions, we derived 32,560 poten- tial NE pairs. Subsequently, an additional transliteration min- ing step was conducted, to collect NE pairs from any capitalized Russian words, not just the words tagged as NE by mystem. We excluded Russian acronyms, sentence-initial words, and personal pronouns (which are capitalized in some styles of Russian writing). Applying the previously de- scribed restrictions for edit distance, instance ra- tio, and sentence length disparity, we derived an additional 22,370 capitalized-word NE pairs. The combined mystem tagged and capitalized-word NE pairs lists were used in the permissive transla- tion of OOV words, considering both the original form of the Russian OOV word and its stemmed form. For the phrase-based and hierarchical systems, which were processed without the NE tagging and translation step, the wiki pairs list was added to the mined NE pairs list for permissive OOV transla- tion. 2.8.4 Selective Transliteration of Remaining Out-of-Vocabulary Words As a final post-processing step, we transliterate some of the remaining OOV words. We attempt to distinguish OOV NE from common words, drop- ping common words and transliterating names. We hypothesize that retaining transliterated forms of NE will improve readability, even if the output is not a direct match to the English reference. We attempt to distinguish NE from common words on the basis of capitalization in the Russian source file. Capitalized words that do not begin a sentence are assumed to be NE, and are translit- erated. For example, transliteration is the source of the name Kostenok in first example sentence shown in Figure 1. Lowercased words, and cap- italized words that begin a sentence, are assumed to be common words and are dropped from the out- put. </chunk></section><section><heading>3 Results </heading><chunk>We submitted three systems for evaluation, each employing a different decoding strategy: tradi- tional phrased-based, hierarchical, and factored phrased-based. Each system is described be- low. Automatically scored results reported in BLEU (Papineni et al., 2002) for our submission systems can be found in Table 3. Finally, as part of WMT15, the results of our submission systems listed in Tables 3 were ranked by monolingual human judges against the machine translation output of other WMT15 participants. These judgements are reported in WMT (2015). </chunk></section><section><heading>3.1 Phrased-Based </heading><chunk>We used a standard phrase-based approach, using lowercased data. The lemma-based phrase table 115 System Process Applied baseline BLEU postproc BLEU BLEU phrase-based PermLookup + SelTranslit 27.72 28.20 +0.48 hiero PermLookup + SelTranslit 27.43 27.91 +0.48 pb-factored NEProc + PermLookup+ SelTranslit 27.18 27.75 +0.57 Table 2: NE post-processing improvement measured in uncased BLEU described in 2.4 was used as a backoff phrase table. We trained a hierarchical lexicalized re- ordering model, and used two separate class based (factored) language models; one using 600 classes on the in-domain target-side parallel data, and the other using the LDC Gigaword-English v5 NYT corpus. N-best lists from moses were rescored with 4-way NNJMs, and the system weights were tuned with PRO (Hopkins and May, 2011). Selec- tive transliteration as described in 2.8.4 was then applied to the decoder output. </chunk></section><section><heading>3.2 Hierarchical </heading><chunk>New for this year, we trained a hierarchical system using the same parallel data as our phrase-based systems. The rule table was created as outlined in 2.4 and then filtered to only contain rules relat- ing to the Russian content of the newstest test set for years 20122015. This filtering was per- formed in order to reduce the size of the rule table for both system memory requirements and expedi- ency. The incremental-search algorithm (Heafield et al., 2013) and BigLM15 were used to decode the dev (newstest2014) and test (newstest2015) data. Drem was employed to tune feature weights, optimizing the sum of the expected sentence- level BLEU and expected sentence-level Meteor (Denkowski and Lavie, 2014) metrics. Finally, se- lective transliteration was employed as described in 2.8.4. </chunk></section><section><heading>3.3 Factored Phrase-Based </heading><chunk>For our last system, we used a factored phrase- based approach (Koehn and Hoang, 2007) where the surface form of the training data was aug- mented with word classes. These classes were generated on the parallel training data outlined in 2.4 using mkcls to group the words into 600 classes for both English and Russian portions of the parallel training corpus. A phrase table and hi- erarchical reordering model was then trained us- ing the moses training process on both the surface form and the class factor. Order-5 operation se- quence models were separately trained on the sur- face forms and the class factors. An order-6 class- factor LM (Shen et al., 2006) was also trained on the English portion of the parallel training data to supplement the use of BigLM15. NNJMs as out- lined in 2.7 were used to rescore the n-best lists from the decode. Following this rescoring, Drem was employed to tune feature weights, optimizing expected corpus-level BLEU (Smith and Eisner, 2006). After optimization and decoding of the test set, remaining unknown words were processed as described in 2.8.2 and 2.8.4. System Cased BLEU Uncased BLEU phrase-based 27.0 28.2 hiero 26.7 27.9 pb-factored 26.4 27.8 Table 3: MT Submission Systems decoding newstest2015 </chunk></section><section><heading>4 Discussion </heading><chunk>Our three submitted systems all scored similarly against the official test set. Manual examination of our systems output shows that there are significant differences in sentence structure and content. </chunk></section><section><heading>4.1 Comparing Submitted Systems for Similarity </heading><chunk>We scored one system output against another (as reference) with mteval13a.pl in both directions as BLEU scores are not symmetric. Results are listed in Table 4. Interestingly, the factored phrase- based and hierarchical systems were more similar to each other than to the traditional phrase-based system. This suggests that the addition of class factors serves a similar function to the use of hi- erarchical decoding. </chunk></section><section><heading>4.2 A Closer Analysis of Performance between Submission Systems </heading><chunk>We now examine two sentences translated with each of our submission systems and compare them with the supplied reference translation and a literal 116 Test Ref BLEU PB Hiero 57.18 PBFac Hiero 76.34 Hiero PB 57.09 PBFac PB 60.54 PB PBFac 60.47 Hiero PBFac 70.18 Table 4: Submission system similarity measured in uncased BLEU translation. These comparisons are shown in Fig- ure 1. In the first sentence, the reference transla- tion shows a reordering of the first clause to the end. The phrase-based system drops this clause. The pb-factored system has informed in- stead of reported which shifts the meaning; per- haps the translation was influenced by the fluent but different-meaning phrase informed the Minis- ter. The hierarchical system follows the original order of the source sentence clauses; while miss- ing the, it reads the best overall. In the second sentence, school (ad- jective) is the probable source of school, academic, and teach. The phrase-based system handles this word best; the phrase-based factored system gen- erates academic and teach but separates them; the hierarchical system generates year to teach. The hierarchical system does the best job with no ear- lier than October. The phrase-based factored sys- tem generates no earlier and October but reorders them (perhaps influenced by the common phrase, in October); and the phrase-based system creates before October, which reverses the meaning. The phrase-based system would have read best here, had it not neglected the negative particle. </chunk></section><section><heading>5 Conclusion </heading><chunk>In this paper, we present data preparation and pro- cessing techniques for our RussianEnglish sub- missions to the 2015 Workshop on Machine Trans- lation (WMT15) shared translation task. Our sub- missions examine three different decoding strate- gies and the effectiveness of sophisticated han- dling of unknown words. While scoring similarly, each system produced markedly different output. Opinions, interpretations, conclusions and recommenda- tions are those of the authors and are not necessarily endorsed by the United States Government. Cleared for public release </chunk></section><section><heading>References </heading><chunk>Peter Brown, Vincent Della Pietra, Stephen Della Pietra, and Robert Mercer. 1993. The mathemat- ics of statistical machine translation: parameter esti- mation. Computational Linguistics, 19(2):263311, June. Michael Denkowski and Alon Lavie. 2014. Meteor universal: Language specific translation evaluation for any target language. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 376380, Baltimore, Maryland, USA, June. Associ- ation for Computational Linguistics. Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, and John Makhoul. 2014. Fast and robust neural network joint models for sta- tistical machine translation. In Proceedings of the 52nd Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), Pro- ceedings of the ACL, Long Papers, pages 1370 1380, Baltimore, MD, USA. Nadir Durrani, Helmut Schmid, and Alexander Fraser. 2011. A joint sequence translation model with in- tegrated reordering. In Proceedings of the 49th An- nual Meeting of the Association for Computational Linguistics (ACL 11), pages 10451054, Portland, Oregon, June. Grant Erdmann and Jeremy Gwinnup. 2015. Drem: The AFRL submission to the WMT15 tuning task. In Proceedings of the Tenth Workshop on Statistical Machine Translation (WMT15), Lisbon, Portugal, September. To appear. William A. Gale. 1995. Good-turing smoothing with- out tears. Journal of Quantitative Linguistics, 2. Michel Galley and Christopher D. Manning. 2008. A simple and effective hierarchical phrase reordering model. In Proceedings of the Conference on Em- pirical Methods in Natural Language Processing, EMNLP 08, pages 848856. Qin Gao and Stephan Vogel. 2008. Parallel imple- mentations of word alignment tool. In Software En- gineering, Testing and Quality Assurance for Natu- ral Language Processing, pages 4957, Columbus, Ohio, June. Kenneth Heafield, Philipp Koehn, and Alon Lavie. 2013. Grouping language model boundary words to speed k-best extraction from hypergraphs. In Pro- ceedings of the 2013 Conference of the North Amer- ican Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 958968, Atlanta, Georgia, USA, June. Kenneth Heafield. 2011. KenLM: faster and smaller language model queries. In Proceedings of the EMNLP 2011 Sixth Workshop on Statistical Machine on 11 Jun 2015. Originator reference number RH-15-114103. Case number 88ABW-2015-2973. 117 Example 1 source: literal: Of this reported minister education and science self-proclaimed republic, Igor Kostenok reference: The Minister of Education and Science for the self-proclaimed republic, Igor Kostenok, reported this. phrase-based: The Minister of Education and Science of the self-declared republic, Igor Kostenok. pb-factored: This was informed the Minister of Education and Science of the self-declared republic, Igor Kostenok. hierarchical: This was announced by Minister of Education and Science of the self-proclaimed republic Igor Kostenok. Example 2 source: . literal: School year in DNR begins not before October. reference: The academic school year in the DPR will begin no earlier than October. phrase-based: The school year in DNR will begin before October. pb-factored: Academic year in October, teach will begin. hierarchical: School year to teach will begin no earlier than October. Figure 1: Comparison of Submission System Translation Output Translation, pages 187197, Edinburgh, Scotland, United Kingdom, July. Hieu Hoang and Philipp Koehn. 2008. Design of the moses decoder for statistical machine translation. In Software Engineering, Testing, and Quality As- surance for Natural Language Processing, SETQA- NLP 08, pages 5865. Mark Hopkins and Jonathan May. 2011. Tuning as ranking. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP 11), pages 13521362, Edinburgh, Scot- land, U.K. Michaeel Kazi, Elizabeth Salesky, Brian Thompson, Jessica Ray, Michael Coury, Tim Shen, Wade An- derson, Grant Erdmann, Jeremy Gwinnup, Kather- ine Young, Brian Ore, and Michael Hutt. 2014. The MIT-LL/AFRL IWSLT-2014 MT system. In Pro- ceedings of the 11th International Workshop on Spo- ken Language Translation (IWSLT14), pages 65 72, Lake Tahoe, California, December. Philipp Koehn and Hieu Hoang. 2007. Factored trans- lation models. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Lan- guage Processing and Computational Natural Lan- guage Learning (EMNLP-CoNLL), pages 868876. Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL 07, pages 177180, Stroudsburg, PA, USA. Association for Computational Linguistics. A Kumaran, Mitesh M. Khapra, and Haizhou Li. 2010. Whitepaper of news 2010 shared task on translitera- tion mining. In Proceedings of the 2010 Named Enti- ties Workshop, pages 2938, Uppsala, Sweden, July. Association for Computational Linguistics. Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):1951, March. Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL 02), pages 311 318, Philadelphia, Pennsylvania, July. Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. 2011. English Gigaword Fifth Edi- tion LDC2011T07. Philadelphia: Linguistic Data Consortium. Lane Schwartz, Timothy Anderson, Jeremy Gwinnup, and Katherine Young. 2014. Machine translation and monolingual postediting: The AFRL WMT-14 system. In Proceedings of the Ninth Workshop on Statistical Machine Translation (WMT14), pages 186194, Baltimore, Maryland, USA, June. Wade Shen, Richard Zens, Nicola Bertoldi, and Mar- cello Federico. 2006. The JHU workshop 2006 IWSLT system. In Proc. of the International Workshop on Spoken Language Translation, Kyoto, Japan, November. 118 David A. Smith and Jason Eisner. 2006. Minimum risk annealing for training log-linear models. In Pro- ceedings of the COLING/ACL 2006 Main Confer- ence Poster Sessions, pages 787 794, Sydney, Aus- tralia. Jason R. Smith, Herve Saint-Amand, Magdalena Pla- mada, Philipp Koehn, Chris Callison-Burch, and Adam Lopez. 2013. Dirt cheap web-scale paral- lel text from the common crawl. In Proceedings of the 51st Annual Meeting of the Association for Com- putational Linguistics (ACL 13), pages 13741383, Sofia, Bulgaria, August. WMT. 2015. Findings of the 2015 Workshop on Sta- tistical Machine Translation. In Proceedings of the Tenth Workshop on Statistical Machine Translation (WMT 15), Lisbon, Portugal, September. Min Zhang, Haizhou Li, A. Kumaran, and Ming Liu, 2012. Proceedings of the 4th Named Entity Work- shop (NEWS) 2012, chapter Report of NEWS 2012 Machine Transliteration Shared Task, pages 1020. Association for Computational Linguistics. 119 </chunk></section></sec_map>