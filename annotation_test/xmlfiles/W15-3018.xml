<sec_map><section><chunk>Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 158163, Lisboa, Portugal, 17-18 September 2015. c 2015 Association for Computational Linguistics. The RWTH Aachen German-English Machine Translation System for WMT 2015 Jan-Thorsten Peter, Farzad Toutounchi, Joern Wuebker and Hermann Ney Human Language Technology and Pattern Recognition Group Computer Science Department RWTH Aachen University D-52056 Aachen, Germany &lt;surname&gt;@cs.rwth-aachen.de Abstract This paper describes the statistical machine translation system developed at RWTH Aachen University for the GermanEnglish translation task of the EMNLP 2015 Tenth Workshop on Statistical Machine Translation (WMT 2015). A phrase-based machine transla- tion system was applied and augmented with hierarchical phrase reordering and word class language models. Further, we ran discriminative maximum expected BLEU training for our system. In addition, we utilized multiple feed-forward neural network language and translation models and a recurrent neural network language model for reranking. </chunk></section><section><heading>1 Introduction </heading><chunk>For the WMT 2015 shared translation task 1 , RWTH utilized a state-of-the-art phrase-based translation system. We participated in the GermanEnglish translation task. The system included a hierarchical reordering model, a word class (cluster) language model, and discrimina- tive maximum expected BLEU training. Further, we reranked the nbest lists produced by our sys- tem with three feed-forward neural network mod- els and a recurrent neural language model. This paper is structured as follows: First, we briefly describe our preprocessing pipeline for the language pair GermanEnglish in Section 2, which is based on our 2014 pipeline. Next, morpho-syntactic analysis for preprocessing the data is described in Section 2.3. Different align- ment methods are discussed in Section 3. In Sec- tion 4, we present a summary of all methods used in our submission. More details are given about 1 http://www.statmt.org/wmt15/ translation-task.html the language models (Section 4.2), maximum ex- pected BLEU training (Section 4.4), the hierarchi- cal reordering model (Section 4.5), feed-forward neural network training (Section 4.6), and recur- rent neural network language model (Section 4.7). Experimental results are discussed in Section 5. We conclude the paper in Section 6. </chunk></section><section><heading>2 Preprocessing </heading><chunk>In this section we briefly describe our preprocess- ing pipeline, which is a modification of our WMT 2014 GermanEnglish preprocessing pipeline (Peitz et al., 2014). </chunk></section><section><heading>2.1 Categorization </heading><chunk>We worked on the categorization of the digits and written numbers for the translation task. All writ- ten numbers were categorized. As the training data and also the test sets contain several errors for numbers in the source as well as in the target part, we put effort into producing correct English num- bers. In addition, , and . marks were inverted in most cases, as in German the former mark is the decimal mark and the latter is the thousand sepa- rator. </chunk></section><section><heading>2.2 Remove Foreign Languages </heading><chunk>The WMT GermanEnglish Common Crawl cor- pora contains bilingual sentence pairs with non- German source or non-English target sentences. By using an ASCII filtering, we removed all sen- tences with more than 5% non-ASCII characters from the Common Crawl corpus. Chinese, Arabic and Russian are among the languages which can be easily filtered by deleting the sentences con- taining too many non-ASCII words. Our experi- ments showed that the translation quality does not change by removing sentences with wrong lan- guages. Nevertheless, this method reduced the training data size and also the vocabulary size without introducing any degradation in translation 158 Table 1: Comparison of a simple GIZA++ alignment vs. merging multiple alignments. Even though the multiple alignment approach did not improve the GIZA++ alignment for the baseline system, it improved translation quality in combination with a neural network joint model (NNJM). BLEU and TER are given in percentage. newstest2011 newstest2012 newstest2013 newstest2014 BLEU TER BLEU TER BLEU TER BLEU TER GIZA++ 23.1 58.8 23.7 58.2 26.5 54.7 25.9 54.2 + NNJM 23.3 58.4 24.0 57.7 26.6 54.3 26.2 53.7 Multiple alignment 23.0 58.9 23.8 58.2 26.6 54.6 25.9 54.2 + NNJM 23.3 58.4 24.1 57.8 27.0 54.3 26.3 53.8 quality. Further, this method prevents us from gen- erating words from these languages. </chunk></section><section><heading>2.3 Compound Splitting and POS-based Word Reordering </heading><chunk>We reduced the source vocabulary size for the GermanEnglish translation and split the Ger- man compound words with the frequency-based method described in Koehn and Knight (2003). To reduce translation complexity, we employed the long-range part-of-speech based reordering rules proposed by Popovi  c and Ney (2006). In this re- gard, we did no further morphological analysis in our preprocessing pipeline. </chunk></section><section><heading>3 Alignment </heading><chunk>We experimented with creating multiple align- ments and merging them via a majority vote. For the majority voting to work in a meaningful way we need obviously more than two different align- ments. A larger number of alignments gives us more confidence that the alignment points are cor- rect. To create these different alignments, we used fast align (Dyer et al., 2013) and two imple- mentations of GIZA++ (Och and Ney, 2003). The alignment was trained in both source to target di- rection and target to source direction. To double the number of alignments, we trained each setup also with a reverse ordered source side and re- versed it back after the alignment process finished (Freitag et al., 2013). Using a reversed source side usually creates a different alignment since the word order influences the results of fast align and GIZA++. This gave us a total of 12 differ- ent alignments (three toolkits two translation directions two source side direction). These alignments were merged by keeping all alignment points generated by at least 5 of the methods. We compared this setup with an alignment gen- erated by GIZA++. The voting setup did not im- prove directly on the baseline system as shown in Table 1. However, in combination with a feed-forward neural network joint model (Section 4.6) the results on newstest2013 improved by 0.4% BLEU after reranking. We stuck in the fol- lowing experiments to the multiple alignments ap- proach. </chunk></section><section><heading>4 Translation System </heading><chunk>In this evaluation, we used the open source ma- chine translation toolkit Jane 2 (Vilar et al., 2012; Wuebker et al., 2012). This open-source toolkit was developed at the RWTH Aachen University and includes a phrase-based decoder used in all of our experiments. </chunk></section><section><heading>4.1 Phrase-based System </heading><chunk>Our phrase based decoder includes an implemen- tation of the source cardinality synchronous search procedure described in Zens and Ney (2008). We used the standard set of models with phrase trans- lation probabilities, lexical smoothing in both di- rections, word and phrase penalty, distance-based distortion model, a 4-gram target language model and enhanced low frequency feature (Chen et al., 2011). Additional models used in this evaluation were the hierarchical reordering model (HRM) (Galley and Manning, 2008) and a word class lan- guage model (wcLM) (Wuebker et al., 2013). The parameter weights were optimized with minimum error rate training (MERT) (Och, 2003). The op- 2 http://www.hltpr.rwth-aachen.de/jane/ 159 timization criterion was BLEU (Papineni et al., 2002). </chunk></section><section><heading>4.2 Language Models </heading><chunk>We used a 4-gram language model trained on the target side of the bilingual data, 1 2 of the Shuffled News Crawl corpus, 1 2 of the 10 9 French-English corpus and 1 4 of the LDC Gigaword Fifth Edition corpus. The monolingual data selection was based on cross-entropy difference as described in Moore and Lewis (2010). For this language model, we trained separate language models using SRILM for each corpus, which were then interpolated. The interpolation weights are tuned by minimiz- ing the perplexity of the interpolated model on the development data. In addition, a word class lan- guage model was utilized. We trained 200 classes on the target side of the bilingual training data (Brown et al., 1992; Och, 1999). We used the same data as the 4-gram language model for train- ing a 7-gram wcLM. Furthermore, we also trained a single unpruned language model on the con- catenation of all monolingual data using KenLM, which was used as an extra model in our final ex- periments. All language models used interpolated Kneser-Ney smoothing. </chunk></section><section><heading>4.3 Evaluation </heading><chunk>All setups were evaluated with MultEval (Clark et al., 2011). To evaluate our models, we used the average of three MERT optimization runs for case sensitive BLEU (Papineni et al., 2002) and case in- sensitive TER 3 (Snover et al., 2006). </chunk></section><section><heading>4.4 Maximum Expected BLEU Training </heading><chunk>In our baseline translation system the phrase ta- bles were extracted from word alignments and the probabilities were estimated as relative fre- quencies, which is still the state-of-the-art for many standard SMT systems. For the WMT 2015 GermanEnglish task, we applied discriminative maximum expected BLEU training as described by Wuebker et al. (2015). The expected BLEU objec- tive function is optimized with the resilient back- propagation algorithm (RPROP) (Riedmiller and Braun, 1993). Similar to He and Deng (2012), the objective function is computed on n-best lists (here: n = 100) generated by the translation decoder. To avoid over-fitting due to spurious </chunk></section><section><chunk>3 TER is always evaluated in case insensitive form by Mul- tEval.  Output Layer 2nd Hidden Layer Projection Layer Input Layer p(w|h) 0 1 0 v Figure 1: LM neural network segmentations, we apply a leave-one-out heuris- tic (Wuebker et al., 2010) during the n-best list generation step. Using these n-best lists, we it- eratively trained the phrasal and lexical feature sets, denoted as (a) and (b) in Wuebker et al. (2015). Each of the two feature types are con- densed into a single model within the log-linear model combination. After every five iterations we ran MERT, and finally selected the iteration per- forming best on newstest2013. In this work, we used a subset of the training data to gener- ate the n-best lists, namely the concatenation of newstest2008 through newstest2010 and the News-Commentary corpus. </chunk></section><section><heading>4.5 Hierarchical Reordering Model </heading><chunk>In Galley and Manning (2008), a hierarchical re- ordering model for phrase-based machine trans- lation was introduced. The model scores mono- tone, swap, and discontinuous phrase orientations in the manner of the one presented by Tillmann (2004). The orientation classes are determined based on phrase blocks, which can subsume mul- tiple phrase pairs and are computed with an SR- parser. The model has proven effective in previ- ous evaluations. As the word order is more flexi- ble in German compared to English, we expected that an additional reordering model could improve the translation quality. </chunk></section><section><heading>4.6 Feed-Forward Neural Network Training </heading><chunk>We used three feed-forward neural network (FFNN) models with a similar structure as the net- work models used by Devlin et al. (2014) and Le et al. (2012). All networks were trained with dif- ferent input features: Translation Model (TM), the 5 source words around the alignment source word 160 Table 2: Results for the GermanEnglish translation task. The results are the average of three optimiza- tion runs. newstest2011 and newstest2012 were used as development data. The submission system used all models and the best optimization run on the development data. BLEU and TER are given in percentage. newstest2011 newstest2012 newstest2013 newstest2014 BLEU TER BLEU TER BLEU TER BLEU TER Baseline 23.0 58.9 23.8 58.2 26.6 54.6 25.9 54.2 + max. exp. BLEU 23.1 58.6 24.0 57.8 26.8 54.4 26.2 53.9 + updated LM 23.2 58.7 24.0 57.9 26.8 54.3 26.3 53.7 + unpruned LM 23.2 59.0 24.1 58.1 26.9 54.6 26.6 54.0 + 3 FFNN 23.7 58.4 24.5 57.7 27.4 54.0 27.1 53.3 + LSTM 23.8 58.4 24.7 57.4 27.5 53.8 27.1 53.2 Submission System 24.1 57.6 25.0 56.5 28.1 52.9 27.6 52.3 Language Model (LM), the 7 last words on the target side Joint Model (JM), the 5 source words around the alignment source word and the 4 last words on the target side The TM and LM were trained with two hidden layers (1000 and 500 nodes) while the JM con- tained three hidden layers with 2000 nodes each. The output layer was in all cases a softmax layer with a short list of 10000. All remaining words were clustered into 1000 classes and their class probabilities were predicted. The neural networks were applied to rerank 1000-best lists. </chunk></section><section><heading>4.7 Recurrent Neural Network Language Model </heading><chunk>In addition to the feed-forward neural network model we employed a recurrent neural network model. The recurrency was handled with the long short-term memory (LSTM) architecture (Hochre- iter and Schmidhuber, 1997) and we used a class- factored output layer for increased efficiency as described in Sundermeyer et al. (2012). The topol- ogy of the network is illustrated in Figure 1. All neural network models were trained on the bilin- gual data with 2000 word classes. The language models were set up with 500 nodes in both the projection layer and the hidden LSTM layer. The recurrent network models were applied together with the feed-forward models to rerank 1000-best lists. </chunk></section><section><heading>5 Setup </heading><chunk>We trained the phrase-based system on all avail- able bilingual training data. The preprocessed bilingual corpus contained around 4 million sen- tences. The preprocessed data contained a source vocabulary size of 814K and a target vocabulary size of 733K. We used the target side of the bilingual data along with the monolingual corpora for training the language models. First, we started using our old language models from our WMT 2014 setup as baseline. Then we updated our system to the new language models trained according to Section 4.2. All results are reported as average of three opti- mization runs. </chunk></section><section><heading>5.1 Experimental Results </heading><chunk>The results of the phrase-based system are summa- rized in Table 2. It was tuned on the concatenation of newstest2011 and newstest2012. The phrase-based baseline system, which in- cluded the hierarchical reordering model (Gal- ley and Manning, 2008) and a word class lan- guage model (wcLM) (Wuebker et al., 2013), reached a performance of 25.9% BLEU on newstest2014. Maximum expected BLEU training selected on newstest2013 improved the results on newstest2014 by 0.3% BLEU absolute. There was improvement of 0.1% in BLEU on newstest2014 by replacing the old language models from WMT 2014 with an updated gen- eral 4-gram LM and word class LM. Further- 161 more, adding an extra unpruned language model trained on the concatenation of the monolingual data improved the results on newstest2014 by 0.3% BLEU. Adding three feed-forward neural network models yielded an improvement of 0.5% BLEU on newstest2013 and newstest2014. Adding the LSTM language model improved the TER by an additional 0.1% on newstest2014 and by 0.2% on newstest2013. The submission system used all models and we chose the best optimization run on the develop- ment data. This optimization run by itself was 0.5% BLEU stronger on newstest2014 com- pared to the average across three optimization runs which included this run. </chunk></section><section><heading>6 Conclusion </heading><chunk>For the participation in the WMT 2015 shared translation task, RWTH experimented with a phrase-based translation system. For this ap- proach, we applied a hierarchical phrase reorder- ing model and a word class language model. fast align and two versions of GIZA++ were used for training word alignments, and a voting setup was implemented, which improved the re- sults in combination with neural network models. We also employed discriminative maximum ex- pected BLEU training. Additionally, we utilized feed-forward and recurrent neural networks mod- els for our phrase-based system, which improved the performance. Furthermore, we adapted our preprocessing pipeline based on our WMT 2014 setup. Filtering the corpus for non-ASCII letters gave us lower vocabulary sizes for both source and target side without loss in performance. Acknowledgments This paper has received funding from the Euro- pean Unions Horizon 2020 research and innova- tion programme under grant agreement n o 645452 (QT21). We further thank the anonymous review- ers for their valuable comments. References Peter F. Brown, Vincent J. Della Pietra, P. V. deSouza deSouza, J. C. Lai, and Robert L. Mercer. 1992. Class-Based n-gram Models of Natural Language. Computational Linguistics, 18(4):467479. Boxing Chen, Roland Kuhn, George Foster, and Howard Johnson. 2011. Unpacking and transform- ing feature functions: New ways to smooth phrase tables. In MT Summit XIII, pages 269275, Xiamen, China, September. Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A. Smith. 2011. Better hypothesis testing for statistical machine translation: Controlling for opti- mizer instability. In Proceedings of the 49th Annual Meeting of the Association for Computational Lin- guistics: Human Language Technologies: Short Pa- pers - Volume 2, HLT 11, pages 176181, Strouds- burg, PA, USA. Association for Computational Lin- guistics. Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, and John Makhoul. 2014. Fast and robust neural network joint models for sta- tistical machine translation. In Proceedings of the 52nd Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), pages 13701380, Baltimore, Maryland, June. Association for Computational Linguistics. Chris Dyer, Victor Chahuneau, and Noah A. Smith. 2013. A simple, fast, and effective reparametriza- tion of ibm model 2. In Proceedings of the NAACL 7th Workshop on Syntax, Semantics and Structure in Statistical Translation, Atlanta, Georgia, USA, June. Markus Freitag, Minwei Feng, Matthias Huck, Stephan Peitz, and Hermann Ney. 2013. Reverse word or- der models. In Machine Translation Summit, pages 159166, Nice, France, September. Michel Galley and Christopher D. Manning. 2008. A Simple and Effective Hierarchical Phrase Reorder- ing Model. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Pro- cessing, pages 847855, Honolulu, Hawaii, USA, October. Xiaodong He and Li Deng. 2012. Maximum Expected BLEU Training of Phrase and Lexicon Translation Models. In Proceedings of the 50th Annual Meet- ing of the Association for Computational Linguistics (ACL), pages 292301, Jeju, Republic of Korea, Jul. Sepp Hochreiter and J  urgen Schmidhuber. 1997. Long short-term memory. Neural Computation, 9(8):17351780. Philipp Koehn and Kevin Knight. 2003. Empirical Methods for Compound Splitting. In Proceedings of European Chapter of the ACL (EACL 2009), pages 187194. Hai-Son Le, Alexandre Allauzen, and Franc  ois Yvon. 2012. Continuous space translation models with neural networks. pages 3948, Montr  eal, Canada, June. Association for Computational Linguistics. Robert C. Moore and William Lewis. 2010. Intelligent Selection of Language Model Training Data. In ACL (Short Papers), pages 220224, Uppsala, Sweden, July. 162 Franz Josef Och and Hermann Ney. 2003. A System- atic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1):1951, March. Franz Josef Och. 1999. An Efficient Method for Determining Bilingual Word Classes. In Proc. 9th Conf. of the Europ. Chapter of the Assoc. for Com- putational Linguistics, pages 7176, Bergen, Nor- way, June. Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proc. of the 41th Annual Meeting of the Association for Compu- tational Linguistics (ACL), pages 160167, Sapporo, Japan, July. Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a Method for Automatic Evaluation of Machine Translation. In Proceed- ings of the 41st Annual Meeting of the Associa- tion for Computational Linguistics, pages 311318, Philadelphia, Pennsylvania, USA, July. Stephan Peitz, Joern Wuebker, Markus Freitag, and Hermann Ney. 2014. The RWTH Aachen German- English Machine Translation System for WMT 2014. In Proceedings of the ACL 2014 Ninth Work- shop on Statistical Machine Translation, Baltimore, MD, USA, June. Maja Popovi  c and Hermann Ney. 2006. POS-based Word Reorderings for Statistical Machine Transla- tion. In International Conference on Language Re- sources and Evaluation, pages 12781283, Genoa, Italy, May. Martin Riedmiller and Heinrich Braun. 1993. A direct adaptive method for faster backpropagation learn- ing: The rprop algorithm. In IEEE International Conference on Neural Networks, pages 586591. Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin- nea Micciulla, and John Makhoul. 2006. A Study of Translation Edit Rate with Targeted Human Annota- tion. In Proceedings of the 7th Conference of the As- sociation for Machine Translation in the Americas, pages 223231, Cambridge, Massachusetts, USA, August. Martin Sundermeyer, Ralf Schl  uter, and Hermann Ney. 2012. Lstm neural networks for language modeling. In Interspeech, Portland, OR, USA, September. Christoph Tillmann. 2004. A Unigram Orientation Model for Statistical Machine Translation. In Pro- ceedings of HLT-NAACL 2004: Short Papers, HLT- NAACL-Short 04, pages 101104, Boston, MA, USA. David Vilar, Daniel Stein, Matthias Huck, and Her- mann Ney. 2012. Jane: an advanced freely avail- able hierarchical machine translation toolkit. Ma- chine Translation, 26(3):197216, September. Joern Wuebker, Arne Mauser, and Hermann Ney. 2010. Training phrase translation models with leaving-one-out. In Proceedings of the 48th Annual Meeting of the Assoc. for Computational Linguistics, pages 475484, Uppsala, Sweden, July. Joern Wuebker, Matthias Huck, Stephan Peitz, Malte Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab Mansour, and Hermann Ney. 2012. Jane 2: Open Source Phrase-based and Hierarchical Statis- tical Machine Translation. In International Confer- ence on Computational Linguistics, pages 483491, Mumbai, India, December. Joern Wuebker, Stephan Peitz, Felix Rietig, and Her- mann Ney. 2013. Improving statistical machine translation with word class models. In Conference on Empirical Methods in Natural Language Pro- cessing, pages 13771381, Seattle, USA, October. Joern Wuebker, Sebastian Muehr, Patrick Lehnen, Stephan Peitz, and Hermann Ney. 2015. A compar- ison of update strategies for large-scale maximum expected bleu training. In Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, pages 15161526, Denver, CO, USA, May. Richard Zens and Hermann Ney. 2008. Improvements in Dynamic Programming Beam Search for Phrase- based Statistical Machine Translation. In Interna- tional Workshop on Spoken Language Translation, pages 195205, Honolulu, Hawaii, USA, October. 163 </chunk></section></sec_map>