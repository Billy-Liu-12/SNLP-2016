<sec_map><section><chunk>System Combination of RBMT plus SPE and Preordering plus SMT Terumasa EHARA Ehara NLP Research Laboratory Seijo, Setagaya, Tokyo, JAPAN eharate gmail.com </chunk></section><section><heading>Abstract </heading><chunk>System architecture, experimental set- tings and evaluation results of EHR group in the en-ja, zh-ja, JPCzh-ja and JPCko-ja tasks are described. Our system concept is combination of a rule based method and a statistical method. System combination of rule-based machine translation (RBMT), RBMT plus statistical post-editing (SPE) and preordering plus statistical machine translation (SMT) is conducted. From the multiple outputs of three systems, candi- date selection part selects the best output by language model score. For JPCzh-ja task devtest data translation, SPE im- proves BLEU score by 17.81, preordering improves BLEU score by 1.89 and system combination improves BLEU score by 0.26. </chunk></section><section><heading>1 Introduction </heading><chunk>Two main processes of machine translation are lexical transfer and structural transfer. Machine translation techniques and related techniques are classified in terms of these two processes as shown in Table 1. Technique Lex. Trans. Struct. Trans. RBMT SMT Monotone SPE Preordering Monotone SMT Table 1: Classification of MT and related techniques. </chunk></section><section><heading>1 For JPCko-ja task, we dont use preordering part. </heading><chunk>RBMT and SMT conduct lexical transfer and structural transfer simultaneously. On the other hand, monotone SPE and monotone SMT, which are technically the same process, conduct lexical transfer only. Preordering conducts structural transfer only. We have made researches combining a rule based method and a statistical method that is RBMT plus SPE (Ehara, 2014). This year, we add preordering plus SMT part to our system for WAT2015. This new system is also the combina- tion of rule based method (RBMT and preorder- ing) and statistical method (SPE and SMT). </chunk></section><section><heading>2 System architecture </heading><chunk>Our basic system architecture is shown in Figure 1. Figure 1: Basic system architecture. Input sentence is fed to RBMT system, RBMT plus SPE system and Preordering plus SMT sys- tem 1 . From outputs of three systems, candidate se- lection part selects best output as the system out- put. Here, our SPE and SMT are semi-monotone, RBMT Semi-monotone SPE Preordering Semi-monotone SMT Candidate selection Input Output RBMT 29 Proceedings of the 2nd Workshop on Asian Translation (WAT2015), pages 29 34, Kyoto, Japan, 16th October 2015. 2015 Copyright is held by the author(s). because distortion limit of decoder is set to 6 in- stead of 0. We will explain details of the each part of the system in the following subsections. </chunk></section><section><heading>2.1 RBMT part </heading><chunk>We use a commercial based RBMT system for the RBMT part. We, also, use user terminology dic- tionaries for zh-ja, JPCzh-ja and JPCko-ja tasks. For zh-ja and JPCzh-ja tasks, we use a part of Chi- nese to Japanese technical term dictionary of JPO (Japan Patent Office) (Japio, 2015) 2 . Original JPOs dictionary includes 2,210,294 words (nouns and verbs). We filter out all verbs and the nouns which have identical Japanese translations with the commercial based RBMT outputs. As the result, we select 1,463,265 terms for the user dic- tionary for JPCzh-ja and zh-ja tasks. For JPCko- ja task, we make a user dictionary from the train- ing corpus of the task. We get 434,334 terms for the user dictionary for the JPCko-ja task. For en- ja task, we dont use any user dictionary. We also use sentence pattern dictionary for JPCzh-ja task. We use only 13 sentence patterns for the task. </chunk></section><section><heading>2.2 SPE part </heading><chunk>SPE part intends to improve the translation quality of the output of the RBMT part. All target lan- guages of the tasks are Japanese. So SPE part translates Japanese to Japanese. We use phrase based Moses (Koehn et al., 2003) with default op- tions as the SPE engine. Word segmenter for Jap- anese is Juman ver.7.01 (Kurohashi et al., 1994). Translation models (TM) of each task is built from RBMT output and reference translation of the training corpus of each task. Training corpus size (number of sentence pairs) will be listed in Table 3. Language model (LM) is built from the follow- ing monolingual corpora. LM for en-ja task and zh-ja task is built from target side of the training corpora both of the en-ja task and zh-ja task (3.6M sentences). LM is built by lmplz tool in Moses (KenLM) with order 6. LM for JPCko-ja task and JPCzh-ja task is built from target side of the train- ing corpora both of the JPCko-ja task and JPCzh- 2 https://alaginrc.nict.go.jp/resources/jpo-info/jpo- list.html 3 Dev, devtest and test data of JPCko-ja task and JPCzh-ja task are extracted from Japanese patent doc- uments published in 2013. On the other hand, NTCIR-10s training corpora is extracted from Japa- nese patent documents published in 1990 to 2005. They are not overlapped. ja task and also Japanese side of NTCIR-10s training corpora of PatentMT task EJ subtask (Goto et al. 2013) 3 . Total number of sentences for this LM training is 5M. This LM is also built by lmplz with order 6. Distortion limit (DL) of the decoder is set to 6. Usual setting of DL between Japanese and Eng- lish or between Japanese and Chinese is 10 or over. So we call our SPE part semi-monotone SPE. </chunk></section><section><heading>2.3 Preordering part </heading><chunk>Preordering is one of the effective technique to improve structural transfer accuracy (Isozaki, 2010). Our preordering method uses context free parsing rules with reordering rules. Figure 2 shows examples of parsing rules with reordering rules and example of parsed phrases 4 . First exam- ple is English grammar rule with reordering rule. The right hand side of the parsing rule ADVP VBN PP is reordered to PP ADVP VBN by the reordering rule 2 0 1 5 . Second example is Chi- nese grammar rule with reordering rule. Reorder- ing rules are built by a heuristic algorithm. Figure 2: Example of parsing rules and reordering rules with examples Parsing accuracy directly affects preordering accuracy. We use Stanford Chinese word seg- menter (Tseng et al., 2005; Chang et al., 2008) and Berkeley parser (Petrov et al., 2006) as the parsing engine of our preordering part. Two improve- ments for the parsing are conducted. First one is grammar improvement for Chinese grammar. For en-ja task, we use the original English grammar of the Berkeley parser, eng_sm6.gr. For JPCko-ja task, we dont conduct preordering because of the similarity of word order of Korean and Japanese. For zh-ja task and JPCzh-ja task, we improve the original Chinese grammar, chn_sm5.gr. It will be explained in section 2.3.1. Second improvement of parsing is reranking of k-best parse trees that will be explained in section 2.3.2. 4 All sample sentences and phrases in this paper are from ASPEC Corpora or JPO Patent Corpora pro- vided by the workshop organizer. 5 Reordering rule 2 0 1 means that position of right hand side tags permutates from 0 1 2 to 2 0 1. Then, ADVP VBN PP is reordered to PP ADVP VBN. VP ADVP VBN PP 2 0 1 (widely utilized in many fields) VP VV NP IP 1 2 0 ( 13 ) 30 2.3.1 Grammar improvement The idea for grammar improvement is to use word alignment of JPCzh-ja bilingual training corpus. Firstly, word alignment is conducted from JPCzh- ja training corpus (1M sentence pairs) by GIZA++ (Och and Ney, 2003). For each sentence pairs, we decide sentence head word both for Japanese and Chinese using word alignment. Since Japanese is a typical head final language, head word of Japa- nese sentence is positioned at the end of the sen- tence. So it is easy to find the sentence head word for Japanese sentences. We find Chinese sentence head word as the aligned word to the Japanese sentence head word. For example, in the ja-zh sen- tence pair shown in Figure 3, Japanese sentence head word is aligned to Chinese word . So is decided as the head word of this Chinese sentence. We consider it as the gold standard head word. Figure 3: Example of zh-ja word alignment. (a) top ranked tree (b) second ranked tree Figure 4: Parsed trees. Next step is to make a tree bank. Chinese sen- tences of training corpus are parsed by the original grammar i.e. chn_sm5.gr, and we get k-best parse trees for each sentence (k is set to 100). Then we select the best parse, in which the sentence head word is same as the gold standard head word. For example, the Chinese sentence in Figure 3 is k- best parsed shown in Figure 4. The top ranked tree has as the sentence head word and the sec- ond ranked tree has as the sentence head word, which is same as the gold standard. So we 6 Devtest data is not included in the training data for the grammar improvement. Gold standard of Chinese put the second ranked tree to our tree bank in this case. From 1M JPCzh-ja training corpus, the number of second or lower ranked tree is selected is about 151K. Re-training Berkeley Chinese grammar us- ing this 151K tree bank, we get new grammar named chn_jpo.gr. Comparing original chn_sm5.gr and chn_jpo.gr, the agreement rate of sentence head word of top ranked parse tree and gold standard for devtest data is shown in Table 2 6 . Agreement rate rises about 13%. Grammar Agreement rate (%) chn_sm5.gr 50.5 chn_jpo.gr 63.0 Table 2: Agreement rate of sentence head word for the devtest data 2.3.2 Reranking of k-best parse trees Another improvement of preordering part is re- ranking of k-best parse trees. For the training cor- pus, reordered source sentence is compared to gold standard reordered source sentence. Here, gold standard reordered source sentence is deter- mined using alignment to a target sentence. For example, Chinese sentence shown in Figure 3 54 X is gold standard reordered to 54 X using alignment to the target sentence. This comparison is meas- ured by word error rate and select the parse tree which has the minimum word error rate in the k- best parse trees as the best parse tree. The parse tree shown in Figure 4 (a) is reordered to 54 X and the parse tree shown in Figure 4 (b) is reordered to 54 X . Then, Figure 4 (b) is selected as the best parse tree in this case. For the dev, devtest and test sets, we use LM based reranking to select the best parse tree. Firstly, we make reordered source sentence cor- pus from the training corpus by the above method and build LM using this corpus. Next, we select the best parse tree which has the maximum LM score in the k-best reordered sentences in dev, devtest and test sets. Here, LM score of a sentence is a score calculated by the tool query in Moses divided by the number of words in the sentence. sentence head for devtest data is determined by the method described above. 54 X 54 X IP NP NR NN54 VP VV NP NNX NN IP VP VV NP NN PU IP NP NR NN54 VP VP VV NP NNX NN VP VV NP NN PU 31 </chunk></section><section><heading>2.4 SMT part </heading><chunk>SMT part uses phrase based Moses same as SPE part. For JPCko-ja task, SMT part translates source sentences to target sentences as the usual phrase based SMT. As the segmenter for Korean, we use MeCab-ko 7 . For JPCzh-ja task, zh-ja task and en-ja task, SMT part translates reordered source sentences to the target. Reordering is made by the method described in 2.3. Distortion limit is set to 6 both JPCko-ja task and other tasks. So, we call our SMT semi-monotone SMT. LM for SMT is same as LM for SPE. TM is trained from the training corpus provided by the workshop organ- izer. Training corpus sizes (number of sentence pairs) are listed in Table 3. Task Corpus size JPCko-ja 994,998 JPCzh-ja 995,385 zh-ja 668,468 en-ja 2,351,575 Table 3: Training corpus size </chunk></section><section><heading>2.5 Candidate selection part </heading><chunk>The last part of our translation system is a candi- date selection part. This part select the candidate which has the maximum LM score from the out- puts of RBMT part, RBMT+SPE part and Preor- dering+SMT part. Here, LM score is calculated from the LM for SMT part by the method de- scribed in section 2.3.2. </chunk></section><section><heading>2.6 Other ad-hoc processing </heading><chunk>For JPCko-ja task, we conduct an ad-hoc prepro- cessing for Korean source sentences of the train, dev, devtest and test corpora and their RBMT out- puts. It is deletion of brackets surrounding the number, because the use of brackets between Ko- rean and Japanese is different shown in Figure 5. In Korean sentence, number 2 is surrounded by the brackets. However, in Japanese sentence, number 2 is not surrounded by the brackets. So we delete brackets surrounding the number in Ko- rean side to improve alignment accuracy of brack- ets. Figure 5: Different bracket usage in Korean and Japanese. 7 https://bitbucket.org/eunjeon/mecab-ko/ Another ad-hoc processing is to convert all half width characters in RBMT and SMT outputs to full width characters, because Japanese document tend to use full width characters. </chunk></section><section><heading>2.7 Issues for context-aware machine trans- lation </heading><chunk>We have no consideration for context-awareness in our system. </chunk></section><section><heading>3 Experimental results </heading></section><section><heading>3.1 Translation results </heading><chunk>Table 4 shows the official evaluation results of the translation of the test data (Nakazawa et al., 2015). In all cases, BLEU and RIBES are calculated us- ing Japanese segmenter Juman. Task System BLEU RIBES HUMAN RBMT+SPE 70.13 0.9419 6.500 SMT 70.81 0.9430 -- Combination 70.67 0.9430 10.250 RBMT+SPE 40.35 0.8195 8.250 Preordering+SMT 40.70 0.8243 -- Combination 41.06 0.8270 22.000 RBMT+SPE 35.59 0.8158 -- Preordering+SMT 39.43 0.8377 -- Combination 37.90 0.8260 25.750 RBMT+SPE 30.46 0.7685 -- Preordering+SMT 29.78 0.7536 32.500 Combination 30.88 0.7657 -- JPCko-ja JPCzh-ja zh-ja en-ja Table 4: Evaluation results of the translation In JPCko-ja task and JPCzh-ja task, system combination using candidate selection by LM score is more accurate than RBMT+SPE system both in automatic and human evaluation. In zh-ja task, Preordering+SMT system has higher BLEU and RIBES than system combination. However, we dont have human score for preordering+SMT system for the zh-ja task. </chunk></section><section><heading>3.2 Candidate selection results </heading><chunk>Table 5 shows the candidate selection results. Most of outputs of RBMT part are not selected. Outputs of RBMT+SPE part and outputs of preor- dering+SMT part are selected about half and half. , (2) , 2 32 Task RBMT RBMT+SPE Preordering +SMT Total JPCko-ja 25 1177 798 2000 JPCzh-ja 2 875 1123 2000 zh-ja 9 1270 828 2107 en-ja 5 658 1149 1812 Table 5: The number of each system outputs selected by the candidate selection part. To confirm effectiveness of candidate selection process, we compare LM scores and human eval- uation scores for JPCzh-ja task. Table 6 shows hu- man evaluation score of SPE 8 outputs and SMT outputs when the case of LM score for SMT out- put exceeds LM score for SPE output. SMT\SPE -1 0 1 -1 6 4 2 0 25 101 26 1 8 32 18 Table 6: The number of human evaluation scores for SPE outputs and SMT outputs when the case of LM score of SMT output exceeds LM score of SPE output. From the Table 6, we can see this candidate se- lection process makes human score better in 65 cases (SMT &gt; SPE) and worse in 32 cases (SPE &gt; SMT). The number of tie cases is 125. To investigate worsen cases, we show several translation examples. Table 7 shows SMT output and SPE output, baseline (BASE) output, refer- ence (REF) and source (SRC) of two translation examples. In these cases, LM score of SMT output is greater than LM score of SPE output. But hu- man score of SMT output (-1) is less than human score of SPE output (1). In the first example, the word is less general than the word . Actually, LM score of the former word is -5.61676 and LM score of the latter word is -4.12944. Then LM score of the former sentence is less than LM score of the latter sentence. In the second example, 410 415 is less general than 415 410 in our LM. Actually, LM score of SMT output sentence is -66.1355 and the LM score of the sentence which is converted the term 415410 of SMT output to 4104 8 In this section, the term SPE is used for RBMT+SPE and the term SMT is used for preor- dering+SMT for the simplicity. 15 is -71.1855. These two examples indicate the limitation of LM score based candidate selec- tion method. SMT 5 SPE 5 BASE 5 REF 5 SRC 5 SMT 4 415410 405OLED400 1 SPE 4 410415 405OLED4001 BASE 4410415 405OLED4001 REF 4 410415 405OLED4001 SRC 4 4 1 0 4 1 5 405OLED 400 Table 7: Example of candidate selection part making worse output. </chunk></section><section><heading>3.3 Other results </heading><chunk>Table 8 shows other evaluation results for JPCzh- ja task on devtest data translation. System Additional feature BLEU RIBES RBMT 16.55 0.7192 RBMT User dictionary 23.54 0.7510 RBMT+SPE User dictionary 41.35 0.8203 SMT (DL=10) 40.86 0.8071 Preordering+SMT original grammar 40.15 0.8164 Preordering+SMT improved grammar 41.84 0.8218 Preordering+SMT improved grammar + reranking of parse trees 42.75 0.8237 System combination 43.01 0.8265 Table 8: Evaluation results for JPCzh-ja devtest data translation User dictionary, SPE and preordering greatly improve RIBES score. Improving of grammar, re- ranking of parse trees and system combination slightly improve RIBES score. For BLEU score, 33 results are almost similar as RIBES case. How- ever, preordering with original grammar makes BLEU score worse compared with simple SMT. RBMT+SPE with user dictionary improves BLEU score by 17.81 compared with simple RBMT. Preordering+SMT with the improved grammar and reranking of parse trees improves BLEU score by 1.89 compared with simple SMT with DL 10. System combination improves BLEU score by 0.26 compared with preordering+SMT. </chunk></section><section><heading>4 Conclusion </heading><chunk>System architecture, experimental settings and evaluation results of the EHR group in the WAT2015 tasks are described. Our system design concept is combining of rule-based method and statistical method and it gives the good effect to the translation accuracy. One of the future issues is to improve parsing accuracy both in RBMT part and preordering part. </chunk></section><section><heading>Reference </heading><chunk>Pi-Chuan Chang, Michel Galley and Chris Manning. 2008. Optimizing Chinese Word Segmentation for Machine Translation Performance. Proceedings of the Third Workshop on Statistical Machine Transla- tion, pages 224-232. Terumasa Ehara. 2014. A machine translation system combining rule-based machine translation and sta- tistical post-editing, Proceedings of the 1st Work- shop on Asian Translation (WAT2014), pages 50-54. Isao Goto, Ka Po Chow, Bin Lu, Eiichiro Sumita and Benjamin K. Tsou. 2013. Overview of the Patent Machine Translation Task at the NTCIR-10 Work- shop, Proceedings of the 10th NTCIR Conference, pages 260-286. Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada and Kevin Duh. 2010. Head finalization: A simple reor- dering rule for SOV languages. Proceedings of the Joint 5th Workshop on Statistical Machine Transla- tion and MetricsMATR, pages 244-251. Japio (Japan Patent Information Organization). 2015. Investigation report of dictionary improvement and quality evaluation of machine translation for Chi- nese patent documents (in Japanese). Philipp Koehn, Franz J. Och and Daniel Marcu. 2003. Statistical Phrase-Based Translation, Proceedings of HLT-NAACL 2003, pages 48-54. Sadao Kurohashi, Toshihisa Nakamura, Yuji Matsu- moto and Makoto Nagao. 1994. Improvements of Japanese morphological analyzer JUMAN. Pro- ceedings of The International Workshop on Shara- ble Natural Language Resources, pages 22-28. Toshiaki Nakazawa, Hideya Mino, Isao Goto, Graham Neubig, Sadao Kurohashi and Eiichiro Sumita. 2015. Overview of the 2nd Workshop on Asian Transla- tion, Proceedings of the 2nd Workshop on Asian Translation (WAT2015). Franz Josef Och, Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Mod- els, Computational Linguistics, volume 29, number 1, pages 19-51. Slav Petrov, Leon Barrett, Romain Thibaux and Dan Klein. 2006. Learning Accurate, Compact, and In- terpretable Tree Annotation, Proceedings of COL- ING-ACL 2006, pages 433-440. Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel Jurafsky and Christopher Manning. 2005. A Condi- tional Random Field Word Segmenter. In Fourth SIGHAN Workshop on Chinese Language Pro- cessing. 34 </chunk></section></sec_map>