<sec_map><section><chunk>Proceedings of the 14th International Conference on Parsing Technologies, pages 8791, Bilbao, Spain; July 2224, 2015. c 2015 Association for Computational Linguistics Enhancing the Inside-Outside Recursive Neural Network Reranker for Dependency Parsing Phong Le Institute for Logic, Language and Computation University of Amsterdam, the Netherlands p.le@uva.nl Abstract We propose solutions to enhance the Inside-Outside Recursive Neural Network (IORNN) reranker of Le and Zuidema (2014). Replacing the original softmax function with a hierarchical softmax us- ing a binary tree constructed by combining output of the Brown clustering algorithm and frequency-based Huffman codes, we significantly reduce the rerankers com- putational complexity. In addition, en- riching contexts used in the reranker by adding subtrees rooted at (ancestors) cousin nodes, the accuracy is increased. </chunk></section><section><heading>1 Introduction </heading><chunk>Using neural networks for syntactic parsing has become popular recently, thanks to promising re- sults that those neural-net-based parsers achieved. For constituent parsing, Socher et al. (2013) using a recursive neural network (RNN) got an F1 score close to the state-of-the-art on the Penn WSJ cor- pus. For dependency parsing, the inside-outside recursive neural net (IORNN) reranker proposed by Le and Zuidema (2014) is among the top sys- tems, including the Chen and Manning (2014)s extremely fast transition-based parser employing a traditional feed-forward neural network. There are many reasons why neural-net-based systems perform very well. First, Bansal et al. (2014) showed that using word-embeddings can lead to significant improvement for dependency parsing. Interestingly, those neural-net-based sys- tems can transparently and easily employ word- embeddings by initializing their networks with those vectors. Second, by comparing a count- based model with their neural-net-based model on perplexity, Le and Zuidema (2014) suggested that predicting with neural nets is an effective solution for the problem of data sparsity. Last but not least, as showed in the work of Socher and colleagues on RNNs, e.g. Socher et al. (2013), neural net- works are capable of semantic transfer, which is essential for disambiguation. We focus on how to enhance the IORNN reranker of Le and Zuidema (2014) by both reduc- ing its computational complexity and increasing its accuracy. Although this reranker is among the top systems in accuracy, its computation is very costly due to its softmax function used to com- pute probabilities of generating tokens: all possi- ble words in the vocabulary are taken into account. Solutions for this are to approximate the original softmax function by using a hierarchical softmax (Morin and Bengio, 2005), noise-contrastive esti- mation (Mnih and Teh, 2012), or factorization us- ing classes (Mikolov et al., 2011). A cost of using those approximations is, however, drop of the sys- tem performance. To reduce the drop, we use a hi- erarchical softmax with a binary tree constructed by combining Brown clusters and Huffman codes. We show that, thanks to the reranking frame- work and the IORNNs ability to overcome the problem of data sparsity, more complex contexts can be employed to generate tokens. We intro- duce a new type of contexts, named full-history. By employing both the hierarchical softmax and the new type of context, our new IORNN reranker has significantly lower complexity but higher ac- curacy than the original reranker. </chunk></section><section><heading>2 The IORNN Reranker </heading><chunk>We firstly introduce the IORNN reranker (Le and Zuidema, 2014). 87 </chunk></section><section><heading>2.1 The -order Generative Model </heading><chunk>The reranker employs the generative model pro- posed by Eisner (1996). Intuitively, this model is top-down: starting with ROOT, we generate its left dependents and its right dependents. We then generate dependents for each ROOTs dependent. The generative process recursively continues until there is no dependent to generate. Formally, this model is described by the following formula P (T (H)) = L l=1 P H L l |C H L l P T (H L l ) R r=1 P H R r |C H R r P T (H R r ) (1) where H is the current head, T (N ) is the subtree rooted at N , and C N is the context to generate N . H L , H R are respectively Hs left dependents and right dependents, plus EOC (End-Of-Children), a special token to inform that there are no more dependents to generate. Thus, P (T (ROOT )) is the probability of generating the entire T . Le and Zuidemas -order generative model is defined as the Eisners model in which the context C D to generate D contains all of Ds generated siblings, its ancestors and theirs siblings. Because of very large fragments that contexts are allowed to hold, traditional count-based methods are im- practical (even if we use smart smoothing tech- niques). They thus introduced the IORNN archi- tecture to estimate the model. </chunk></section><section><heading>2.2 Estimation with the IORNN </heading><chunk>Each tree node y carries three vectors: inner rep- resentation i y , representing y, outer representation o y , representing the full context of y, and partial outer representation o y , representing the partial context C y which generates the token of y. Without loss of generality and ignoring direc- tions for simplicity, we assume that the model is generating dependent y for node h conditioning on context C y (see Figure 1). Under the approxima- tion that the inner representation of a phrase equals the inner representation of its head, and thanks to the recursive definition of full/partial contexts (C y is a combination of C h and ys previously gener- ated sisters), the (partial) outer representations of y are computed as follows. o y = f (W hi i h + W ho o h + b o + r) Figure 1: The process to (a) generate y, (b) com- pute outer representation o y , given head h and sib- ling x. Black, grey, white boxes are respectively inner, partial outer, and outer representations. (Le and Zuidema, 2014) where r = 0 if y is the first dependent of h; oth- erwise, r = 1 | S(y)| v S(y) W dr(v) i v , where S(y) is the set of ys sisters generated before. And, o y = f (W hi i h + W ho o h + b o + s) where s = 0 if y is the only depen- dent of h (ignoring EOC); otherwise s = 1 |S(y)| vS(y) W dr(v) i v where S(y) is the set of ys sisters. dr(v) is the dependency relation of v with h. W hi/ho/dr(v) are n n real matrices, and b o is an n-d real vector. The probability P (w|C y ) of generating a token w at node y is given by sof tmax(w, o y ) = e u(w,  oy) w V e u(w ,  oy) (2) where u(w 1 , o y ), ..., u(w |V | , o y ) T = W  o y + b and V is the set of all possible tokens (i.e. vocab- ulary). W is a |V | n real matrix, b is an |V |-d real vector. </chunk></section><section><heading>2.3 The Reranker Le and Zuidemas (mixture) reranker is </heading><chunk>T = arg max T D(S) log P (T (ROOT )) + (1 )s(S, T ) (3) where D(S) and s(S, T ) are a k-best list and scores given by a third-party parser, and [0, 1]. </chunk></section><section><heading>3 Reduce Complexity </heading><chunk>The complexity of the IORNN reranker above for computing P (T (ROOT )) is approximately 1 O = l (3 n n + n |V |) 1 Look at Figure 1, we can see that each node requires four matrix-vector multiplications: two for computing chil- drens (partial) outer representation, one for computing sis- ters (partial) outer representations, and one for computing the softmax. 88 where l is the length of the given sentence, n and |V | are respectively the dimensions of representa- tions and the vocabulary size (sums of vectors are ignored because their computational cost is small w.r.t l n n). In Le and Zuidemas reranker, |V | 14000 n = 200. It means that the reranker spends most of its time on computing sof tmax(w, o y ) in Equation 2. This is also true for the complexity in the training phase. To reduce the rerankers complexity, we need to approximate this softmax. Mnih and Teh (2012) propose using the noise-contrastive esti- mation method which is to force the system to discriminate correct words from randomly chosen candidates (i.e., noise). This approach is very fast in training thanks to fixing normalization factors to one, but slow in testing because normalization fac- tors are explicitly computed. Vaswani et al. (2013) use the same approach, and also fix normalization factors to one when testing. This, however, doest not guarantee to give us properly normalized prob- abilities. We thus employ the hierarchical sofmax proposed by Morin and Bengio (2005) which is fast in both training and testing and outputs prop- erly normalized probabilities. Assume that there is a binary tree whose leaf nodes each correspond to a word in the vocabulary. Let (u w 1 , u w 2 , ..., u w L ) be a path from the root to the leaf node w (i.e. u w 1 = root and u w L = w). Let L(u) the left child of node u, and [x] be 1 if x true and 1 otherwise. We then replace Equation 2 by P (w|C y ) = L1 i=1 [u w i+1 = L(u w i )]v T u w i o y where (z) = 1/(1 + e z ). If the binary tree is perfectly balanced, the new complexity is approx- imately l (3 n n + n log(|V |)), which is less than 4l n n if |V | &lt; 2 n (1.6 10 60 as n = 200 in the Le and Zuidemas reranker). Constructing a binary tree for this hierarchi- cal softmax turns out to be nontrivial. Morin and Bengio (2005) relied on WordNet whereas Mikolov et al. (2013) used only frequency-based Huffmann codes. In our case, an ideal tree should reflect both semantic similarities between words (e.g. leaf nodes for dog and cat should be close to each other), and word frequencies (since we want to minimize the complexity). Therefore we propose combining output of the Brown hierarchi- cal clustering algorithm (Brown et al., 1992) and frequency-based Huffman codes. 2 Firstly, we use the Brown algorithm to find c hierarchical clusters (c = 500 in our experiments). 3 We then, for each cluster, compute the Huffman code for each word in that cluster. </chunk></section><section><heading>4 Enrich Contexts </heading><chunk>Although suggesting that predicting with neural networks is a solution to overcome the problem of sparsity, Le and Zuidemas reranker still relies on two widely-used independence assumptions: (i) the two Markov chains that generate dependents in the two directions are independent, given the head, and (ii) non-overlapping subtrees are generated in- dependently. 4 That is why its partial context (e.g. the red-dashed shape in Figure 2) used to generate a node ignores: (i) sisters in the other direction and (ii) ancestors cousins and their descendants. We, in contrast, eliminate those two assump- tions by proposing the following top-down left-to- right generative story. From the head node, we generate its dependents from left to right. The par- tial context to generate a dependent is the whole fragment that is generated so far (e.g. the blue shape in Figure 2). We then generate subtrees rooted at those nodes also from left to right. The full context given to a node to generate the sub- tree rooted at this node is thus the whole fragment that is generated so far (e.g. the combination of the blue shape and the blue-dotted shape in Fig- ure 2). In this way, the model always uses the whole up-to-date fragment to generate a depen- dent or to generate a subtree rooted at a node. To our knowledge, these contexts, which contain full derivation histories, are the most complete ones ever used for graph-based parsing. Extending the IORNN reranker in this way is straight-forward. For example, we first generate a subtree tr(x) rooted at node x in Figure 3. We then compute the inner representation for tr(x): if tr(x) contains only x then i tr(x) = i x ; otherwise i tr(x) = f (W i h i x + 1 |S(x)| uS(x) W i dr(u) i tr(u) +b i ) 2 Another reason not to use the Brown clustering algo- rithm alone is that the algorithm is inefficient with high num- bers of clusters (|V | 14000 in this case). 3 We run Liang (2005)s implementation at https:// github.com/percyliang/brown-cluster on the training data. 4 Le and Zuidema rephrased this assumption by the ap- proximation that the meaning of a phrase equals to the mean- ing of its head. 89 Figure 2: Context used in Le and Zuidemas reranker (red-dashed shape) and full-history context (blue- solid shape) to generate token authorization. Figure 3: Compute the full-history outer represen- tation for y. where S(x) is the set of xs dependents, dr(u) is the dependency relation of u with x, W i h/dr(u) are n n real matrices, and b i is an n-d real vector. 5 5 Experiments We use the same setting reported in Le and Zuidema (2014, Section 5.3). The Penn WSJ Tree- bank is converted to dependencies using the Ya- mada and Matsumoto (2003)s head rules. Sec- tions 2-21 are for training, section 22 for devel- opment, and section 23 for testing. The develop- ment and test sets are tagged by the Stanford POS- tagger trained on the whole training data, whereas 10-way jackknifing is used to generate tags for the training set. For the new IORNN reranker, we set n = 200, initialise it with the 50-dim word em- beddings from Collobert et al. (2011). We use the MSTParser (with the 2nd-order feature mode) (McDonald et al., 2005) to generate k-best lists, and optimize k and (Equation 3) on the devel- opment set. Table 1 shows the comparison of our new reranker against other systems. It is a surprise that our reranker with the proposed hierarchical soft- max alone can achieve an almost equivalent score with Le and Zuidemas reranker. We conjecture that drawbacks of the hierarchical softmax com- pared to the original can be lessened by probabil- ities of generating other elements like POS-tags, 5 Note that the overall computational complexity increases linearly with l n n. For instance, for computing P (T (ROOT )), the increase is approximately 2l n n since each node requires maximally two more matrix-vector multiplications. System UAS MSTParser (baseline) 92.06 Koo and Collins (2010) 93.04 Zhang and McDonald (2012) 93.06 Martins et al. (2013) 93.07 Bohnet and Kuhn (2012) 93.39 Reranking Hayashi et al. (2013) 93.12 Le and Zuidema (2014) 93.12 Our reranker (h-softmax only, k = 45) 93.10 Our reranker (k = 47) 93.27 Table 1: Comparison with other systems on sec- tion 23 (excluding punctuation). dependency relations. Adding enriched contexts, our reranker achieves the second best accuracy among those systems. Because in this experiment no words have paths longer than 20 n = 200, our new reranker has a significantly lower complexity than the one of Le and Zuidemas reranker. On a computer with an Intel Core-i5 3.3GHz CPU and 8GB RAM, it takes 20 minutes to train this reranker, which is implemented in C++, and 2 minutes to evaluate it on the test set. </chunk></section><section><heading>6 Conclusion </heading><chunk>Solutions to enhance the IORNN reranker of Le and Zuidema (2014) were proposed. We showed that, by replacing the original softmax function with a hierarchical softmax, the rerankers com- putational complexity significantly decreases. The cost of this, which is drop on accuracy, is avoided by enriching contexts with subtrees rooted at (an- cestors) cousin nodes. The new reranker, accord- ing to experimental results on the Penn WSJ Tree- bank, has even higher accuracy than the old one. Acknowledgments We thank Willem Zuidema and four anonymous reviewers for helpful comments. 90 References Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2014. Tailoring continuous word representations for dependency parsing. In Proceedings of the Annual Meeting of the Association for Computational Lin- guistics. Bernd Bohnet and Jonas Kuhn. 2012. The best of both worlds: a graph-based completion model for transition-based parsers. In Proceedings of the 13th Conference of the European Chapter of the Associ- ation for Computational Linguistics, pages 7787. Association for Computational Linguistics. Peter F Brown, Peter V Desouza, Robert L Mercer, Vincent J Della Pietra, and Jenifer C Lai. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4):467479. Danqi Chen and Christopher D Manning. 2014. A fast and accurate dependency parser using neural net- works. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 740750. Ronan Collobert, Jason Weston, L  eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Re- search, 12:24932537. Jason M Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In Pro- ceedings of the 16th conference on Computational linguistics-Volume 1, pages 340345. Association for Computational Linguistics. Katsuhiko Hayashi, Shuhei Kondo, and Yuji Mat- sumoto. 2013. Efficient stacked dependency pars- ing by forest reranking. Transactions of the Associ- ation for Computational Linguistics, 1(1):139150. Terry Koo and Michael Collins. 2010. Efficient third- order dependency parsers. In Proceedings of the 48th Annual Meeting of the Association for Com- putational Linguistics, pages 111. Association for Computational Linguistics. Phong Le and Willem Zuidema. 2014. The inside- outside recursive neural network model for depen- dency parsing. In Proceedings of the 2014 Con- ference on Empirical Methods in Natural Language Processing. Association for Computational Linguis- tics. Percy Liang. 2005. Semi-supervised learning for nat- ural language. In MASTERS THESIS, MIT. Andre Martins, Miguel Almeida, and Noah A. Smith. 2013. Turning on the turbo: Fast third-order non- projective turbo parsers. In Proceedings of the 51st Annual Meeting of the Association for Computa- tional Linguistics (Volume 2: Short Papers), pages 617622, Sofia, Bulgaria, August. Association for Computational Linguistics. Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 9198. Association for Computa- tional Linguistics. Tomas Mikolov, Stefan Kombrink, Lukas Burget, JH Cernocky, and Sanjeev Khudanpur. 2011. Extensions of recurrent neural network language model. In Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on, pages 55285531. IEEE. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor- rado, and Jeff Dean. 2013. Distributed representa- tions of words and phrases and their compositional- ity. In Advances in Neural Information Processing Systems, pages 31113119. Andriy Mnih and Yee Whye Teh. 2012. A fast and simple algorithm for training neural probabilistic language models. In Proceedings of the 29th In- ternational Conference on Machine Learning, pages 17511758. Frederic Morin and Yoshua Bengio. 2005. Hierarchi- cal probabilistic neural network language model. In AISTATS, volume 5, pages 246252. Citeseer. Richard Socher, John Bauer, Christopher D Manning, and Andrew Y Ng. 2013. Parsing with composi- tional vector grammars. In Proceedings of the 51st Annual Meeting of the Association for Computa- tional Linguistics, pages 455465. Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and David Chiang. 2013. Decoding with large-scale neural language models improves translation. In EMNLP, pages 13871392. Citeseer. Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis- tical dependency analysis with support vector ma- chines. In Proceedings of International Conference on Parsing Technologies (IWPT), pages 195206. Hao Zhang and Ryan McDonald. 2012. Generalized higher-order dependency parsing with cube prun- ing. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Process- ing and Computational Natural Language Learning, pages 320331. Association for Computational Lin- guistics. 91 </chunk></section></sec_map>