<sec_map><section><chunk>Proceedings of the 2015 Workshop on Vision and Language (VL15), pages 67, Lisbon, Portugal, 18 September 2015. c 2015 Association for Computational Linguistics. Towards Reliable Automatic Multimodal Content Analysis Olli-Philippe Lautenbacher Liisa Tiittula Maija Hirvonen Dept. of Modern Languages University of Helsinki firstname.surname @helsinki.fi Jorma Laaksonen Dept. of Computer Science Aalto University jorma.laaksonen @aalto.fi Mikko Kurimo Dept. of Signal Processing and Acoustics Aalto University mikko.kurimo @aalto.fi </chunk></section><section><heading>Abstract </heading><chunk>This poster presents a pilot where audio de- scription is used to enhance automatic con- tent analysis, for a project aiming at creating a tool for easy access to large AV archives. </chunk></section><section><heading>1 Introduction </heading><chunk>This poster presents a pilot study for a new inter- disciplinary project which aims at creating an automated, time-aligned and language-based ac- cess to large archives of audiovisual documents. The idea is to facilitate the work of researchers who wish to pinpoint particular segments of AV material without having to browse through entire data sets. The project analyses human descrip- tions and film-viewing patterns in order to inte- grate that knowledge into an automatic content analyser. The pilot was set out to compare the results of the automatic and human methods available for content description. </chunk></section><section><heading>2 AD vs. AMCA </heading><chunk>Currently verbal content description for retriev- ing visual data is still scarce, although different methods exist: human-made audio description (AD) verbalizes visual information for visually impaired people (Maszerowska &amp; al 2014) but is a slow and costly process. Automatic Multimodal Content Analysis (AMCA), on the other hand, consists of computer-driven detection of visual and auditory elements from multimedia (Rohrbach &amp; al 2015; Viitaniemi &amp; al 2015). AMCA is cost-effective and produces consistent output, but is still insufficient for high-level se- mantic analysis. Our project combines these approaches to create an automatically produced narrative, but which is more informative than a mere list of descriptive concepts. </chunk></section><section><heading>3 The pilot and its tools </heading><chunk>We are now tackling our first pilot, a 15-minute excerpt from a documentary (Helsinki, forever, Peter von Bagh, 2008), a genre which the whole project will be concentrating on. </chunk></section><section><heading>3.1 Automatic tools </heading><chunk>A preliminary AMCA has already been made, based on earlier filmic contents, giving lists of descriptive concepts for each picture as an out- put. Consider the following example: Screenshot from Helsinki, forever. For this shot of 301 frames, the AMCA provides the following occurrence numbers for concepts: Body_Parts (301); Man_Made_Thing (301); Outdoor (278); Legs (277); Building (254); Suits (245); Actor (184); Suburban (163); Person (141); etc. Naturally, such concepts might seem coun- terintuitive for a human reading of an image, mainly because they do not inform us about the respective relevance of the various semantic el- ements retrieved from the picture. The AMCA concepts will thus need further filtering. Another tool used for the visual description is automatic sentence-like caption generation per frame (Karpathy &amp; Fei-Fei 2015), which will be combined with the abovementioned concept re- triever. For the same shot, we now get for 97% of the frames: a man in a suit and tie standing in front of a building For the audio, an automatic transcription of the dialogue and voice-over can be made, using 6 voice recognition (see Remes &amp; al 2015). The output is a transcript that is coded on a confi- dence basis, informing the researcher on the de- gree of certainty of the recognized linguistic segments. A description of on-screen sounds, including automatic music recognition, could also enhance the validity of relevant concept re- trieval. </chunk></section><section><heading>3.2 Human input </heading><chunk>In order to improve these automatic describers, three human ADs of the excerpt were ordered from professionals. The comparison of those ADs is important for the pilot since it reveals the characteristics they share in terms of visual ele- ment selection and lexical choices (identicality of referents and words, synonymy, level of abstrac- tion etc.). For our example shot, the ADs are (translated from Finnish): AD1: A nervous looking man [...] stops at the corner of the bank changing his briefcase from hand to hand and throwing glances around him. AD2: A man [...] stops in front of a Bank sign look- ing confused and hesitating, holding a briefcase with both hands. AD3: A black suited man stops at the door of the bank and hesitates. He looks around, fingering his portfolio. Some words are identical in all ADs (man; stops; bank), some concepts are almost synonymous (briefcase / portfolio; hesitating / nervous looking), and some expressions reveal a point of view (at the corner of x / in front of x / at the door of x; changing y from hand to hand / holding y with both hands / fingering y). It appears that all descriptions are similar in terms of the thematised entities and actions, but the various lexical items used in re- ferring to them invites to re-evaluate the idea that there is only one equivalent description per im- age. All in all, the pilot studies the semantic vari- ability of the descriptions by both qualitative and quantitative comparative analyses. These human descriptions will then serve to feed the AMCA, helping to filter its concept- suggestions in terms of relevance, adequacy and degree of precision. For instance, key word lists created in a corpus analysis enable us to compare the descriptions, harmonize the content words of AD and finally merge them with the concepts suggested by the AMCA. Furthermore, we also use eye tracking (Kruger &amp; al, 2015) in the pilot, to identify con- vergence patterns in the gaze positions of aver- age viewers watching the excerpt. This natural viewing gives further insight into the relevance of the visual element selection made by the AD and the AMCA. Within the selected shot, we can notice that people tend to look at the most in- formative parts of the image (the mans face and the Bank sign) especially during the first sec- onds of their appearance on screen: SMI heat maps (21 viewers) on the same shot. </chunk></section><section><heading>4 Outcomes of the pilot </heading><chunk>This poster presentation includes a demo video of each of these tools and their respective out- puts. Later on, all the collected data from the ex- cerpt will be integrated to the AMCA to enhance its output, which can be further enriched by new human input. Such a recursive machine learning process will lead, eventually, to a reliable auto- matic description tool for documentary films. </chunk></section><section><heading>References </heading><chunk>Andrej Karpathy and Li Fei-Fei. 2015. Deep Visual- Semantic Alignments for Generating Image De- scriptions. CVPR 2015. Anna Maszerowska, Anna Matamala and Pilar Orero (eds). 2014. Audio Description: New perspectives illustrated. Benjamins, Amsterdam, NL / Philadel- phia, USA. Anna Rohrbach, Marcus Rohrbach, Niket Tandon and Bernt Schiele. 2015. A Dataset for Movie Descrip- tion. CVPR 2015. Jan-Louis Kruger, Agnieszka Szarkowska, Isabela Krejtz. 2015. Subtitles on the Moving Image: an Overview of Eye Tracking Studies. Refractory a Journal of Entertainment Media, vol. 25. Ulpu Remes, Ana Ramirez Lopez, Kalle Palomaki and Mikko Kurimo. Forthcoming. Bounded condi- tional mean imputation with observation uncertain- ties and acoustic model adaptation. IEEE Transac- tions on Audio, Speech and Language Processing. Ville Viitaniemi, Mats Sjoberg, Markus Koskela, Satoru Ishikawa and Jorma Laaksonen. 2015. Ad- vances in Visual Concept Detection: Ten years of TRECVID. In Ella Bingham et al. (ed.): Advances in Independent Component Analysis and Learning Machines, 1st edition. Elsevier, Amsterdam, NL. 7 </chunk></section></sec_map>