<sec_map><section><chunk>Proceedings of the 14th Meeting on the Mathematics of Language (MoL 14), pages 8798, Chicago, USA, July 2526, 2015. c 2015 Association for Computational Linguistics General Perspective on Distributionally Learnable Classes Ryo Yoshinaka Kyoto University, Japan ry@i.kyoto-u.ac.jp Abstract Several algorithms have been proposed to learn different subclasses of context-free grammars based on the idea generically called distributional learning. Those tech- niques have been applied to many formalisms richer than context-free grammars like mul- tiple context-free grammars, simple context- free tree grammars and others. The learning algorithms for those different formalisms are actually quite similar to each other. We in this paper give a uniform view on those algo- rithms. </chunk></section><section><heading>1 Introduction </heading><chunk>Approaches based on the idea generically called dis- tributional learning have been making great success in the algorithmic learning of various subclasses of context-free grammars (CFGs) (Clark, 2010c; Yoshi- naka, 2012). Those techniques are applied to richer formalisms as well. The formalisms studied so far include multiple CFGs (Yoshinaka, 2011a), simple context-free tree grammars (CFTGs) (Kasprzik and Yoshinaka, 2011), second-order abstract categorial grammars (Yoshinaka and Kanazawa, 2011), par- allel multiple CFGs (Clark and Yoshinaka, 2014), conjunctive grammars (Yoshinaka, 2015) and oth- ers. The goal of this paper is to present a uniform view on those algorithms. Every grammar formalism for which distribu- tional learning techniques have been proposed so far generate their languages through context-free derivation trees, whose nodes are labeled by produc- tion rules. The formalism and grammar rules deter- mine how a context-free derivation tree is mapped to a derived object = d. A context-free deriva- tion tree can be decomposed into a subtree and a tree-context so that = []. The subtree deter- mines a substructure s = of d and the tree-context determines a contextual structure c = in which the substructure is plugged to form the derived ob- ject d = c s, where we represent the plugging operation by . In the CFG case, c is a string pair l, r and s is a string u and l, r u = lur, which may correspond to a derivation I lXr lur where I is the initial symbol and X is a nontermi- nal symbol. In richer formalisms those substructures and contexts may have richer structures, like tuples of strings or -terms. A learner does not know how a given example d is derived by a hidden grammar behind the observed examples. A learner based on distributional learning simply tries all the possible decompositions of a positive example into arbitrary two parts c and s such that d = c s where some grammar may derive d thorough a derivation tree = [ ] with = c and = s . Based on observation on the relation between substructures and contexts collected from given examples, a hy- pothesis grammar is computed. We call properties on grammars with which distributional learning ap- proaches work distributional properties. This paper first formally defines grammar for- malisms based on context-free derivation trees. We then show that grammars with different distribu- tional properties are learnable by standard distri- butional learning techniques if the formalism sat- isfies some conditions, which include polynomial- time decomposability of objects into contexts and 87 substructures. In addition, we discuss cases where we cannot enumerate all of the possible contexts and substructures. 2 -grammars There is a number of ways to represent a language, a subset of an object set O , whose elements are typically strings, trees but anythings encodable are eligible. Formalisms this paper discusses generate objects in O through context-free derivation trees , which are mapped to an element d O in a uniform way. The map is inductively defined and computed. Each derivation subtree of also de- termines an object, which we call a substructure of d. Each substructure is not necessarily a mem- ber of O . For example, nonterminal symbols of multiple CFGs (Seki et al., 1991) derive n-tuples of strings, where the value n is unique to each non- terminal, while the languages generated by multiple CFGs are still simply string sets. A generalization of the CFG formalism is specified by kinds of objects that each nonterminal generates and admissible op- erations over those objects. Let O be a set of objects, which are identified with their codes of finite length. We have a set of fi- nite representations O which are interpreted as sub- sets O O of O through an effective procedure. By a sort we flexibly refer to O or O O O. We also have an indexed family of computable func- tions from tuples of objects of some sorts to objects of some sort. Let F be a set of function names or function indices f , which represent functions f . By O 1 O n O 0 we denote the set of functions whose domain is O 1 O n and codomain is O 0 . By F O 0 ,O 1 ,...,On , we denote the set of function names f F with f O O 1 O On O O 0 . We assume that the domain sorts O 1 , . . . , O n and the codomain sort O 0 are easily computed from f . We specify a class of grammars by a triple, which we call a signature, = , F, O where O is a special sort of objects. We write O for O O . A context-free -grammar (-grammar for short) is a tuple G = N, , F, P, I where N is a finite set of nonterminal symbols, I N is a set of initial symbols, N is a sort assignment on nonterminals such that (X) = O for all X I, F F is a finite set of function names, and P is a finite set of production rules, which are elements of N F N . Each production rule is denoted as X 0 f X 1 , . . . , X n where X 0 , . . . , X n N and f F O 0 ,O 1 ,...,On for (X i ) = O i . For each O , N O = 1 (O) N is the set of O-nonterminals which are assigned the sort O. By G() we denote the class of - grammars. A -grammar defines its language via derivation trees, which are recursively defined as follows. If i are X i -derivation trees for i = 1, . . . , n and is a rule of the form X 0 f X 1 , . . . , X n , then the term 0 = [ 1 , . . . , n ] is an X 0 - derivation tree. Its yield 0 is f (  1 , . . . , n ) O (X 0 ) where i is the yield of i . The case where n = 0 gives the base of this re- cursive definition. An X-derivation tree is com- plete if X I. The yield of any X-derivation tree is called an X-substructure. By S(G, X) we de- note the set of X-substructures. The language of G is L(G) = XI S(G, X), which we call a - language. In other words, L(G) is the set of the yields of complete derivation trees. The class of - languages is denoted by L(). Distributional learning is concerned with what X- derivation contexts represent. An X-derivation con- text is obtained by replacing an occurrence of an X-derivation tree in a complete derivation tree by a special symbol (X) . Accordingly the yield of an X-derivation context should be a finite repre- sentation of a function that gives [ ] when applied to for any X-derivation tree . We assume to have a set E O of representations of functions from O O to O for O to which the yields of derivation con- texts belong. X is an X-derivation context for all X I and its yield O E O represents the identity func- tion on O , For an X-derivation context 0 , a rule = X f X 1 , . . . , X n and X i -derivation trees i for i {1, . . . , n} {j}, the term obtained by replacing X in 0 by [ 1 , . . . , j1 , X j , j+1 , . . . , n ] is an X j - derivation context. Its yield E (X j ) , which 88 is denoted as = 0 f (  1 , . . . , j1 , (X j ) , j1 , . . . , n ), represents the function O (X j ) O such that for all s O (X j ) , (s) = 0 ( f (  1 , . . . , j1 , s, j+1 , . . . , n )) , where 0 is the function represented by 0 . The yield of any X-derivation context is called an X-context. By C(G, X) we denote the set of X- contexts. For c C(G, X) and s S(G, X), c s is the result of the application of the function repre- sented by c to s. </chunk></section><section><heading>3 Context-substructure relation </heading><chunk>By S and C we denote the set of substructures and contexts, respectively, which can be obtained by some grammar in G(): S = O S O and C = O C O where S O = { S(G, X) | X is an O-nonterminal of some G G() } C O = { C(G, X) | X is an O-nonterminal of some G G() }. We write S for S O . Note that the above definition is relative to . Even if O O 1 = O O 2 for different O 1 , O 2 , it can be the case that S O 1 = S O 2 and C O 1 = C O 2 . Though usually O O has a definition independent from , it is possible to specify O O in terms of the signature so that S O = O O . Clearly if s S O and c C O , then there is a grammar G G() generating cs using a nonterminal of sort O. Therefore, c s is well defined for any s S O and c C O without specifying a particular -grammar. Similarly c f (s 1 , . . . , s j1 , O j , s j+1 , s n ) is well defined for any c C O 0 , f F O 0 ,...,On and s i S O i . This operation is generalized to sets S S O and C C O in the straightforward way, like C S = { c s | c C and s S }. Hereafter, whenever we write c s and f (s 1 , . . . , s n ), we assume they are well-formed. That is, the domains of the functions repre- sented by c and f match the sorts to which s and s 1 , . . . , s n belong, respectively. Accordingly we drop the subscript O from O and write f (s 1 , . . . , s j1 , , s j+1 , . . . , s n ). When we have a substructure set S, we assume S S O for some O . We often identify s with {s} unless confu- sion arises. Also we assume S O = for all O . The same assumptions apply to contexts. We are interested in whether the composition cs belongs to a concerned language L L(). Clark (2010b) has introduced syntactic concept lattices to analyze the context-substring relation on string lan- guages and particularly to design a distributional learning algorithm for CFLs. Generalizing his dis- cussion, we define an O-concept lattice B O (L) of a language L O for respective sorts O . As- suming L and O understood from the context, let us write S = { c C O | c S L } , C = { s S O | C s L } for S S O and C C O . We write S for S and C for C . We call a pair S, C S O C O a concept iff S = C and C = S. For any S S O and C C O , S , S and C , C are concepts. We call them the concepts induced by S and C, respectively. For two concepts S 1 , C 1 and S 2 , C 2 in B O (L), we write S 1 , C 1 O L S 2 , C 2 if S 1 S 2 , which is equivalent to C 2 C 1 . With this partial order, B O (L) is a complete lattice. We can introduce a partial order to substructure sets based on the concepts that they induce. Let us write S 1 O L S 2 if S 2 S 1 . The relation represents the substitutability of S 1 for S 2 . Lemma 1. The following three are equivalent for S, T S O : S O L T , c T L implies c S L for all c C O , T S L. If S i O i L T i for i = 1, . . . , n, then for any f F O 0 ,O 1 ,...,On , we have f (S 1 , . . . , S n ) O 0 L f (T 1 , . . . , T n ) . If S 1 L S 2 and S 2 L S 1 , we write S 1 L S 2 . 89 4 Conditions to be distributionally learnable Distributional learning algorithms decompose ex- amples d S into contexts c C O and substruc- tures s S O so that c s = d. Then a primal approach uses substructures or sets of substructures as nonterminals of a conjecture grammar. We want each nonterminal [[S]] indexed by S S O to sat- isfy S(G, [[S]]) = S . On the other hand, a dual approach uses contexts or sets of contexts as non- terminals where the semantics of the nonterminal is S(G, [[C]]) = C . For an object d S , O and O = (O 0 , . . . , O n ) with O i , we define S O|d = { s S O | c s = d for some c C O } C O|d = { c C O | c s = d for some s S O } F O|d = { f F O | c f (s 1 , . . . , s m ) = d for some c C O 0 |d and s i S O i |d } , S |D = dD O S O|d , C O|D = dD O C O|d and F |D = dD O F O|d for D O . Let |D = dD |d for |d = { O | S O|d = }. We require G() to be a tractable formalism such that composition and decomposition can be done ef- ficiently. Assumption 1. There are polynomial-time algo- rithms which decide whether s S O from s S and O , compute f (s 1 , . . . , s n ) from s i S O i and f F O 0 ,O 1 ,...,On , decide whether c C O from c C and O , compute c s from c C O and s S O for any O . decide whether s L(G) from s S and G G(), Assumption 2. There is p N such that the arity of every f F is at most p. Assumption 3. There are polynomial-time algo- rithms that compute SUB(d), CON(d) and FUN(d) from d S such that S |d SUB(d) S, C |d CON(d) C and F |d FUN(d) F. Actually by Assumptions 2 and 3, one can de- rive the polynomial-time uniform membership de- cidability. Moreover, it is easy to filter out nonmem- bers of S |d , C |d and F |d from SUB(d), CON(d) and FUN(d), respectively, but it is not necessary. As- sumption 3 implies | |d | is polynomially bounded, since O |d iff F O,O 1 ,...,On = for some O 1 , . . . , O n . We write SUB O (D) = SUB(D) S O , CON O (D) = CON(D) C O and FUN O (D) = FUN(D) F O . It is often the case that elements of repre- sents pairwise disjoint sets. Actually for any sig- nature , one can find = , F , O that sat- isfies this condition such that L() = L( ). Let = { O | O } {O } and O O = O O {O} for each O {O }. For f F O 0 ,...,On with f (s 1 , . . . , s n ) = s 0 , we have f F O 0 ,...,O n with f (s 1 , . . . , s n ) = s 0 where s i = (s i , O i ) if O i {O } and s i = s i if O i = O . Clearly every -grammar has an equivalent -grammar. More- over, this makes it clear that from s S one can immediately specify the unique sort O such that s O O . Similarly we may assume that each c C has unique O such that c C O and find- ing that O is a trivial task. Hereafter we work under this assumption. By O and f we mean O O and f for notational convenience. Example 1. A right regular grammar over an alpha- bet is a reg -grammar for reg = { }, F, . F has nullary functions which are members of {} and unary functions f a for f a (w) = aw for all w for some a . Clearly the class of right regular grammars satisfies Assumptions 1, 2 and 3. Example 2. A CFG is a cfg -grammar for cfg = { }, F, where each f F is represented as an (n + 1)-dimension vector of strings u 0 , . . . , u n such that f (v 1 , . . . , v n ) = u 0 v 1 u 1 . . . v n u n for all v i . The class of CFGs itself satisfies Assump- tion 1 but not Assumptions 2 and 3, since we have no limit on n. But several normal forms fulfill As- sumptions 2 and 3. Example 3. Let = { O 1 , O 2 , . . . } where O m denotes the set of m-tuples of strings. Linear context-free rewriting systems, equivalent to non- deleting multiple CFGs, are mcfg -grammars where O = O 1 = and every f F Om 0 ,Om 1 ,...,Om n concatenates strings u i,j occurring in an input u 1,1 , . . . , u 1,m 1 , . . . , u n,1 , . . . , u n,mn in some way to form an m 0 -tuple of strings. The uni- form membership problem of this class is PSPACE- 90 complete (Kaji et al., 1992). There are infinitely many ways to decompose a string d into substruc- tures and contexts as O m |d for all m. Assump- tions 1 and 3 will be fulfilled when we restrict ad- missible functions so that F Om 0 ,...,Om n = only if n p and m i q for all i. As is the case for multiple CFGs, Assumption 2 is often needed to make the uniform membership prob- lem solvable in polynomial-time (Assumption 1). 5 Learning models Learning algorithms in this paper work under three different learning models. A positive presentation (text) of a language L O is an infinite sequence d 1 , d 2 , O such that L = { d i | i 1 }. In the framework of identifica- tion in the limit from positive data, a learner is given a positive presentation of the language L = L(G ) of the target grammar G and each time a new ex- ample d i is given, it outputs a grammar G i computed from d 1 , . . . , d i . We say that a learning algorithm A identifies G in the limit from positive data if for any positive presentation d 1 , d 2 , . . . of L(G ), there is an integer n such that G n = G m for all m n and L(G n ) = L(G ). We say that A identifies a class G of grammars in the limit from positive data iff A identifies all G G in the limit from positive data. We say that A identifies a class G of grammars in the limit from positive data and membership queries when we allow A to ask membership queries (MQs) to an oracle when it computes a hypothesis grammar. An instance of an MQ is an object d O and the oracle answers whether d L in constant time. The third model is the learning with a minimally adequate teacher (MAT). A learner is not given a positive presentation but it may ask equivalence queries (EQs) to an oracle in addition to MQs. An in- stance of an EQ is a grammar G. If L(G) = L , the oracle answers Congratulations! and the learn- ing process ends. Otherwise, the oracle returns a counerexample d (L L(G)) (L(G) L ), which is called positive if d L L(G) and neg- ative if d L(G) L . When we have an oracle, the learning task itself is trivial unless we show some favorable property on the learning efficiency. 6 Learnable subclasses This section presents how -grammars with distri- butional properties can be learned. Note that all of those properties are relative to . We assume - grammars G = N , , F , P , I in this section have no useless nonterminals or functions. That is, S(G , X) = , C(G , X) = for all X N and every f F appears in some rule in P . 6.1 Substitutable Languages Definition 1 (Clark and Eyraud (2007)). A language L L() is said to be substitutable if for any O , c 1 , c 2 C O and s 1 , s 2 S O , c 1 s 1 , c 1 s 2 , c 2 s 1 L implies c 2 s 2 L . The definition can be rephrased as follows: s 1 s 2 = implies s 1 L s 2 . Example 4. Yoshinaka (2008) has proposed a learn- ing algorithm for k, l-substitutable CFLs, which sat- isfy the following property: x 1 uy 1 vz 1 , x 1 uy 2 vz 1 , x 2 uy 1 vz 2 L = x 2 uy 2 vz 2 L for any x i , y i , z i , u k and v l . We define a signature k,l = k,l , F k,l , O as fol- lows. Let k,l = {O } { O u,v | u k and v l }{ O u | u &lt;k+l }, where O = , O u,v = { uwv | w } and O u = {u}. Here we put overlines to make elements of pairwise disjoint. Let F k,l = { + , | , { } } { O | {O } } . The binary function + , con- catenates two strings from sorts and and gives the right sort in {O }. For example, + , F with = O u,v , = O w has codomain O u,x where x is the suffix of vw of length l. The unary opera- tion O F O, simply removes the overline and promotes u to u O . consists of the nullary functions giving a single letter from . It is not hard to see that every CFG has an equivalent k,l - grammar. Note that O -nonterminals never occur on the right hand side of a rule in a k,l -grammar. Hence C O is just the singleton { O } such that O u = u for all u , whereas C = C O contains arbitrary pairs of strings l, r such that l, r u = lur for any u . The k,l - substitutability is exactly the k, l-substitutability. 91 Theorem 1. The class of substitutable -languages is identifiable in the limit from positive data. The theorem follows Lemmas 2 and 3 below. From a finite set D of positive examples, Al- gorithm 1 computes the grammar SUBSTP(D) = N, , F, P, I defined as follows: N O = { [[s]] | s SUB O (D) } for O |D , I = { [[s]] | s D }, F = FUN(D), P consists of the rules of the form [[s 0 ]] f [[s 1 ]], . . . , [[s n ]] where f FUN O 0 ,...,On (D) for [[s i ]] N O i if there is c CON O (D) such that c s 0 , c f (s 1 , . . . , s n ) D . Since we assume elements of are pairwise dis- joint, each [[s]] belongs to a unique sort. Otherwise, each nonterminal should be tagged with a sort like [[s, O]]. Algorithm 1 Learning substitutable -grammars Data: A positive presentation d 1 , d 2 , . . . Result: A sequence of grammars G 1 , G 2 , . . . let G be a grammar such that L( G) = ; for n = 1, 2, . . . do let D = {d 1 , . . . , d n }; if D L( G) then let G = SUBSTP(D); end if output G as G n ; end for An alternative way to construct a grammar is to use contexts rather than substructures for nontermi- nals. One can replace SUBSTP(D) in the algorithm by SUBSTD(D) which is defined as follows. N O = { [[c]] | c CON O (D) } for O |D , I = { [[ O ]] }, F = FUN(D), P consists of the rules of the form [[c 0 ]] f [[c 1 ]], . . . , [[c n ]] where f FUN O 0 ,...,On for [[c i ]] N O i if there are s i SUB O i (D) such that c i s i D for all i and c 0 f (s 1 , . . . , s n ) D . The existing algorithms for different classes of substitutable languages (Clark and Eyraud, 2007; Yoshinaka, 2008; Yoshinaka, 2011a) are based on slight variants of SUBSTP. This paper shows the correctness of the algorithm using SUBSTD. Lemma 2. Let D be a finite subset of a - substitutable language L and G the grammar out- put by SUBSTD(D). Then L(G) L . Proof. One can show by induction on the deriva- tion that if s S(G, [[c]]) then c s L . Sup- pose that G has a rule [[c]] f [[c 1 ]], . . . , [[c n ]], s i S(G, [[c i ]]) and s = f (s 1 , . . . , s n ). The induc- tion hypothesis says c i s i L for all i. By the rule construction, there are t i for i = 1, . . . , n such that c i t i D L and c f (t 1 , . . . , t n ) D L . We have s i L t i since they occur in the same con- text c i . By Lemma 1, c f (s 1 , . . . , s n ) L . Let G = N , , F , P , I be a -grammar generating L . Fix s X S(G , X) and c X C(G , X) where c X = O for X I . Define D by D = { c X s X | X N } { c X 0 f (s X 1 , . . . , s Xn ) | X 0 f X 1 , . . . , X n P } . Lemma 3. If D D, then S(G , X) S(SUBSTD(D), [[s X ]]) for all X. Proof. Let G = SUBSTD(D). If G has a rule X 0 f X 1 , . . . , X n then G has the correspond- ing rule [[c X 0 ]] f [[c X 1 ]], . . . , [[c Xn ]], since c X 0 f (s X 1 , . . . , s Xn ), c X i s X i D . In particular since c X for X I is the identity func- tion O , the corresponding nonterminal [[c X ]] = [[ O ]] is the initial symbol of G, too. This shows that we do not need too many data to achieve a right grammar, since |D | |P | + |N |, where | | denotes the cardinality of a set. Moreover, it is easy to see Algorithm 1 updates its conjecture in polynomial time in the total size of D by Assump- tions 1, 2 and 3. 92 6.2 Finite kernel property Definition 2 (Clark et al. (2009), Yoshinaka (2011b)). A nonempty finite set S S (X) is called a k-kernel of a nonterminal X if |S| k and S(G, X) L(G) S . A -grammar G is said to have the k-finite kernel property (k-FKP) if every nonterminal X has a k- kernel S X . Theorem 2. Under Assumptions 1, 2 and 3, Algo- rithm 2 identifies -grammars with the k-FKP in the limit from positive data and membership queries. Algorithm 2 Learning -grammars with k-FKP Data: A positive presentation d 1 , d 2 , . . . of L ; Result: A sequence of -grammars G 1 , G 2 , . . . ; let D := K := F := J := ; let G := PRIMAL k (K, F, J); for n = 1, 2, . . . do let D := D {d n }; J := CON(D); if D L( G) then let K := SUB(D) and F := FUN(D); end if output G = PRIMAL k (K, F, J) as G n ; end for The conjecture grammar PRIMAL k (K, F, J) = N, , F, P, I of Algorithm 2 is defined from finite sets of substructures K S, functions F F and contexts J C. The subsets of those sets corre- sponding to respective sorts are denoted as K O = K S O , J O = J C O and F O = F F O . N O = { [[S]] | S K O with 1 |S| k } for each O |D , I = { [[S]] N O | S L }, P consists of the rules of the form [[S 0 ]] f [[S 1 ]], . . . , [[S n ]] where f F O 0 ,O 1 ,...,On for [[S i ]] N O i if (S 0 J O 0 ) f (S 1 , . . . , S n ) L . (1) The grammar is constructed by the aid of finitely many MQs. PRIMAL k (K, F, J) can be computed in polynomial time by Assumptions 1, 2 and 3. A rule [[S 0 ]] f [[S 1 ]], . . . , [[S n ]] is compatible with the semantics of the nonterminals if S 0 f (S 1 , . . . , S n ), which is equivalent to S 0 f (S 1 , . . . , S n ) L (2) by Lemma 1. However, this condition (2) cannot be checked by finitely many MQs. The condition (1) can be seen as an approximation of (2), which is decidable by finitely many MQs. Clearly (2) im- plies (1) but not vice versa. If a rule satisfies (1) but not (2), we call the rule incorrect. If a rule is in- correct, there is a witness c C O 0 J O 0 such that c S 0 L and c f (S 1 , . . . , S n ) / L . Lemma 4. For every finite K S and F F there is J C such that G = PRIMAL(K, F, J) has no incorrect rules and |J| |F ||K| k(p+1) , in which case L( G) L . Let S X be a k-kernel of each nonterminal X of a grammar G = N , , F , P , I generating L . Lemma 5. There is a finite subset D L such that S X S |D for all X N , F F |D and |D| k|N | + |P |. Moreover, if S X K for all X N and F F , then L L( G). We prove Theorem 2 discussing the efficiency. Proof of Theorem 2. Clearly Algorithm 2 updates its conjecture in polynomial time in the data size. Polynomially (in the size of G ) many positive ex- amples will stabilize K and F by Lemma 5. After K and F stabilized, all the incorrect rules will be re- moved with at most polynomially (in |K||F |) many examples by Lemma 4. After that point Algorithm 2 never changes the conjecture, which generates the target language L . </chunk></section><section><heading>6.3 Congruential grammars </heading><chunk>Definition 3 (Clark (2010a)). A -grammar G is said to be congruential if every s S(G, X) is a 1-kernel of every X N . Congruential -grammars have the 1-FKP. Un- der the following additional assumption, this special case will be polynomial-time learnable with a mini- mally adequate teacher. Assumption 4. For any derivation tree , the size of its yield is polynomially bounded by that of . Theorem 3. Under Assumptions 1, 2, 3 and 4, Al- gorithm 3 learns any language L generated by a 93 congruential -grammar G with a minimally ad- equate teacher in time polynomial in |N |, |F |, l where l is the total size of counterexamples given to the learner. Algorithm 3 Learning congruential -grammars let K := F := J := ; let G := PRIMAL 1 (K, F, J); for n = 1, 2, . . . do if L( G) = L (equivalence query) then output G and halt; else if the given counterexample d is positive (d L L( G)) then let K := K SUB(d) and F := F FUN(d); else let J := J WITNESSP( d , O ) where d is an (implicit) parse tree of d by G end if let G = PRIMAL 1 (K, F, J); end for Algorithm 3 uses the same grammar construction PRIMAL as Algorithm 2 where the parameters K and F are calculated from positive counterexam- ples given by the oracle. On the other hand, J is computed in a different way. By Lemma 4, when the oracle answers a negative counterexample d to- wards an EQ, our conjecture G must use an incorrect rule to derive d. To find and remove such an incor- rect rule, Algorithm 3 calls a subroutine WITNESSP with input ( d , ), where d is a derivation tree of G whose yield is d. To be precise, d does not have to be a derivation tree. Rather what we re- quire is that for each s S |d , one can compute at least one tuple of s 1 , . . . , s n S |d and f F |d such that s = f (s 1 , . . . , s n ) and the height of the lowest derivation tree of each s i is strictly lower than that of s. Indeed one can do this in polyno- mial time by a dynamic programming method from SUB(d) and FUN(d). Yet for explanatory easiness, we treat such information as an (implicit) derivation tree d . The procedure WITNESSP returns a con- text that witnesses an incorrect rule that contributes to generating d by searching d recursively calling itself. The procedure WITNESSP in general takes a pair (, c) such that is an [[s]]-derivation tree of G and c s . Let = ( 1 , . . . , n ) where = [[s]] f [[s 1 ]], . . . , [[s n ]]. If c f (s 1 , . . . , s n ) / L then the rule is incorrect. So WITNESSP returns c which witnesses the incorrectness of the rule. Oth- erwise, we have c f (s 1 , . . . , s n ) L c f (  1 , . . . , n ) / L for the yields i of i . One can find i such that c f (s 1 , . . . , s i1 , s i , i+1 , . . . , n ) L c f (s 1 , . . . , s i1 , i , i+1 , . . . , n ) / L . This means an incorrect rule is in i . We call WITNESSP( i , c f (s 1 , . . . , s i1 , , i+1 , . . . , n )). Lemma 6. The procedure WITNESSP( d , ) runs in polynomial time in l and |d|. Proof. The number of recursive calls of WITNESSP is no more than the height of d , which is at most |S |d |. Let the instance of the j-th recursive call be ( j , c j ) and j the derivation context for c = j . j+1 is obtained from j by replacing at most p subtrees by a derivation tree whose yield is an el- ement of K. By Assumption 4, the size of c j and thus the size of an instance of an MQ is polynomi- ally bounded by |d|l. WITNESSP runs in polynomial time. Lemma 7. Each time Algorithm 3 receives a neg- ative counterexample, at least one incorrect rule is removed. Lemma 8. Let G = N , , F , P , I be a con- gruential grammar generating L . Each time Algo- rithm 3 receives a positive counterexample, the car- dinality of the set { X N | K L(G , X) = } (F F ) decreases strictly. Proof of Theorem 3. Time between an EQ and an- other is polynomially bounded by Lemma 6. By Lemmas 5 and 8, Algorithm 3 gets at most |N | + |F | positive counterexamples. The grammar G = PRIMAL(K, F, J) is constructed from those positive counterexamples, so it has polynomially many rules. Therefore, by Lemma 7, after getting polynomially many negative counterexamples, which suppress all the incorrect rules, Algorithm 3 gets a right grammar representing L . 94 6.4 Finite context property Definition 4 (Clark (2010b), Yoshinaka (2011b) 1 ). A nonempty finite set C C is called a k-context of a nonterminal X if |C| k and S(G, X) L(G) C . A -grammar G is said to have the k-(weak) finite context property (k-FCP) if every nonterminal X has a k-context C X . Theorem 4. Under Assumptions 1, 2 and 3, Algo- rithm 4 identifies -grammars with the k-FCP in the limit from positive data and membership queries. The theorem can be shown by an argument similar to the proof of Theorem 2 based on Lemmas 9 and 10 below. The discussion on the learning efficiency of Algorithm 2 is applied to Algorithm 4 as well. Algorithm 4 Learning -grammars with k-FCP Data: A positive presentation d 1 , d 2 , . . . of L ; Result: A sequence of -grammars G 1 , G 2 , . . . ; let D := J := F := K := ; let G := DUAL k (J, F, K); for n = 1, 2, . . . do let D := D {d n }; K := SUB(D); if D L( G) then let J := CON(D) and F := FUN(D); end if output G = DUAL k (J, F, K) as G n ; end for The conjecture grammar DUAL k (J, F, K) = N, , F, P, I of Algorithm 4 is defined from finite sets of contexts J C, functions F F and sub- structures K S. For each C J O , we write C (K) to mean C K O . This set can be seen as a finite ap- proximation of C , which is computable with MQs. N O = { [[C]] | C J O with 1 |C| k } for O |D , I = { [[{ }]] }, P consists of the rules of the form [[C 0 ]] f [[C 1 ]], . . . , [[C n ]] where f F O 0 ,...,On for [[C i ]] N O i if C 0 f (C (K) 1 , . . . , C (K) m ) L . 1 We adopt the definition by Yoshinaka, which is slightly weaker than Clarks. We say that a rule [[C 0 ]] f ([[C 1 ]], . . . , [[C n ]]) is incorrect if C 0 f (C 1 , . . . , C n ) L . In that case, there are s i C i such that C 0 f (s 1 , . . . , s n ) L . Lemma 9. For every finite J C and F F there is K S such that G = DUAL(J, F, K) has no incorrect rules and |K| p|F ||J| k(p+1) , in which case L( G) L . Let G = N , , F , P , I generate L and C X a k-context of each nonterminal X N . Lemma 10. There is a finite subset D L such that C |D C X for all X N , F |D F and |D| k|N | + |P |. Moreover, if J C X for all X N and F F , then L L( G). 6.5 Context-deterministic grammars Definition 5 (Shirakawa and Yokomori (1993), Yoshinaka (2012) 2 ). A -grammar G is said to be (weakly) context-deterministic if every c C(G, X) is a 1-context of every X N O . Differently from Theorem 3, we do not need As- sumption 4 for learning context-deterministic gram- mars with a minimally adequate teacher. Theorem 5. Under Assumptions 1, 2 and 3, Al- gorithm 3 learns any language L generated by a context-deterministic -grammar G with a min- imally adequate teacher in time polynomial in |N |, |F |, l where l is the total size of counterex- amples given to the learner. Proof. By Lemmas 11, 12 and 13 below. Algorithm 5 uses the same grammar construc- tion DUAL as Algorithm 4. By Lemma 9, when the oracle answers a negative counterexample d to- wards an EQ, our conjecture G must use an incor- rect rule to derive d. To find and remove such an incorrect rule, Algorithm 5 calls a subroutine WITNESSD with a derivation tree d of G whose yield is d. The procedure WITNESSD returns a fi- nite set of substructures that witnesses an incorrect rule that contributes to generating d. An input given to WITNESSD is in general a [[c]]-derivation tree such that c / L . Let = [ 1 , . . . , n ] where = [[c]] f [[c 1 ]], . . . , [[c n ]]. If there is i such that c i i / L , we recursively call WITNESSD( i ). 2 We adopt the definition by Yoshinaka, which is slightly weaker than Shirakawa and Yokomoris. 95 Algorithm 5 Learning context-deterministic - grammars let J := F := K := ; let G := DUAL 1 (J, F, K); for n = 1, 2, . . . do if L( G) = L (equivalence query) then output G and halt; else if the given counterexample d is positive (d L L( G)) then let J := J CON(d) and F := F FUN(d); else let K := K WITNESSD( ) where is an (implicit) parse tree of d by G end if let G = DUAL 1 (J, F, K); end for Otherwise, i c i for all i, which means the rule is incorrect. WITNESSD( ) returns the set { 1 , . . . , n }. Differently from the case of WIT- NESSP, an instance of a recursive call is always an (implicit) derivation tree of some s S |d . This ex- plains why we do not need Assumption 4 in this case. Lemma 11. Time between an EQ and another is polynomially bounded. Lemma 12. Each time Algorithm 5 receives a neg- ative counterexample, at least one incorrect rule is removed. Lemma 13. Let G = N , , F , P , I be a context-deterministic grammar for L . Each time Algorithm 5 receives a positive counterexample, the set { X N | J C(G , X) = } (F F ) gets shrunk. </chunk></section><section><heading>6.6 Combined approaches </heading><chunk>By combining primal and dual approaches, one can obtain stronger approaches (Yoshinaka, 2012). The class of -grammars whose nonterminals ad- mit either a k-kernel or l-context can be learned by combining the techniques presented in Sections 6.2 and 6.4 under Assumptions 1, 2 and 3. Also -grammars whose nonterminals satisfy either the requirement to be congruential or to be context- deterministic can be learned with a minimally ade- quate teacher under Assumptions 1, 2, 3 and 4 (Sec- tions 6.3 and 6.5). </chunk></section><section><heading>7 Restricted cases </heading><chunk>In some grammar classes, it may be the case that only (supersets of) C |d and F |d are computable in polynomial-time but S |d is not, or the other way around: S |d and F |d are efficiently computable but C |d is not. For example, in non-permuting paral- lel multiple CFGs (Seki et al., 1991), elements of S |d for a string d are tuples of strings of the form v 1 , . . . , v m for d = u 0 v 1 u 1 . . . v m u m and such substrings are polynomially many if m is fixed. However, C |d contains exponentially many contexts. Clark and Yoshinaka (2014) showed that still a dual approach works for parallel multiple CFGs if nonter- minals are known to have k-contexts belonging to a certain subset C C such that C |d = C |d C is polynomial-time computable. A symmetric re- sult of a primal approach has also been obtained by Kanazawa and Yoshinaka (2015) targeting a certain kind of tree grammars. This section does not postu- late Assumption 3. Definition 6. A -grammar G is said to have the (k, S)-FKP if every nonterminal admits a k-kernel which is a subset of S. Assumption 5. There are polynomial-time algo- rithms that compute SUB(d), CON(d) and FUN(d) such that S |d SUB(d) S, C |d CON(d) C and F |d FUN(d) F, where S |d = S S |d . It is not hard to see that Algorithm 2 works for learning -grammars with (k, S)-FKP under As- sumptions 1, 2 and 5. All discussions in Section 6.2 hold for this restricted case. The symmetric definition and assumption are as follows. Definition 7. A -grammar G is said to have the (k, C)-FCP if every nonterminal admits a k-context which is a subset of C. Assumption 6. There are polynomial-time algo- rithms that compute SUB(d), CON(d) and FUN(d) such that S |d SUB(d) S, C |d CON(d) C and F |d FUN(d) F. It is not hard to see that under Assumptions 1, 2 and 6, Algorithms 4 work for learning -grammars with (k, C)-FCP -grammars. All discussions in Section 6.4 hold for this restricted case. When learning substitutable languages, even a weaker assumption suffices. 96 Assumption 7. There are sets S S and C C such that for every nonterminal X of G G(), we have S(G, X) S = and C(G, X) C = . Moreover, there are polynomial-time algorithms that compute SUB(d), CON(d) and FUN(d) such that S |d SUB(d) S, C |d CON(d) C and F |d FUN(d) F. Under Assumptions 1, 2 and 7, Algorithm 1 works using either SUBSTP or SUBSTD. On the other hand, the results on the polynomial- time MAT learnability of congruential and context- deterministic -grammars do not hold anymore un- der any of Assumptions 5, 6 and 7. 8 Extending learnable classes This section compares learnable classes of - languages for different with the same special sort O . For 1 and 2 with i = i , F i , O , if 1 2 and F 1 F 2 , every 1 -grammar is a 2 -grammar, so L( 1 ) L( 2 ). However, since the distributional properties defined so far are rela- tive to a signature, a 1 -grammar with a distribu- tional property under 1 does not necessarily have the corresponding property under 2 . Yet if S O and C O are preserved by moving from 1 to 2 , the dis- tributional properties other than the substitutability are preserved. Let us define the direct union 0 = 0 , F 0 , O of arbitrary signatures 1 and 2 by 0 = {O } { (O, i) | O i with i {1, 2} } where O (O,i) = { (s, i) | s O } and F 0 = G 1 G 2 { 1 , 2 }, where G i is a trivial variant of F i working on the new domain and codomain of the form (O, i) and i (s, i) = s for all s O . Then every i -grammar G can be seen as a special type of 0 -grammar by adding a new initial symbol Z and rules of the form Z i X for all initial symbols X of G. We have L( 1 ) L( 2 ) L( 0 ). Every i -grammar that is congruential, context-deterministic, with the k-FKP or with the k-FCP for i = 1, 2 can be seen as a 0 -grammar with those properties. Note that C O is the singleton of the identity function in 0 , which means any element of L(G) is a 1-kernel of the new initial symbol Z. In this way, from two signatures, one can obtain a richer learnable class of languages. The above argument on signature generalization does not hold for substitutable case. Rather the op- posite holds. If 1 2 and F 1 F 2 , then a lan- guage substitutable under 2 is substitutable under 1 but not vice versa. Let us say that 2 is finer than 1 if every sort of 1 is partitioned into finite number of sorts in 2 and every function of F 2 is a subfunction of some function in F 1 which accords with the partition. That is, every sort O of 1 has a finite set 2 O 2 such that O = 2 O and F 1 O 0 ,...,On = { F 2 O 0 ,...,O n | O i 2 O i }. For instance, k,l is finer than k ,l for k k and l l in Example 4. If 2 is finer than 1 , L( 1 ) = L( 2 ) holds. Every lan- guage substitutable under 1 is substitutable under 2 but not vice versa. Moreover, every congruen- tial (resp. context-deterministic) 1 -grammar has an equivalent congruential (resp. context-deterministic) 2 -grammar but not vice versa. 9 Grammars with partial functions Yoshinaka (2015) showed that a dual approach can be applied to the learning of conjunctive grammars. Conjunctive grammars (Okhotin, 2001) are CFGs ex- tended with the conjunctive operation &amp; so that one can extract the intersection of the languages of non- terminals. For example, a conjunctive rule A 0 A 1 &amp;A 2 means that if both A 1 and A 2 generate the same string u then so does A 0 . Conjunctive gram- mars cannot be seen as -grammars, since the con- junctive operation &amp; is a partial function whose do- main is not represented as the direct product of two sorts, which is not legitimate in the general frame- work of -grammars. A partial signature is a triple = , F, O which is defined in the way similar to a (total) sig- nature but F may have partial functions. Accord- ingly contexts in C will be partial functions. We do not have C(G, X) S(G, X) L(G) any more, since c s may not be defined for some elements c C(G, X) and s S(G, X). The correspon- dence between O-concept lattices and -grammars collapses. This prevents the application of the theory of distributional learning developed in this paper to -grammars. Still we can generalize the discussion on the learning of conjunctive grammars. Definition 8. A -grammar G is said to have the strong k-FCP if for any X N O , there is a finite set 97 C X C O with |C X | k such that S(G, X) = { s | c s L for all c C X } . Definition 8 requires every c C X to be to- tal on S(G, X). One can learn -grammars with the strong k-FCP under Assumptions 1, 2 and 6, where C consists of total functions only. The gram- mar construction DUAL k should be modified so that we have a rule [[C 0 ]] f [[C 1 ]], . . . , [[C n ]] if c f (s 1 , . . . , s n ) L for any c C 0 and s i C (K) i such that f (s 1 , . . . , s n ) is defined. One might think that one can naturally define context-deterministic grammars accordingly: Every c C(G, X) should be a 1-context of X. However, this means that func- tions in such a -grammar are essentially total. Acknowledgments The view presented in this paper has been sharpened through the interactions and discussions with sev- eral researchers with whom I worked on the distri- butional learning of generalized CFGs. I would like to show my deepest gratitude to Alexander Clark, Makoto Kanazawa, Anna Kasprzik and Gregory Ko- bele. Without those people this work would have been hard to accomplish. Any insufficiency or er- rors in this paper are of course of my own. </chunk></section><section><heading>References </heading><chunk>Alexander Clark and R  emi Eyraud. 2007. Polynomial identification in the limit of substitutable context-free languages. Journal of Machine Learning Research, 8:17251745. Alexander Clark and Ryo Yoshinaka. 2014. Distribu- tional learning of parallel multiple context-free gram- mars. Machine Learning, 96(1-2):531. Alexander Clark, R  emi Eyraud, and Amaury Habrard. 2009. A note on contextual binary feature grammars. In EACL 2009 workshop on Computational Linguistic Aspects of Grammatical Inference, pp. 3340. Alexander Clark. 2010a. Distributional learning of some context-free languages with a minimally ade- quate teacher. In J. Sempere and P. Garc  a, editors, ICGI, LNCS 6339, pp. 2437. Springer. Alexander Clark. 2010b. Learning context free gram- mars with the syntactic concept lattice. In J. Sempere and P. Garc  a, editors, ICGI, LNCS 6339, pp. 3851. Springer. Alexander Clark. 2010c. Towards general algorithms for grammatical inference. In M. Hutter, F. Stephan, V. Vovk, and T. Zeugmann, editors, ALT, LNCS 6331, pp. 1130. Springer. Yuichi Kaji, Ryuichi Nakanishi, Hiroyuki Seki, and Tadao Kasami. 1992. The universal recognition prob- lems for parallel multiple context-free grammars and for their subclasses. IEICE Transaction on Informa- tion and Systems, E75-D(7):499508. Makoto Kanazawa and Ryo Yoshinaka. 2015. Distribu- tional learning and context/substructure enumerability in non-linear tree grammars. In Formal Grammar - 20th International Conference, FG 2015, Barcelona, Spain, August 8-9, 2015. Proceedings. to appear. Anna Kasprzik and Ryo Yoshinaka. 2011. Distribu- tional learning of simple context-free tree grammars. In J. Kivinen, C. Szepesv  ari, E. Ukkonen, and T. Zeugmann, editors, ALT, LNCS 6925, pp. 398412. Springer. Alexander Okhotin. 2001. Conjunctive grammars. Journal of Automata, Languages and Combinatorics, 6(4):519535. Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii, and Tadao Kasami. 1991. On multiple context-free gram- mars. Theoretical Computer Science, 88(2):191229. Hiromi Shirakawa and Takashi Yokomori. 1993. Polynomial-time MAT learning of c-deterministic context-free grammars. Transaction of Information Processing Society of Japan, 34:380390. Ryo Yoshinaka and Makoto Kanazawa. 2011. Distribu- tional learning of abstract categorial grammars. In S. Pogodalla and J.-P. Prost, editors, LACL, LNCS 6736, pp. 251266. Springer. Ryo Yoshinaka. 2008. Identification in the limit of k, l-substitutable context-free languages. In A. Clark, F. Coste, and L. Miclet, editors, ICGI, LNCS 5278, pp. 266279. Springer. Ryo Yoshinaka. 2011a. Efficient learning of multiple context-free languages with multidimensional substi- tutability from positive data. Theoretical Computer Science, 412(19):18211831. Ryo Yoshinaka. 2011b. Towards dual approaches for learning context-free grammars based on syntactic concept lattices. In G. Mauri and A. Leporati, editors, DLT, LNCS 6795, pp. 429440. Springer. Ryo Yoshinaka. 2012. Integration of the dual approaches in the distributional learning of context-free grammars. In A. H. Dediu and C. Mart  n-Vide, editors, LATA, LNCS 7183, pp. 538550. Springer. Ryo Yoshinaka. 2015. Learning conjunctive grammars and contextual binary feature grammars. In A. H. Dediu, E. Formenti, C. Mart  n-Vide, and B. Truthe, editors, LATA, LNCS 8977, pp. 623635. Springer. 98 </chunk></section></sec_map>