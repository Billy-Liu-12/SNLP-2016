<sec_map><section><chunk>Preserving Trees in Minimal Automata Jan Daciuk Gda  nsk University of Technology jandac@eti.pg.gda.pl Abstract We present a method to store additional in- formation in a minimal automaton so that it is possible to compute a corresponding tree node number for a state. The number can then be used to retrieve additional in- formation. The method works for minimal (and any other) deterministic acyclic finite state automata (DFAs). We also show how to compute the inverse mapping. </chunk></section><section><heading>1 Introduction </heading><chunk>Deterministic finite state automata and transduc- ers are widely used in natural language processing and computational linguistics. The most frequent uses include dictionaries (Daciuk et al., 2010), acoustic and language models (the latter indirectly, using perfect hashing to number words), as well as hidden Markov models used for tagging and chunking. Acyclic minimal automata recognize the same languages as automata in form of trees, but they take considerably less space. Therefore, minimization is an obligatory step in most applica- tions. In LZ-style compression of automata (Ris- tov and Korencic, 2015), it is a trie (a letter-tree) that is compressed without going through mini- mization. However, the compression finds iden- tical sequences of transitions, so it also finds iso- morphic trees, which is what minimization does. In speech recognition, trees are often used (Ort- manns et al., 1997) instead of minimal automata, because lookahead probabilities are computed for individual nodes of the trees. At a given moment in time represented by a node (a state) in a tree- like automaton, one wants to know the probability of the most probable word that is recognized by going through that state. Minimal perfect hashing (Roche, 1995) can deliver a range of numbers for all words going through that state. It is then pos- sible to use them to access probabilities of each word, and find the maximal one. This can be time- consuming, especially close to the root of the tree. What is needed for lookahead probabilities is a storage for the probability of the most probable word recognized by going through that state, i.e. one probability for each state. This is trivial to implement in a tree one simply stores the prob- ability in the state. In a minimal DFA, a state can represent many sets of words as it may be equiv- alent to several nodes in a tree. Therefore in a state of a minimal DFA, one must refer somehow to the corresponding node in the tree, or at least to a place associated with that node. The solution is to provide a dynamically computed mapping from prefixes of words to node numbers in the tree. </chunk></section><section><heading>2 Definitions </heading><chunk>A deterministic finite state automaton (DFA) is a 5-tuple M = (Q, , , q 0 , F ), where Q is a finite set of states, is a finite set of symbols called an alphabet, : Q Q is a transition function, q 0 Q is the initial state, and F Q is the set of final states. The size of an automaton |M | is the number |Q| of its states. We use incomplete au- tomata, so our transition function is partial. When (q, ) Q, we write (q, ) = . The transition function can be extended in the usual way so that : Q Q: (q, ) = q (q, u) = ( (q, u), ) (1) The language L(M ) of an automaton M is de- fined as all words that are recognized along paths from the initial state to any of the final states: L(M ) = {w : (q 0 , w) F } (2) Among all automata recognizing the same lan- guage there is one (up to isomorphism) that has the smallest number of states. It is called the minimal automaton M min . The automata described throughout the paper are acyclic. This means there is no pair q Q and w + such that (q, w) = q. The lan- guage of an acyclic DFA is always a finite set of finite-length words. Among all acyclic DFAs that recognize the same language, there is one in form of a tree. All states of such automaton, except for the ini- tial state, have exactly one incoming transition, i.e. they are the target of exactly one transition. The initial state has no incoming transitions. We will use the name nodes for states of a tree, and we will use the name edges for its transitions. </chunk></section><section><heading>3 Counting Tree Nodes </heading><chunk>26 27 7 8 3 4 1 2 0 1 2 1 6 3 5 2 4 1 14 7 10 3 8 1 9 1 13 3 12 2 11 1 18 4 17 3 15 1 16 1 25 7 21 3 19 1 20 1 24 3 23 2 22 1 c a r t t l a y p a t y l a y r a t y s a t y t a y Figure 1: A DFA as a tree recognizing words: car, cart, cat, clay, pat, pay, play, rat, ray, sat, say, and stay. A dynamically computed mapping from pre- fixes of words to node numbers in the tree can be achieved by counting states (nodes) in the tree. Let c(q) be the number of states reachable from state q including state q. For a given state q, c(q) can be computed as: c(q) = 1 + :(q,) = c((q, )) (3) If the automaton has a tree shape, and q is the root of a subtree, then c(q) is the number of nodes in that subtree. In a minimal acyclic DFA, the state q can represent several subtrees. However, those trees are isomorphic, and they have exactly the same number of nodes. By induction, equation (3) correctly counts the number of nodes in the corre- sponding subtree for a minimal acyclic DFA. Ac- tually, it counts that number for any acyclic DFA. A state in a minimal DFA does not have a node number, as it can correspond to several nodes in the tree. Let uv = w L(M ). The prefix u denotes a single node in the tree; it also denotes a state q = (q 0 , u) corresponding to that node (and perhaps to some other nodes). Nodes are numbered in postorder. Counting nodes from 0, the node number (u) for a prefix u can be cal- culated as the number of nodes with smaller node numbers. Those are (c( (q 0 , u))1) nodes reach- able from (q 0 , u) (excluding (q 0 , u)), and all (u) nodes visited while recognizing all words w w = uv that are not visited while recog- nizing the prefix u: (u) = c( (q 0 , u)) 1 + (u) (4) where: (u) = 0 if u = (x) + ( (q 0 , x), a) if u = xa, a (5) and for a state q and a label : (q, ) = , let (q, ) be: (q, ) = :(q, ) = c((q, )) (6) The inverse mapping is also easy to calculate. Let q,n be a label of a transition going out from state q such that (q, ) &lt; n (q, ) + c((q, )). Let (q, n) be the prefix correspond- ing to node number n in a subtree with a root q. (q, n) = if n = c(q) 1 q,n ((q, q,n ), z) if 0 &lt; n &lt; c(q) if n &gt; c(q) (7) where: z = n (q, q,n ) (8) To find a prefix u corresponding to node number n, one computes (q 0 , n). Let us see an example. Figure 1 shows a tree recognizing words car, cart, cat, clay, pat, pay, play, rat, ray, sat, say, and stay. The number be- side node q represents c(q). Let us calculate the node number for a prefix pl. As (q 0 , pl) = 13, and c(13) = 3, (pl) = 3 1 + (pl). We have (pl) = (p) + (14, l), and (14, l) = 3. As (p) = () + (26, p), (26, p) = 8, and () = 0, we have (pl) = 3 1 + ((0 + 8) + 3) = 13. In the other direction, (26, 13) = p(14, 13 (26, p) 1) = p(14, 13 8 1) = p(14, 4) = pl(13, 4 3 1) = pl(13, 0) = pl = pl. 0 27 1 8 2 4 3 2 4 3 5 2 6 7 7 3 8 4 9 7 10 1 c a r t t l a y p a t y l r a s a t Figure 2: A minimal DFA recognizing the same words as the tree in Figure 1. Figure 2 shows a DFA corresponding to the tree. Let us repeat the calculations. As (q 0 , pl) = 4, and c(4) = 3, (pl) = 3 1 + (pl). We have (pl) = (p)+(6, l), and (6, l) = 3. As (p) = ()+(0, p), (0, p) = 8, and () = 0, we have (pl) = 3 1 + ((0 + 8) + 3) = 13. In the other direction, (0, 13) = p(6, 13 (0, p) 1) = p(6, 1381) = p(6, 4) = pl(4, 431) = pl(4, 0) = pl = pl. The only thing that changes with regard to the tree are state numbers. </chunk></section><section><heading>4 Edges Can Be Counted Too </heading><chunk>Sometimes, it may be more beneficial to use edges of the tree instead of nodes. It is possible to pro- vide equations very similar to those already given for states. However, they are not necessary. Let us look at Figure 1 again. An edge number is the number of the node that the edge leads to. There- fore, we can index the edges using exactly the same formulas as those given for nodes. </chunk></section><section><heading>5 Implementation </heading><chunk>To implement the mappings one needs to store the value of c(q) in states q, or the value of (q, ) on the transition going out from states q and labeled with . The second solution is faster, but it re- quires more memory, as there are more transitions than states. The first solution delivers smaller memory foot- print than the second one as there are more tran- sitions than states in a minimal DFA (unless the DFA is a tree). However, in many automata repre- sentations, states are implicit; there addresses are the addresses of their first outgoing transition. The additional number can be stored in front of the first transition, but that would impede storing one state inside another (a common compression tech- nique). Superimposed coding (Liang, 1983) based on a sparse matrix representation technique (Tar- jan and Yao, 1979; Dencker et al., 1984; Fredman et al., 1984) enforces fixed-length transitions be- longing to overlapping states, which excludes stor- ing something else before the first transition of a state. It is possible to provide an additional vec- tor to store the desired values, but there is a prob- lem with addressing them, as state numbers are not used for addressing a states in the DFA (transition numbers are used instead). The mapping from prefixes (identifying nodes in a tree) to numbers is implemented as func- tion Pref2N given below. In order to get the prefix number for prefix w, one calls Pref2N(M, w, q 0 ). Function Pref2N(M, w = ax, q) 1 if w = then 2 return c(q) 1 3 else 4 s 0 5 for : &lt; a do 6 s s + c((q, )) 7 if (q, a) = then 8 raise an exception 9 else 10 return s+ Pref2N(M, x, (q, a)) Line 2 computes c( (q 0 , u))1. The for loop in lines 5 and 6 computes ((q 0 , x), a). Function Pref2N is called |w| times for each symbol a in the prefix. The for loop runs at most || times in each invocation of function Pref2N. This gives us overall time complexity of O(|w|||) times, i.e. linear with regard of the prefix. With the values of (q, a) stored on transitions (q, a), we replace line 4 with s (q, a) and remove lines 5 and 6, making the function a bit faster. The inverse mapping is implemented as func- tion N2Pref. To get a prefix number n, one should call N2Pref(M, n, q 0 ). Function N2Pref(M, n, q) 1 if n = c(q) 1 then 2 return 3 else 4 s 0 5 for do 6 if (q, ) = then 7 if s + c((q, s)) n then 8 return N2Pref(n-s) 9 else 10 s s + c((q, s)) 11 raise an exception Function N2Pref is called as many times as there are symbols in the computed prefix. The for loop runs at most || times. As other oper- ations take constant time, the time complexity of the function is O(|||w max |), where w max is the longest word in the language of the DFA. Note that in the superimposed coding (sparse matrix) repre- sentation, the for loop must run for every sym- bol of the alphabet. In a list representation, the loop runs only on (symbols on) the outgoing tran- sitions, which can be significantly faster. Having pre-computed values of on transitions saves us computing the sum in line 10; the value is used directly in line 7. </chunk></section><section><heading>6 Conclusions </heading><chunk>We have shown methods to store indexes of nodes or edges of a tree in a minimal (or pseudo- minimal) automaton. Trees are sometimes used in place of minimal DFAs because information as- sociated with their nodes or edges is needed. The presented methods render such trees obsolete. The minimal DFA can provide indexes in vectors that would store tree-related data. As a minimal DFA is usually much smaller than a tree, the methods can save a lot of memory. We have also developed similar but more complicated methods for index- ing subtrees in a minimal, deterministic, bottom- up tree automaton. 7 Acknowledgments We wish to thank Marcin Kuropatwi  nski from speech recognition company Voice Lab for dis- cussions that led to development of the algorithms presented in this paper. References [Daciuk et al.2010] Jan Daciuk, Jakub Piskorski, and Strahil Ristov. 2010. Natural language dictionaries implemented as finite automata. In Carlos Martin- Vide, editor, Scientific Applications of Language Methods, pages 133204. Imperial College Press. [Dencker et al.1984] Peter Dencker, Karl Durre, and Jo- hannes Heuft. 1984. Optimization of parser tables for portable compilers. ACM Transactions on Pro- gramming Languages and Systems, 6(4):546572, October. [Fredman et al.1984] Michael L. Fredman, Janos Kom- los, and Endre Szemeredi. 1984. Storing a sparse table with 0(1) worst case access time. Journal of the ACM, 31(3):538544, July. [Liang1983] Franklin Mark Liang. 1983. Word Hy- phen-a-tion by Com-put-er. Ph.D. thesis, Stanford University. [Ortmanns et al.1997] S. Ortmanns, H. Ney, N. Coenen, and A. Eiden. 1997. Look-ahead techniques for fast beam search. In proceedings of IEEE International Conference on Acoutic, Speech and Signal Process- ing, volume 3, pages 17831786, Munich, Germany, April. [Ristov and Korencic2015] Strahil Ristov and Damir Korencic. 2015. Fast construction of space- optimized recursive automaton. Software: Practice and Experience, (45). [Roche1995] Emmanuel Roche. 1995. Finite-state tools for language processing. In ACL95. Associ- ation for Computational Linguistics. Tutorial. [Tarjan and Yao1979] Robert Endre Tarjan and Andrew Chi-Chih Yao. 1979. Storing a sparse table. Com- munications of the ACM ACM, 22(11):606611, November. </chunk></section></sec_map>