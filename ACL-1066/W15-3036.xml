<sec_map><section><chunk>Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 309315, Lisboa, Portugal, 17-18 September 2015. c 2015 Association for Computational Linguistics. UAlacant word-level machine translation quality estimation system at WMT 2015 Miquel Espl` a-Gomis Felipe S  anchez-Mart  nez Mikel L. Forcada Departament de Llenguatges i Sistemes Inform` atics Universitat dAlacant, E-03071 Alacant, Spain {mespla,fsanchez,mlf}@dlsi.ua.es Abstract This paper describes the Universitat dAlacant submissions (labelled as UAla- cant) for the machine translation quality estimation (MTQE) shared task in WMT 2015, where we participated in the word- level MTQE sub-task. The method we used to produce our submissions uses external sources of bilingual information as a black box to spot sub-segment correspondences between a source segment S and the trans- lation hypothesis T produced by a machine translation system. This is done by seg- menting both S and T into overlapping sub- segments of variable length and translating them in both translation directions, using the available sources of bilingual informa- tion on the fly. For our submissions, two sources of bilingual information were used: machine translation (Apertium and Google Translate) and the bilingual concordancer Reverso Context. After obtaining the sub- segment correspondences, a collection of features is extracted from them, which are then used by a binary classifer to obtain the final GOOD or BAD word-level qual- ity labels. We prepared two submissions for this years edition of WMT 2015: one using the features produced by our system, and one combining them with the baseline features published by the organisers of the task, which were ranked third and first for the sub-task, respectively. </chunk></section><section><heading>1 Introduction </heading><chunk>Machine translation (MT) post-editing is nowadays an indispensable step that allows to use machine translation for dissemination. Consequently, MT quality estimation (MTQE) (Blatz et al., 2004; Spe- cia et al., 2010; Specia and Soricut, 2013) has emerged as a mean to minimise the post-editing ef- fort by developing techniques that allow to estimate the quality of the translation hypotheses produced by an MT system. In order to boost the scientific efforts on this problem, the WMT 2015 MTQE shared task proposes three tasks that allow to com- pare different approaches at three different levels: segment-level (sub-task 1), word-level (sub-task 2), and document-level (sub-task 3). Our submissions tackle the word-level MTQE sub-task, which proposes a framework for evalu- ating and comparing different approaches. This year, the sub-task used a dataset obtained by trans- lating segments in English into Spanish using MT. The task consists in identifying which words in the translation hypothesis had to be post-edited and which of them had to be kept unedited by applying the labels BAD and GOOD, respectively. In this paper we describe the approach behind the two submissions of the Universitat dAlacant team to this sub-task. For our submissions we applied the approach proposed by Espl` a-Gomis et al. (2015b), who use black-box bilingual resources from the Internet for word-level MTQE. In particular, we combined two on-line MT systems, Apertium 1 and Google Translate, 2 and the bilingual concordancer Reverso Context 3 to spot sub-segment correspon- dences between a sentence S in the source lan- guage (SL) and a given translation hypothesis T in the target language (TL). To do so, both S and T are segmented into all possible overlapping sub- 1 http://www.apertium.org 2 http://translate.google.com 3 http://context.reverso.net/ translation/ 309 segments up to a certain length and translated into the TL and the SL, respectively, by means of the sources of bilingual information mentioned above. These sub-segment correspondences are used to extract a collection of features that is then used by a binary classifier to determine the final word-level MTQE labels. One of the novelties of the task this year is that the organisation provided a collection of baseline features for the dataset published. Therefore, we submitted two systems: one using only the fea- tures defined by Espl` a-Gomis et al. (2015b), and another combining them with the baseline features published by the organisers of the shared task. The results obtained by our submissions were ranked third and first, respectively. The rest of the paper is organised as follows. Section 2 describes the approach used to produce our submissions. Section 3 describes the experi- mental setting and the results obtained. The paper ends with some concluding remarks. </chunk></section><section><heading>2 Sources of bilingual information for word-level MTQE </heading><chunk>The approach proposed by Espl` a-Gomis et al. (2015b), which is the one we have followed in our submissions for the MTQE shared task in WMT 2015, uses binary classification based on a collection of features computed for each word by using available sources of bilingual informa- tion. These sources of bilingual information are obtained from on-line tools and are used on-the-fly to detect relations between the original SL seg- ment S and a given translation hypothesis T in the TL. This method has been previously used by the authors in other cross-lingual NLP tasks, such as word-keeping recommendation (Espl` a-Gomis et al., 2015a) or cross-lingual textual entailment (Espl` a- Gomis et al., 2012), and consists of the following steps: first, all the overlapping sub-segments of S up to given length L are obtained and translated into the TL using the sources of bilingual informa- tion available. The same process is carried out for all the overlapping sub-segments of T , which are translated into the SL. The resulting collections of sub-segment translations M ST and M T S are then used to spot sub-segment correspondences be- tween T and S. In this section we describe a collec- tion of features designed to identify these relations for their exploitation for word-level MTQE. </chunk></section><section><heading>2.1 Positive features </heading><chunk>Given a collection of sub-segment translations M = {, }, such as the collections M ST and M T S ) described above, one of the most obvious features consists in computing the amount of sub- segment translations (, ) M that confirm that word t j in T should be kept in the translation of S. We consider that a sub-segment translation (, ) confirms t j if is a sub-segment of S, and is a sub-segment of T that covers position j. Based on this idea, we propose the collection of positive features Pos n : Pos n (j, S, T, M ) = |{ : (, ) conf n (j, S, T, M )}| |{ : seg n (T ) j span(, T )}| where seg n (X) represents the set of all possible n-word sub-segments of segment X and func- tion span(, T ) returns the set of word positions spanned by the sub-segment in the segment T . 4 Function conf n (j, S, T, M ) returns the collection of sub-segment pairs (, ) that confirm a given word t j , and is defined as: conf n (j, S, T, M ) = {(, ) M : seg n (T ) seg (S) j span(, T )} where seg (X) is similar to seg n (X) but without length constraints. 5 We illustrate this collection of features with an example. Suppose the Catalan segment S =Associaci  o Europea per a la Traducci  o Autom` atica, an English translation hypothesis T =European Association for the Automatic Translation, and the most adequate (reference) translation T =European Association for Machine Translation. According to the reference, the words the and Automatic in the translation hypothesis should be marked as BAD: the should be removed and Automatic should be replaced by Machine. Fi- nally, suppose that the collection M ST of sub- segment pairs (, ) is obtained by applying the available sources of bilingual information to trans- late into English the sub-segments in S up to length 3: 6 4 Note that a sub-segment may be found more than once in segment T : function span(, T ) returns all the possible positions spanned. 5 Espl` a-Gomis et al. (2015b) conclude that constraining only the length of leads to better results than constraining both and . 6 The other translation direction is omitted for simplicity. 310 M ST ={(Associaci  o, Association), (Europea, European), (per, for), (a, to), (la, the), (Traducci  o, Translation), (Autom` atica, Automatic), (Associaci  o Europea, European Association), (Europea per, European for), (per a, for), (a la, to the), (la Traducci  o, the Translation), (Traducci  o Autom` atica, Machine Translation), (Associaci  o Europea per, European Association for), (Europea per a, European for the), (per a la, for the), (a la Traducci  o, to the Translation), (la Traducci  o Autom` atica, the Machine Translation)} Note that the sub-segment pairs (, ) in bold are those confirming the translation hypothesis T , while the rest contradict some parts of the hypoth- esis. For the word Machine (which corresponds to word position 5), there is only one sub-segment pair confirming it (Autom` atica, Automatic) with length 1, and no one with lengths 2 and 3. Therefore, we have that: conf 1 (5, S, T, M ) = {(Autom` atica, Automatic )} conf 2 (5, S, T, M ) = conf 3 (5, S, T, M ) = In addition, we have that the sub-segments in seg (T ) covering the word Automatic for lengths in [1, 3] are: { : seg 1 (T ) j span(, T )} = {Automatic } { : seg 2 (T ) j span(, T )} = {the Automatic , Automatic Translation } { : seg 3 (T ) j span(, T )} = {for the Automatic , the Automatic Translation } Therefore, the resulting positive features for this word would be: Pos 1 (5, S, T, M ) = conf 3 (5, S, T, M ) { : seg 1 (T ) j span(, T )} = 1 1 Pos 2 (5, S, T, M ) = conf 2 (5, S, T, M ) { : seg 2 (T ) j span(, T )} = 0 2 Pos 3 (5, S, T, M ) = conf 3 (5, S, T, M ) { : seg 3 (T ) j span(, T )} = 0 2 A second collection of features, which use the in- formation about the translation frequency between the pairs of sub-segments in M is also used. This information is not available for MT, but it is for the bilingual concordancer we have used (see Sec- tion 3). This frequency determines how often is translated as and, therefore, how reliable this translation is. We define Pos freq n to obtain these features as: Pos freq n (j, S, T, M ) = (, )confn(j,S,T,M ) occ(, , M ) (, )M occ(, , M ) where function occ(, , M ) returns the number of occurrences in M of the sub-segment pair (, ). Following the running example, we may have an alternative and richer source of bilingual informa- tion, such as a sub-segmental translation memory, which contains 99 occurrences of word Autom` atica translated as Automatic, as well as the following alternative translations: Machine (11 times), and Mechanic (10 times). Therefore, the positive fea- ture using these frequencies for sub-segments of length 1 would be: Pos freq 1 (5, S, T, M ) = 99 99 + 11 + 10 = 0.825 Both positive features, Pos() and Pos freq (), are computed for t j for all the values of sub-segment length n [1, L]. In addition, they can be com- puted for both M ST and M T S ; this yields 4L positive features in total for each word t j . </chunk></section><section><heading>2.2 Negative features </heading><chunk>The negative features, i.e. those features that help to identify words that should be post-edited in the translation hypothesis T , are also based on sub- segment translations (, ) M , but they are used in a different way. Negative features use those sub- segments that fit two criteria: (a) they are the translation of a sub-segment from S but are not sub-segments of T ; and (b) when they are aligned to T using the edit-distance algorithm (Wagner and Fischer, 1974), both their first word 1 and last 311 word | | can be aligned, therefore delimiting a sub-segment of T . Our hypothesis is that those words t j in which cannot be aligned to are likely to need postediting. We define our negative feature collection Neg mn as: Neg mn (j, S, T, M ) = NegEvidence mn (j,S,T,M ) 1 alignmentsize(, T ) where alignmentsize(, T ) returns the length of the sub-segment delimited by in T . Func- tion NegEvidence mn () returns the set of sub- segments of T that are considered negative evi- dence and is defined as: NegEvidence mn (j, S, T, M ) = { : (, ) M seg m (S) | | = n / seg (T ) IsNeg(j, , T )} In this function length constraints are set so that sub-segments take lengths m [1, L]. While for the positive features, only the length of was constrained, the experiments carried out by Espl` a- Gomis et al. (2015b) indicate that for the negative features, it is better to constrain also the length of . On the other hand, the case of the sub-segments is slightly different: n does not stand for the length of the sub-segments, but the number of words in which are aligned to T . 7 Function IsNeg() defines the set of conditions required to consider a sub- segment a negative evidence for word t j : IsNeg(j, , T ) = j , j [1, |T |] : j &lt; j &lt; j aligned(t j , 1 ) aligned(t j , | | ) k seg 1 ( ) : aligned(t j , k ) where aligned(X, Y ) is a binary function that checks whether words X and Y are aligned or not. For our running example, only two sub-segment pairs (, ) fit the conditions set by function IsNeg(j, , T ) for the word Automatic: (la Tra- ducci  o, the Translation), and (la Traducci  o Autom` atica, the Machine Translation). As can be seen, for both (, ) pairs, the words the and Translation in the sub-segments can be aligned to the words in positions 4 and 6 in T , respectively, which makes the number of words aligned n = 2. In this way, we would have the evidences: NegEvidence 2,2 (5, S, T, M ) = {the Translation } 7 That is, the length of longest common sub-segment of and T . NegEvidence 3,2 (5, S, T, M ) = {the Machine Translation } As can be seen, in the case of sub-segment = the Translation , these alignments suggest that word Automatic should be removed, while for the sub-segment = the Machine Translation they suggest that word Automatic should be replaced by word Machine. The resulting negative features are: Neg 2,2 (5, S, T, M ) = 1 3 Neg 3,2 (5, S, T, M ) = 1 3 Negative features Neg mn () are computed for t j for all the values of SL sub-segment lengths m [1, L] and the number of TL words n [2, L] which are aligned to words k in sub-segment . Note that the number of aligned words between T and cannot be smaller than 2 given the con- straints set by function IsNeg(j, , T ). This results in a collection of L (L 1) negative features. Obviously, for these features only M ST is used, since in M T S all the sub-segments can be found in T . </chunk></section><section><heading>3 Experiments </heading><chunk>This section describes the dataset provided for the word-level MTQE sub-task and the results obtained by our method on these datasest. This year, the task consisted in measuring the word-level MTQE on a collection of segments in Spanish that had been obtained through machine translation from English. The organisers provided a dataset consisting of: training set: a collection of 11,272 segments in English (S) and their corresponding ma- chine translations in Spanish (T ); for every word in T , a label was provided: BAD for the words to be post-edited, and GOOD for those to be kept unedited; development set: 1,000 pairs of segments (S, T ) with the corresponding MTQE labels that can be used to optimise the binary classi- fier trained by using the training set; test set: 1,817 pairs of segments (S, T ) for which the MTQE labels have to be estimated with the binary classifier trained on the train- ing and the development sets. 312 </chunk></section><section><heading>3.1 Binary classifier </heading><chunk>A multilayer perceptron (Duda et al., 2000, Section 6) was used for classification, as implemented in Weka 3.6 (Hall et al., 2009), following the approach by Espl` a-Gomis et al. (2015b). A subset of 10% of the training examples was extracted from the training set before starting the training process and used as a validation set. The weights were itera- tively updated on the basis of the error computed in the other 90%, but the decision to stop the training (usually referred as the convergence condition) was based on this validation set, in order to minimise the risk of overfitting. The error function used was based on the the optimisation of the metric used for ranking, i.e. the F BAD 1 metric. Hyperparameter optimisation was carried out on the development set, by using a grid search (Bergstra et al., 2011) in order to choose the hyper- parameters optimising the results for the metric to be used for comparison, F 1 for class BAD: Number of nodes in the hidden layer: Weka (Hall et al., 2009) makes it possible to choose from among a collection of predefined net- work designs; the design performing best in most cases happened to have a single hidden layer containing the same number of nodes in the hidden layer as the number of features. Learning rate: this parameter allows the di- mension of the weight updates to be regulated by applying a factor to the error function after each iteration; the value that best performed for most of our training data sets was 0.1. Momentum: when updating the weights at the end of a training iteration, momentum smooths the training process for faster conver- gence by making it dependent on the previous weight value; in the case of our experiments, it was set to 0.03. </chunk></section><section><heading>3.2 Evaluation </heading><chunk>As already mentioned, two configurations of our system were submitted: one using only the features defined in Section 2, and one combining them with the baseline features. In order to obtain our fea- tures we used two sources of bilingual information, as already mentioned: MT and a bilingual concor- dancer. As explained above, for our experiments we used two MT systems which are freely available on the Internet: Apertium and Google Translate. The bilingual concordancer Reverso Context was also used for translating sub-segments. Actually, only the sub-sentential translation memory of this system was used, which provides the collection of TL translation alternatives for a given SL sub- segment, together with the number of occurrences of the sub-segments pair in the translation memory. Four evaluation metrics were proposed for this task: The precision P c , i.e. the fraction of instances correctly labelled among all the instances la- belled as c, where c is the class assigned (ei- ther GOOD or BAD in our case); The recall R c , i.e. the fraction of instances correctly labelled as c among all the instances that should be labelled as c in the test set; The F c 1 score, which is defined as F c 1 = 2 P c R c P c + R c ; although the F c 1 score is computed both for GOOD and for BAD, it is worth noting that the F 1 score for the less frequent class in the data set (label BAD, in this case) is used as the main comparison metric; The F w 1 score, which is the version of F c 1 weighted by the proportion of instances of a given class c in the data set: F w 1 = N BAD N TOTAL F BAD 1 + N GOOD N TOTAL F GOOD 1 where N BAD is the number of instances of the class BAD, N GOOD is the number of in- stances of the class GOOD, and N TOTAL is the total number of instances in the test set. </chunk></section><section><heading>3.3 Results </heading><chunk>Table 1 shows the results obtained by our system, both on the development set during the training phase and on the test set. The table also includes the results for the baseline system as published by the organisers of the shared task, which uses the baseline features provided by them and a standard logistic regression binary classifier. As can be seen in Table 1, the results obtained on the development set and the test set are quite simi- lar and coherent, which highlights the robustness of the approach. The results obtained clearly out- perform the baseline on the main evaluation metric (F BAD 1 ). It is worth noting that, on this metric, the 313 Data set System P BAD R BAD F BAD 1 P GOOD R GOOD F GOOD 1 F w 1 development set SBI 31.2% 63.7% 41.9% 88.5% 66.7% 76.1% 69.5% SBI+baseline 33.4% 60.9% 43.1% 88.5% 71.1% 78.8% 72.0% test set baseline 16.8% 88.9% 75.3% SBI 30.8% 63.9% 41.5% 88.8% 66.5% 76.1% 69.5% SBI+baseline 32.6% 63.6% 43.1% 89.1% 69.5% 78.1% 71.5% Table 1: Results of the two systems submitted to the WMT 2015 sub-task on word-level MTQE: the one using only sources of bilingual information (SBI) and the one combining these sources of information with the baseline features (SBI+baseline). The table also includes the results of the baseline system proposed by the organisation; in this case only the F1 scores are provided because, at the time of writing this paper, the rest of metrics remain unpublished. SBI and SBI+baseline submissions scored first and third among the 16 submissions to the shared task. 8 The submission scoring second obtained very simi- lar results; for F BAD 1 it obtained 43.05%, while our submission obtained 43.12%. On the other hand, using the metric F w 1 for comparison, our submis- sions ranked 10 and 11 in the shared task, although it is worth noting that our system was optimised us- ing only the F BAD 1 metric, which is the one chosen by the organisers for ranking submissions. </chunk></section><section><heading>4 Concluding remarks </heading><chunk>In this paper we described the submissions of the UAlacant team for the sub-task 2 in the MTQE shared task of the WMT 2015 (word-level MTQE). Our submissions, which were ranked first and third, used online available sources bilingual of informa- tion in order to extract relations between the words in the original SL segments and their TL machine translations. The approach employed is aimed at being system-independent, since it only uses re- sources produced by external systems. In addition, adding new sources of information is straightfor- ward, which leaves considerable room for improve- ment. In general, the results obtained support the conclusions obtained by Espl` a-Gomis et al. (2015b) regarding the feasibility of this approach and its performance. </chunk></section><section><heading>Acknowledgements </heading><chunk>Work partially funded by the Spanish Minis- terio de Ciencia e Innovaci  on through project TIN2012-32615 and by the European Commis- sion through project PIAP-GA-2012-324414 (Abu- MaTran). We specially thank Reverso-Softissimo and Prompsit Language Engineering for providing the access to the Reverso Context concordancer, and to the University Research Program for Google 8 http://www.quest.dcs.shef.ac.uk/ wmt15_files/results/task2.pdf Translate that granted us access to the Google Translate service. References J.S. Bergstra, R. Bardenet, Y. Bengio, and B. K  egl. 2011. Algorithms for hyper-parameter optimiza- tion. In J. Shawe-Taylor, R.S. Zemel, P.L. Bartlett, F. Pereira, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 24, pages 25462554. J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur, C. Goutte, A. Kulesza, A. Sanchis, and N. Ueff- ing. 2004. Confidence estimation for machine trans- lation. In Proceedings of the 20th International Conference on Computational Linguistics, COLING 04. R. O. Duda, P. E. Hart, and D. G. Stork. 2000. Pat- tern Classification. John Wiley and Sons Inc., 2nd edition. M. Espl` a-Gomis, F. S  anchez-Mart  nez, and M.L. For- cada. 2012. UAlacant: Using online machine trans- lation for cross-lingual textual entailment. In Pro- ceedings of the 6th International Workshop on Se- mantic Evaluation, SemEval 2012), pages 472476, Montreal, Canada. M. Espl` a-Gomis, F. S  anchez-Mart  nez, and M. L. For- cada. 2015a. Target-language edit hints in CAT tools based on TM by means of MT. Journal of Ar- tificial Intelligence Research, 53:169222. M. Espl` a-Gomis, F. S  anchez-Mart  nez, and M. L. For- cada. 2015b. Using on-line available sources of bilingual information for word-level machine trans- lation quality estimation. In Proceedings of the 18th Annual Conference of the European Associtation for Machine Translation, pages 1926, Antalya, Turkey. M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute- mann, and I.H. Witten. 2009. The WEKA Data Mining Software: an Update. SIGKDD Explo- rations, 11(1):1018. L. Specia and R. Soricut. 2013. Quality estimation for machine translation: preface. Machine Translation, 27(3-4):167170. 314 L. Specia, D. Raj, and M. Turchi. 2010. Machine trans- lation evaluation versus quality estimation. Machine Translation, 24(1):3950. R.A. Wagner and M.J. Fischer. 1974. The string- to-string correction problem. Journal of the ACM, 21(1):168173. 315 </chunk></section></sec_map>