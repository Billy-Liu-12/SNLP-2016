<sec_map><section><chunk>Proceedings of the 14th Meeting on the Mathematics of Language (MoL 14), pages 112125, Chicago, USA, July 2526, 2015. c 2015 Association for Computational Linguistics Output Strictly Local Functions Jane Chandlee University of Delaware janemc@udel.edu R  emi Eyraud QARMA Team LIF Marseille remi.eyraud@ lif.univ-mrs.fr Jeffrey Heinz University of Delaware heinz@udel.edu Abstract This paper characterizes a subclass of subse- quential string-to-string functions called Out- put Strictly Local (OSL) and presents a learn- ing algorithm which provably learns any OSL function in polynomial time and data. This al- gorithm is more efficient than other existing ones capable of learning this class. The OSL class is motivated by the study of the nature of string-to-string transformations, a cornerstone of modern phonological grammars. </chunk></section><section><heading>1 Introduction </heading><chunk>Motivated by questions in phonology, this paper studies the Output Strictly Local (OSL) functions originally defined by Chandlee (2014) and Chandlee et al. (2014). The OSL class is one way Strictly Local (SL) stringsets can be generalized to string- to-string maps. Their definition is a functional ver- sion of a defining characteristic of SL stringsets called Suffix Substitution Closure (Rogers and Pul- lum, 2011). Similar to SL stringsets, the OSL func- tions contain nested subclasses parameterized by a value k, which is the length of the suffix of output strings that matters for computing the function. As Chandlee (2014) argues, almost all local phonological processes can be modeled with Input Strictly Local (ISL) functions. Yet there is one no- table class of exceptions: so-called spreading pro- cesses, in which a feature like nasality iteratively as- similates over a contiguous span of segments. As we show, the OSL functions are needed to describe this sort of phenomenon. Here we provide a slight, but important, revision to the original definition of OSL functions in Chan- dlee (2014) and Chandlee et al. (2014), which allows two important theoretical contributions while pre- serving the previous results The first is a finite-state transducer (FST) characterization of OSL functions, which leads to the second result, the OSLFIA (OSL Function Inference Algorithm) and a proof that it ef- ficiently identifies the k-OSL functions from posi- tive examples. We compare this algorithm to OS- TIA (Onward Subsequential Transducer Inference Algorithm, Oncina et al. (1993)) which identifies to- tal subsequential functions in cubic time, its modi- fications OSTIA-D and OSTIA-R, which can learn particular subclasses of subsequential functions us- ing domain and range information, respectively, in at least cubic time (Oncina and Var` o, 1996; Castel- lanos et al., 1998), and SOSFIA (Structured On- ward Subsequential Inference Algorithm, Jardine et al. (2014)), which can learn particular subclasses of subsequential functions in linear time and data. We show these algorithms either cannot learn the OSL functions or do so less efficiently than the OSLFIA. These contributions were missing from the initial re- search on OSL functions (except for a preliminary FST characterization in Chandlee (2014)). Finally, we explain how a unified theory of local phonol- ogy will have to draw insights from both the ISL and OSL classes and offer an idea of how this might work. Thus, this paper is a crucial and necessary in- termediate step towards an empirically adequate but restrictive characterization of phonological locality. The remainder of the paper is organized as fol- lows. Motivation and related work are given in sec- 112 tion 2, including an example of the spreading pro- cesses that cannot be modeled with ISL functions. Notations and background concepts are presented in section 3. In section 4 we define OSL functions, and the theoretical characterization and learning re- sults are given in sections 5 and 6. In section 7, we explain how OSL functions model spreading pro- cesses. In section 8 we elaborate on a few important areas for future work, and in section 9 we conclude. </chunk></section><section><heading>2 Background and related work </heading><chunk>A foundational principle of modern generative phonology is that systematic variation in morpheme pronunciation is best explained with a single under- lying representation of the morpheme that is trans- formed into various surface representations based on context (Kenstowicz and Kisseberth, 1979; Odden, 2014). Thus, much of generative phonology is con- cerned with the nature of these transformations. One way to better understand the nature of linguistic phenomena is to develop strong com- putational characterizations of them. Discussing SPE-style phonological rewrite rules (Chomsky and Halle, 1968), Johnson (1972, p. 43) expresses the reasoning behind this approach: It is a well-established principle that any mapping whatever that can be computed by a finitely statable, well-defined proce- dure can be effected by a rewriting sys- tem (in particular, by a Turing machine, which is a special kind of rewriting sys- tem). Hence any theory which allows phonological rules to simulate arbitrary rewriting systems is seriously defective, for it asserts next to nothing about the sorts of mappings these rules can perform. This leads to the important question of what kinds of transformations ought a theory of phonology allow? Earlier work suggests that phonological theo- ries ought to exclude nonregular relations (Johnson, 1972; Kaplan and Kay, 1994; Frank and Satta, 1998; Graf, 2010). More recently, it has been hypothe- sized that phonological theory ought to only allow certain subclasses of the regular relations (Gainor et al., 2012; Chandlee et al., 2012; Chandlee and Heinz, 2012; Payne, 2013; Luo, 2014; Heinz and Lai, 2013). This research places particular em- phasis on subsequential functions, which can infor- mally be characterized as functions definable with a weighted, deterministic finite-state acceptor where the weights are strings and multiplication is con- catenation. The aforementioned work suggests that this hypothesis enjoys strong support in segmental phonology, with interesting and important excep- tions in the domain of tone (Jardine, 2014). Recent research has also showed an increased awareness and understanding of subregular classes of stringsets (formal languages) and their impor- tance for theories of phonotactics (Heinz, 2007; Heinz, 2009; Heinz, 2010; Rogers et al., 2010; Rogers and Pullum, 2011; Rogers et al., 2013). While many of these classes and their properties were studied much earlier (McNaughton and Papert, 1971; Thomas, 1997), little to no attention has been paid to similar classes properly contained within the subsequential functions. Thus, at least within the do- main of segmental phonology, there is an important question of whether stronger computational charac- terizations of phonological transformations are pos- sible, as seems to be the case for phonotactics. As mentioned above, Chandlee (2014) shows that many phonological processes belong to a subclass of subsequential functions, the Input Strictly Lo- cal (ISL) functions. Informally, a function is k- ISL if the output of every input string a 0 a 1 a n is u 0 u 1 u n where u i is a string which only de- pends on a i and the k 1 input symbols before a i (so a ik+1 a ik+2 a i1 ). (A formal definition is given in section 4). ISL functions can model a range of processes including local substitution, epenthesis, deletion, and metathesis. For more details on the ex- act range of ISL processes, see Chandlee (2014) and Chandlee and Heinz (to appear). Processes that arent ISL include long-distance processes as well as local iterative spreading pro- cesses. As an example of the latter, consider nasal spreading in Johore Malay (Onn, 1980). As shown in (1), contiguous sequences of vowels and glides are nasalized following a nasal: (1) /p@Nawasan/ [p@N  a w  asan], supervision This process is not ISL, because the initial trigger of the spreading (the nasal) can be arbitrarily far from a target (as suggested by the nasalization of the glide 113 and the second [  a]) when the distance is measured on the input side. However, on the output side, the triggering context is local; the second [  a] is nasal- ized because the preceding glide on the output side is nasalized. Every segment between the trigger and target is affected; nasalization applies to a contigu- ous, but arbitrarily long, substring. It is this type of process that we will show requires the notion of Output Strict Locality. Processes in which a potentially unbounded num- ber of unaffected segments can intervene between the trigger and target - such as long-distance conso- nant agreement (Hansson, 2010; Rose and Walker, 2004), vowel harmony (Nevins, 2010; Walker, 2011), and consonant dissimilation (Suzuki, 1998; Bennett, 2013) - are neither ISL nor OSL. More will be said about such long-distance processes in 7. </chunk></section><section><heading>3 Preliminaries </heading><chunk>The set of all possible finite strings of symbols from a finite alphabet and the set of strings of length n are and n , respectively. The cardinality of a set S is denoted card(S). The unique empty string is represented with . The length of a string w is |w|, so || = 0. If w 1 and w 2 are strings then w 1 w 2 is their concatenation. The prefixes of w, Pref(w), is {p | (s )[w = ps]}, and the suffixes of w, Suff(w), is {s | (p )[w = ps]}. For all w and n N, Suff n (w) is the single suffix of w of length n if |w| n; otherwise Suff n (w) = w. The following reduction will prove useful later. Remark 1. For all w, v , n N, Suff n Suff n (w)v = Suff n (wv). If w = uv is a string then let v = u 1 w and u = w v 1 . Trivially, 1 w = w = w 1 , uu 1 w = w, and w v 1 v = w. We assume a fixed but arbitrary total order &lt; on the letters of . As usual, we extend &lt; to by defining the hierarchical order (Oncina et al., 1993), denoted , as follows: w 1 , w 2 , w 1 w 2 iff |w 1 | &lt; |w 2 | or |w 1 | = |w 2 | and u, v 1 , v 2 , a 1 , a 2 s.t. w 1 = ua 1 v 1 , w 2 = ua 2 v 2 and a 1 &lt; a 2 . is a total strict order over , and if = {a, b} and a &lt; b, then abaaabbabbaaa. . . The longest common prefix of a set of strings S, lcp(S), is p wS Pref(w) such that p wS Pref(w), |p | &lt; |p|. Let f : A B be a func- tion f with domain A and co-domain B. When A and B are stringsets, the input and output languages of f are pre image(f ) = {x | (y)[x f y]} and image(f ) = {y | (x)[x f y]}, respectively. Jardine et al. (2014) introduce delimited subse- quential FSTs (DSFSTs). The class of functions describable with DSFSTs is exactly the class repre- sentable by traditional subsequential FSTs (Oncina and Garcia, 1991; Oncina et al., 1993; Mohri, 1997), but DSFSTs make explicit use of symbols marking both the beginnings and ends of input strings. Definition 1. A delimited subsequential FST (DS- FST) is a 6-tuple Q, q 0 , q f , , , where Q is a finite set of states, q 0 Q is the initial state, q f Q is the final state, and are finite alphabets of symbols, Q ( {, }) Q is the transition function (where indicates the start of the input and indicates the end of the in- put), and the following hold: 1. if (q, , u, q ) then q = q f and q = q 0 , 2. if (q, , u, q f ) then = and q = q 0 , 3. if (q 0 , , u, q ) then = and if (q, , u, q ) then q = q 0 , 4. if (q, , w, r), (q, , v, s) then (r = s) (w = v). In words, in DSFST, initial states have no incom- ing transitions (1) and exactly one outgoing transi- tion for input (3) which leads to a nonfinal state (2), and final states have no outgoing transitions (1) and every incoming transition comes from a non- initial state and has input (2). DSFSTs are also deterministic on the input (4). In addition, the tran- sition function may be partial. We extend the transi- tion function to recursively in the usual way: is the smallest set containing and which is closed un- der the following condition: if (q, w, u, q ) and (q , , v, q ) then (q, w, uv, q ) . Note no elements of the form (q, , , q ) are elements of . The size of a DSFST T = Q, q 0 , q f , , , is |T | = card(Q) + card() + (q,a,u,q ) |u|. A DSFST T defines the following relation: R(T ) = (x, y) | (q 0 , x, y, q f ) 114 Since DSFSTs are deterministic, the relations they recognize are (possibly partial) functions. Sequen- tial functions are defined as those representable with DSFSTs for which for all (q, , u, q f ) , u = . 1 For any function f : and x , let the tails of x with respect to f be defined as tails f (x) = (y, v) | f (xy) = uv u = lcp(f (x )) . If x 1 , x 2 have the same set of tails with respect to f , they are tail-equivalent with respect to f , writ- ten x 1 f x 2 . Clearly, f is an equivalence relation which partitions . Theorem 1 (Oncina and Garcia, 1991). A function f is subsequential iff f partitions into finitely many blocks. The above theorem can be seen as the functional analogue to the Myhill-Nerode theorem for regular languages. Recall that for any stringset L, the tails of a word w w.r.t. L is defined as tails L (w) = {u | wu L}. These tails can be used to partition into a finite set of equivalence classes iff L is regular. Furthermore, these equivalence classes are the basis for constructing the (unique up to isomor- phism) smallest deterministic acceptor for a regular language. Likewise, Oncina and Garcias proof of Theorem 1 shows how to construct the (unique up to isomorphism) smallest subsequential transducer for a subsequential function f . With little modifi- cation to their proof, the smallest DSFST for f can also be constructed. We refer to this DSFST as the canonical DSFST for f and denote it T C (f ). (If f is understood from context, we may write T C .) States of T C (f ) which are neither initial nor final are in one-to-one correspondence with tails f (x) for all x (Oncina and Garcia, 1991). To construct T C (f ) we first let, for all x and a , the contribution of a w.r.t. x be cont f (a, x) = lcp(f (x ) 1 lcp(f (xa )). Then, Q = {tails f (x) | x } {q 0 , q f }, q 0 , , lcp(f ( )), tails f () For all x , tails f (x), , lcp(f (x )) 1 f (x), q f iff x pre image(f ) </chunk></section><section><heading>1 Sakarovitch (2009) inverts these terms. </heading><chunk>For all x , a , if y with xay pre image(f ) then tails f (x), a, cont f (a, x), tails f (xa) . Nothing else is in . Observe that unlike the traditional construction, the initial state q 0 is not tails f (). The single outgo- ing transition from q 0 , however, goes to this state with the input . Canonical DSFSTs have an im- portant property called onwardness. Definition 2 (onwardness). A DSFST T is onward if for every w , u , (q 0 , w, u, q) u = lcp({f (w )}). Informally, this means that the writing of output is never delayed. For all q Q let the outputs of the edges out of q be outputs(q) = u | ( {, })(q Q)[(q, , u, q ) ] . Lemma 1. If DSFST T recognizes f and is on- ward then q = q 0 lcp(outputs(q)) = and lcp(outputs(q 0 )) = lcp(f ( )). Proof. By construction of a DSFST, only one transition leaves q 0 : (q 0 , , u, q). This implies (q 0 , , u, q) and as the transducer is onward we have lcp(outputs(q 0 )) = lcp(u) = u = lcp(f ( )) = lcp(f ( )). Now take q = q 0 and w such that (q 0 , w, u, q) . Suppose lcp(outputs(q)) = v = . Then v is a prefix of lcp({f (wx) | {}, x }) which im- plies uv is a prefix of lcp(f (w )). But v = , contradicting the fact that T is onward. Readers are referred to Oncina and Garcia (1991), Oncina et al. (1993), and Mohri (1997) for more on subsequential transducers, and Eisner (2003) for generalizations regarding onwardness. 4 Output Strictly Local functions Here we define Output Strictly Local (OSL) func- tions, which were originally introduced by Chandlee (2014) and Chandlee et al. (2014) along with the In- put Strictly Local (ISL) functions. Both classes gen- eralize SL stringsets to functions based on a defin- ing property of SL languages, the Suffix Substitution Closure (Rogers and Pullum, 2011). Theorem 2 (Suffix Substitution Closure). L is Strictly Local iff for all strings u 1 , v 1 , u 2 , v 2 , there 115 exists k N such that for any string x of length k 1, if u 1 xv 1 , u 2 xv 2 L, then u 1 xv 2 L. An important corollary of this theorem follows. Corollary 1 (Suffix-defined Residuals). L is Strictly Local iff w 1 , w 2 , there exists k N such that if Suff k1 (w 1 ) = Suff k1 (w 2 ) then the residuals (the tails) of w 1 , w 2 with respect to L are the same; formally, {v | w 1 v L} = {v | w 2 v L}. Input and Output Strictly Local functions were defined by Chandlee (2014) and Chandlee et al. (2014) in the manner suggested by the corollary. Definition 3 (Input Strictly Local Functions). A function f : is ISL if there is a k such that for all u 1 , u 2 , if Suff k1 (u 1 ) = Suff k1 (u 2 ) then tails f (u 1 ) = tails f (u 2 ). Definition 4 (Output Strictly Local Functions (orig- inal)). A function f : is OSL if there is a k such that for all u 1 , u 2 , if Suff k1 (f (u 1 )) = Suff k1 (f (u 2 )) then tails f (u 1 ) = tails f (u 2 ). While Definition 3 lead to an automata-theoretic characterization and learning results for ISL (Chan- dlee et al., 2014), such results do not appear possi- ble with the original definition of OSL. The trouble is with subsequential functions that are not sequen- tial. The value returned by the function includes the writing that occurs when the input string has been fully read (i.e., the output of transitions going to the final state in a corresponding DSFST). This creates a problem because it does not allow for separation of what happens during the computation from what happens at its end. Figure 1 illustrates the distinction Definition 4 is unable to make. 2 Function f is sequential, but g is not. Otherwise, they are identical. While f (bab) = bba, g(bab) = bbaa. With the original OSL defini- tion, there is no way to refer to the output for input bab before the final output string has been appended. To deal with this problem we first define the prefix function associated to a subsequential function. Definition 5 (Prefix function). Let f : be a subsequential function. We define the prefix function f p : associated to f such that f p (w) = lcp({f (w )}). </chunk></section><section><heading>2 Here and in Figure 3, state q </heading><chunk>f is not shown. Non-initial states are labeled q : u with q being the states name and (q, , u, q f ) . T f q 0 : : a: a:a a:a b: b:b b:b b:a a:b q 0 T g : : a:a a:a a:a b:b b:b b:b b:a a:b 1 Figure 1: Two DSFST recognizing functions f and g. Except for their final transitions, T f and T g are identical. Remark 2. If T is an onward DSFST for f , then w , f p (w) = u q, (q 0 , w, u, q) . Remark 3. If f is sequential then f = f p . We can now revise the definition of OSL functions. Definition 6 (Output Strictly Local Function (re- vised)). We say that a subsequential function f is k-OSL if for all w 1 , w 2 in , Suff k1 (f p (w 1 )) = Suff k1 (f p (w 2 )) tails f (w 1 ) = tails f (w 2 ). Chandlee et al. (2014) provide several theorems which relate ISL functions, OSL functions (defined as in Definition 4), and SL stringsets. Here we ex- plain why those results still hold with Definition 6. The proofs for those results depend on the six func- tions (f i , 1 i 6) reproduced here in Figure 2. The transducers shown there are not DSFSTs but traditional subsequential transducers; readers are re- ferred to Chandlee et al. (2014) for formal defini- tions. With the exception of f 5 , these functions are clearly sequential since each state outputs on (shown as # in Figure 2). The transducer for f 5 is not onward, but an onward, sequential version of this transducer recognizing exactly the same function is obtained by suffixing a (which is the lcp of the out- puts of state 1) onto the output of state 1s incoming transition. Thus, f 5 is also sequential. By Remark 3 then, Theorems 4, 5, 6, and 7 of that paper still hold under Definition 6. </chunk></section><section><heading>5 Automata characterization </heading><chunk>First we show, for any non-initial state of any canon- ical transducer recognizing an OSL function, that if reading a letter a implies writing , then this corre- sponds to a self-loop. So writing the empty string never causes a change of state (except from q 0 ). Lemma 2. For any OSL function f whose canonical DSFST is T C , if q = q 0 , a , and q Q such 116 0 1 a #: #: b,c a,b:a,c f 1 a b b a a b #: #: #: a:b b f 2 1 a b a a b,c:b #: #: #: a,b:a,c:a b,c:b f 3 a b b a:aa a:aa b #: #: #: a:aa b f 4 1 0 1 a #: #:a a f 5 1 0 a a: #: #: f 6 1 Figure 2: Examples used in proofs of Theorems 4 to 7 of Chandlee et al. (2014, see Figure 2). that (q, a, , q ) C then q = q. Proof. Consider w and u such that (q 0 , w, u, q) C and suppose (q, a, , q ) C . Then f p (w) = f p (wa) which implies Suff k1 (f p (w)) = Suff k1 (f p (wa)). As f is k-OSL, tails f (w) = tails f (wa). As T C is canonical the non-initial and non-final states correspond to unique tail-equivalence classes, and two distinct states correspond to two different classes. Therefore q = q. Next we define k-OSL transducers. Definition 7 (k-OSL transducer). An onward DS- FST T = Q, q 0 , q f , , , is k-OSL if 1. Q = S {q 0 , q f } with S k1 2. (u ) (q 0 , , u, q ) = q = Suff k1 (u) 3. (q Q\{q 0 }, a , u ) (q, a, u, q ) = q = Suff k1 (qu) . Next we show that k-OSL functions and functions represented by k-OSL DSFSTs exactly correspond. Lemma 3 (extended transition function). Let T = Q, q 0 , q f , , , be a k-OSL DSFST. We have (q 0 , w, u, q) = q = Suff k1 (u) Proof. By recursion on the size of w . The ini- tial case is valid for |w| = 0 since if (q 0 , , u, q) then (q 0 , , u, q) . By Definition 7, q = Suff k1 (u). Suppose now that the lemma holds for inputs of size n = 0. Let w be of size n such that (q 0 , w, u, q) and suppose (q, a, v, q ) (i.e., (q 0 , wa, uv, q ) ). By recursion, we know that q = Suff k1 (u). By Definition 7, q = Suff k1 (qv) = Suff k1 (Suff k1 (u)v) = Suff k1 (uv) (by Remark 1). Lemma 4. Any k-OSL DSFST corresponds to a k- OSL function. Proof. Let T = Q, q 0 , q f , , , be a k- OSL DSFST computing f and let w 1 , w 2 such that Suff k1 (f p (w 1 )) = Suff k1 (f p (w 2 )). Since T is onward, by Remark 2 there exists q, q Q such that (q 0 , w 1 , f p (w 1 ), q) and (q 0 , w 2 , f p (w 2 ), q ) . By Lemma 3, q = Suff k1 (f p (w 1 )) = Suff k1 (f p (w 2 )) = q which implies tails f (w 1 ) = tails f (w 2 ). Therefore f is a k-OSL function. We now need to show that every k-OSL function can be represented by a k-OSL DSFST. An issue here is that one cannot work from T C since its states are defined in terms of its tails, which themselves are defined in terms of input strings, not output strings. Hence, the proof below is constructive. Theorem 3. Let f be a k-OSL function. The DSFST T defined as followed computes f : Q = S {q 0 , q f } with S k1 (q 0 , , u, Suff k1 (u)) u = f p () a , (q, a, u, Suff k1 (qu)) , (w) Suff k1 (f p (w)) = q f p (wa) = vqu with v = f p (w) q 1 , (q, , u, q f ) u = f p (w q ) 1 f (w q ), where w q = min {w | u, (q 0 , w, u, q) }. The diagram below helps express pictorially how the transitions are organized per the second and third bullets above. The input is written above the arrows, and the output written below. q 0 w f p (w)=vq q a u q 117 Note that T is a k-OSL SFST. To prove this result, we first show the following lemma: Lemma 5. Let T be the transducer defined in The- orem 3. We have: (q 0 , w, u, q) f p (w) = u Proof. () By recursion on the length of w. Sup- pose (q 0 , w, u, q) and |w| = 0. Then (q 0 , , u, q) ; by construction, q = Suff k1 (u) and f p () = u which validates the initial case. Suppose the result holds for w of size n and pick such a w. Suppose then that (q 0 , wa, u, q) . By definition of , there exists u 1 , u 2 , q such that u = u 1 u 2 , (q 0 , w, u 1 , q ) and (q , a, u 2 , q) . We have f p (w) = u 1 (by recursion) and thus q = Suff k1 (f p (w)) (by Lemma 3). By construction of T , q = Suff k1 (q u 2 ) and thus f p (wa) = vq u 2 with v = f p (w) Suff k1 (f p (w)) 1 . Therefore f p (wa) = vq u 2 = f p (w) Suff k1 (f p (w)) 1 q u 2 = f p (w) Suff k1 (f p (w)) 1 Suff k1 (f p (w))u 2 = f p (w)u 2 = u 1 u 2 = u. () By recursion on the length of w. If |w| = 0, then f p () = u. By construction of T , (q 0 , , u, Suff k1 (u)) , which validates the base case. Now fix n &gt; 0 and suppose the result holds for all w of size n. Pick such a w and let f p (wa) = u. As f is subsequential, there ex- ists u 1 such that f p (w) = u 1 . By recursion, there exists q such that (q 0 , w, u 1 , q) . By Lemma 3, q = Suff k1 (u 1 ) = Suff k1 (f p (w)). By definition f p (wa) = u 1 u 1 1 u, which equals u 1 Suff k1 (u 1 ) 1 Suff k1 (u 1 )u 1 1 u. Hence f p (wa) = vqu , with u = u 1 1 u and v = u 1 Suff k1 (u 1 ) 1 , which equals f p (w) Suff k1 (f p (w)) 1 . Thus, by construction (q, a, u , Suff k1 (qu )) . Since u 1 u = u 1 u 1 1 u = u, (q 0 , wa, u, Suff k1 (qu )) . We can now prove Theorem 3. Proof. Let T be the transducer defined in Theo- rem 3. We show that w , (w, u) R(T ) f (w) = u. By definition of R(T ), we know that (q 0 , w, u, q f ) . By definition of , q 0 : C: V: V: N: : C:C V:V N:N V: V C:C V:V C:C N:N N:N C:C N:N N:N V: V V:V C:C 1 Figure 3: A 2-OSL DSFST that models Johore Malay nasal spreading. ={C, N, V} and = {C, N, V, V}. there exists u 1 , u 2 and q Q\{q 0 , q f } such that (q 0 , w, u 1 , q) , (q, , u 2 , q f ) , and u = u 1 u 2 . By Lemma 5 we know that f p (w) = u 1 . By construction of the DS- FST, we have u 2 = f p (w q ) 1 f (w q ) where w q = min {w | u, (q 0 , w , u , q) }. Therefore (w q , u u 2 ) R(T ). Again, by Lemma 5, f p (w q ) = u and so u u 2 = f p (w q )u 2 = f p (w q )f p (w q ) 1 f (w q ) = f (w q ). We have Suff k1 (u 1 ) = Suff k1 (f p (w q )) = Suff k1 (f p (w)). As f is k-OSL, we know tails f (w q ) = tails f (w), which implies that (, f p (w q ) 1 f (w q )) tails f (w). Thus f (w) = f p (w)f p (w q ) 1 f (w q ) = u 1 u 2 = u. Figure 3 presents a 2-OSL transducer that mod- els the nasal spreading example from 2. Note C = obstruent, V = vowels and glides, V = nasalized V, and N = nasal consonant. </chunk></section><section><heading>6 Learning OSL functions </heading></section><section><heading>6.1 Learning criterion </heading><chunk>We adopt the identification in the limit learning paradigm (Gold, 1967), with polynomial bounds on time and data (de la Higuera, 1997). The underlying idea of the paradigm is that if the data available to the algorithm does not contain enough information to distinguish the target from other potential targets, then it is impossible to learn. We first need to define the following notions. A class T of functions is represented by a class R of representations if every r R is of finite size 118 and there is a total and surjective naming function L : R T such that L(r) = t if and only if for all w pre image(t), r(w) = t(w), where r(w) is the output of representation r on the input w. We observe that the class of k-OSL functions can be rep- resented by the class of k-OSL DSFSTs. Definition 8. Let T be a class of functions repre- sented by some class R of representations. 1. A sample S for a function t T is a finite set of data consistent with t, that is to say (w, v) S iff t(w) = v. The size of a sample S is the sum of the length of the strings it is composed of: |S| = (w,v)S |w| + |v|. 2. A (T, R)-learning algorithm A is a program that takes as input a sample for a function t T and outputs a representation from R. The paradigm relies on the notion of characteristic sample, adapted here for functions: Definition 9 (Characteristic sample). For a (T, R)- learning algorithm A, a sample CS is a character- istic sample of a function t T if for all samples S for t it is the case that CS S and A returns a representation r such that L(r) = t. This definition is the one used in the proof of the OSTIA algorithm. The learning paradigm can now be defined as follows. Definition 10 (Identification in polynomial time and data). A class T of functions is identifiable in polynomial time and data if there exists a (T, R)- learning algorithm A and two polynomials p() and q() such that: 1. For any sample S of size m for t T, A returns a hypothesis r R in O(p(m)) time. 2. For each representation r R of size n, with t = L(r), there exists a characteristic sample of t for A of size at most O(q(n)). </chunk></section><section><heading>6.2 Learning algorithm </heading><chunk>We show here that Algorithm 1 learns the OSL func- tions under the criterion introduced. We call this the Output Strictly Local Function Inference Algorithm (OSLFIA). We assume , , and k are fixed and not part of the input to the learning problem. Essentially, the algorithm computes a breadth- first search through the states that are reachable Data: Sample S {} {} and k N Let q 0 , q f be states with {q 0 , q f } k1 = s lcp({y | (x, y) S}); q Suff k1 (s); smallest(q) = ; out(q) = s; {(q 0 , , s, q)}; R {q}; C {q 0 , q f }; while R = do q f irst(R); s smallest(q); for all a in alphabetical order do if (w, u) S, x s.t. w = sax then v lcp({y | x, (sax, y) S}); r Suff k1 (qv); {(q, a, out(q) 1 v, r)}; if r / R C then R R {r}; smallest(r) sa; out(r) v; if u, (s, u) S then {(q, , out(q) 1 u, q f )} R R \ {q}; C C {q}; return C, q 0 , q f , , , ; Algorithm 1: OSLFIA given the learning sample: the set C contains the states already checked while R is a queue made of the states that are reachable but have not been treated yet. Initially, the only transition leaving the initial state is writing the lcp of the output strings of the sample and reaches the state corresponding to the k 1 suffix of this lcp. At each step of the main loop, OSLFIA treats the first state that is in the queue R and computes whenever possible the transitions that leave that state. The outputs associated with each added transition are the longest common pre- fixes of the outputs associated with the smallest in- put prefix in the sample that allows the state to be reachable. We show that provided the algorithm is given a sufficient sample the transducer outputted by OSLFIA is onward and in fact a k-OSL transducer. After adding transitions with input letters from to a state, the transition to the final state is added, pro- vided it can be calculated. </chunk></section><section><heading>6.3 Theoretical results </heading><chunk>Here we establish the theoretical results, which cul- minate in the theorem that OSLFIA identifies the k- 119 OSL functions in polynomial time and data. Lemma 6. For any input sample S, OSLFIA pro- duces its output in time polynomial in the size of S. Proof. The main loop is used at most || k1 which is constant since both and k are fixed for any learning sample. The smaller loop is executed || times. At each execution: the first conditional can be tested in time linear in n, where n = (w,u)S |w|; the computation of the lcp can be done in nm steps where m = max{|u| : (w, u) S} with an appro- priate data structure (for instance a prefix tree); com- puting the suffix requires at most m steps. The sec- ond conditional can be tested in at most card(S) m steps; the computation of the final transitions can be done in less than m steps; all the other instructions can be done in constant time. The overall computa- tion time is thus O(|| k1 ||(n + nm + card(S) m+2m)) = O(n+m(n+card(S)) which is poly- nomial (in fact bounded by a quadratic function) in the size of the learning sample. Next we show that for each k-OSL function f , there is a finite kernel of data consistent with f (a seed) that is a characteristic sample for OSLFIA. Definition 11 (A OSLFIA seed). Given a k-OSL transducer Q, q 0 , q f , , , computing a k-OSL function f , a sample S is a OSLFIA seed for f if For all q Q such that v (q, , v, q f ) , (w q , f (w q )) S, where w q = min {w | u, (q 0 , w, u, q) } For all (q, a, u, q ) with q = q f and a {}, for all b such that there exists (q , b, u , q ) , there exists (w, f (w)) S and x such that w = w q abx and f (w) is defined. Also, if there exists v such that (q , , v, q f ) then (w q a, f (w q a)) S. In what follows, we set T = Q , q 0 , q f , , , be the target k-OSL transducer, f the function it computes, and T = Q, q 0 , q f , , , be the transducer OSLFIA constructs on a sample that contains a seed. Lemma 7. If a learning sample S contains a seed then (q 0 , w, u, r) (q 0 , w, u, r) . Proof. (). By induction on the length of w. If |w| = 0 then (q 0 , , u, r) and so u = lcp({y | (x, y) S}) and r = Suff k1 (u) (initial steps of the algorithm). As S is a seed there is an element (bx, f (bx)) S for all b and (, f ()) S if pre image(f ), which implies that u = lcp(f ( )). As the target is onward, we have (q 0 , , lcp(f ( )), r ) and since it is a k-OSL DSFST r = Suff k1 (lcp(f ( ))) = Suff k1 (u) = r. Suppose the lemma is true for strings of length less than or equal to n. We refer to this as the first Inductive Hypothesis (IH1). Let wa be of size n + 1 such that (q 0 , wa, u, r) . By definition of , there exist u 1 , u 2 , q such that (q 0 , w, u 1 , q) , (q, a, u 2 , r) , and u = u 1 u 2 . By IH1 (q 0 , w, u 1 , q) . We want to show (q 0 , wa, u, r) (i.e., (q, a, u 2 , r) ). First we show that IH1 also implies that s = smallest(q) such that s = w q . Since the algo- rithm searches breadth-first, s is the smallest input that reaches q in T . If w q s then q = q such that (q 0 , w q , u , q ) because w q is a prefix of an input string of the sample S (since S contains a seed). Since w q s and |s| n, by IH1 then (q 0 , w q , u , q ) which implies q = q which contradicts the supposition that w q s. If sw q , then again since (q 0 , s, u , q) then by IH1 (q 0 , s, u , q) . This contradicts the definition of w q . Therefore s = w q . Next we show that IH1 implies f p (w q ) = out(q). By construction of the seed, (w q , f (w q )) S if v (q, , v, q f ) and (w q aw , f (w q aw )) S for all transitions (q, a, x, q ) leaving q in T . As the target is onward, lcp({x | (q, , x, q ) , {}} = (Lemma 1). This implies out(q) = lcp({y | a , x {}, (sax, y) S}) = lcp({y | a , x {}, (w q ax, y) S}) = lcp({f (w q )}) = f p (w q ). Recalling that (q, a, u 2 , r) , we now char- acterize u 2 to help establish (q, a, u 2 , r) . By construction of a seed, there exist elements (w q abx, f (w q abx)) in S for all possible b and an element (w q a, f (w q a)) S if f (w q a) is defined. By the onwardness of the target, this implies that v = lcp({y | b, x, (sabx, y) S} {f (sa)}) = lcp(f (sa )) = f p (sa). There- fore u 2 = out(q) 1 v = f p (s) 1 f p (sa) = f p (w q ) 1 f p (w q a). Finally we identify r to complete this part 120 of the proof. As the target is OSL, we have (q 0 , w q a, f p (w q a), r ) (Lemma 3). The fact that (q 0 , w q , f p (w q ), q) by IH1 and the fact the target is OSL implies (q, a, f p (w q ) 1 f p (w q a), r ) = (q, a, out(q) 1 s, r ) . As T is k-OSL, r = Suff k1 (qout(q) 1 s) which is r by construction of the transition in the algo- rithm. Therefore, as (q 0 , w, u 1 , q) by IH1, we have (q 0 , wa, u 1 out(q) 1 s, r) = (q 0 , wa, u 1 u 2 , r) = (q 0 , wa, u, r) (). This is also by induction on the length of w. If |w| = 0, as T is onward we have lcp(outputs(q 0 )) = lcp(f ( )) (Lemma 1) and thus (q 0 , , lcp(f ( )), r) with r = Suff k1 (lcp(f ( ))) as T is k-OSL. By con- struction of the seed, there is at least one ele- ment in S using each transition leaving r. As lcp(outputs(r)) = (Lemma 1), this implies lcp({y | (x, y) S}) = lcp(f ( )). Therefore (q 0 , , lcp(f ( )), r) . Suppose the lemma is true for all strings up to length n. We refer to this as the second Inductive Hypothesis (IH2). Pick wa of length n + 1 such that (q 0 , wa, u, r) . By definition of , q, u 1 , u 2 exist such that (q 0 , w, u 1 , q) and (q, a, u 2 , r) , with u 1 u 2 = u. By IH2, we have (q 0 , w, u 1 , q), (q 0 , w q , u 1 , q) (since w q w). We want to show (q, a, u 2 , r) . We first show that s = smallest(q) = w q . Suppose s w q . By construction of the SFST s is a prefix of an element of S which means there exists q such that (q 0 , s, f p (s), q ) . But by IH2, this implies that q = q and the definition of w q contradicts s w q . Suppose now that w q s. By the construction of the seed, w q is a prefix of an element of the sample, which implies it is considered by the algorithm. As (q 0 , w q , u 1 , q) by IH2, w q is a smaller prefix than s that reaches the same state which is impossible as s is the earliest prefix that makes the state q reachable. Therefore w q = s and thus the transition from state q reading a is created when s = w q . Next we show that f p (w q ) = out(q). By construction of the seed, there is an element (w q aw , f (w q aw )) S for all transitions (q, a, x, q ) leaving q and (w q , f (w q )) S if v, (q, , v, q f ) . As the target is onward, lcp({x | (q, , x, q) , } = (Lemma 1). This implies out(q) = lcp({y | a, x, (sax, y) S}) = lcp({y | a, x, (w q ax, y) S}) = lcp(f (w q )) = f p (w q ) = f p (s). Now let v = lcp({y | b, x, (sabx, y) S}). Since s = w q , (q 0 , w q a, v, r) since, as be- fore, the onwardness of the target implies the lcp of the output written from r is . This is because each possible output from r is in S (because it is in the seed according to the second item of Definition 11). Consequently v = f p (w q a) = f p (sa). Together these results imply that u 2 = f p (w q ) 1 f p (w q a) = f p (s) 1 f p (sa) = out(q) 1 v. As the target is a k-OSL transducer (and thus de- terministic) Suff k1 (qu 2 ) = r. Therefore the tran- sition (q, a, out(q) 1 v, r) that is added to is the same as the transition (q, a, u 2 , r) in . This implies (q 0 , wa, u, r) and proves the lemma. Lemma 8. Any seed for the OSL Learner is a char- acteristic sample for this algorithm. Proof. A corollary of Lemma 7 is that if a seed is contained in a learning sample we have (q 0 , w, u, q) f p (w) = u (Lemma 3) as the target transducer is k-OSL. For all states q where v, (q, , v, q f ) , we have (w q , f (w q )) in the seed, which implies the algorithm will add (q, , f p (w q ) 1 f (w q ), q f ) to which is exactly the output function of the target. As every state is treated only once, this holds for any learning set containing a seed. Therefore, from any super- set of a seed, for any w, the function computed by the outputted transducer of Algorithm 1 is equal to f p (w)f p (w) 1 f (w) = f (w). Observe that OSLFIA is designed to work with seeds, which contains minimal strings. We believe both the seed and algorithm can be adjusted to relax this requirement, though this is left for future work. Lemma 9. Given any k-OSL transducer T , there exists a seed for the OSL learner that is of size poly- nomial in the size of T . Proof. Let T = Q , q 0 , q f , , , . There are at most card(Q ) pairs (w q , f (w q )) in a seed that corresponds to the first item of Definition 11, each of which is such that | w q | card(Q ) 121 and |f (w q )| (q,,u,q ) |u|. We denote by m this last quantity and note that m = O(|T |). For the elements of the second item of Def- inition 11 we restrict ourselves without loss of generality to pairs (w q abw , f (w q abw )) where w = min {x : f (w q abx) is defined}. We have |w | card(Q ) and |f (w q abw )| is in O(card(Q )m ). There are at most || pairs (w q abw , f (w q abw )) for a given transition (q, a, u, q ) which implies that the overall bound on the number of such pairs is in O(||card()). The overall length of the elements in the seed that fulfill the second item of the definition is in O(card(Q )(card(Q ) + m + ||card()m )). The size of the seed studied in this proof is thus in O((m + |Q |)(|Q | + ||card()) which is poly- nomial (in fact quadratic) in the size of the target transducer. Theorem 4. OSLFIA identifies the k-OSL functions in polynomial time and data. Proof. Immediate from Lemmas 6, 7, 8, and 9. We conclude this section by comparing this result to other subsequential function-learning algorithms. OSTIA (Oncina et al., 1993) is a state-merging algorithm which can identify the class of total sub- sequential functions in cubic time. (Partial subse- quential functions cannot be learned exactly; for a partial function, OSTIA will learn some superset of it.) k-OSL functions include both partial and total functions, so the classes exactly learnable by OSTIA and OSLFIA are, strictly speaking, incomparable. SOSFIA (Jardine et al., 2014) identifies sub- classes of subsequential functions in linear time and data. These subclasses are determined by fixing the structure of a transducer in advance. For every in- put string, SOSFIA knows exactly which state in the transducer is reached. The sole carrier of informa- tion regarding reached states is the input string. But for k-OSL functions, the output strings carry the in- formation about the states reached. As the theorems demonstrate, the destination of a transition is only determined by the output of the transition. Thus no class learned by SOSFIA contains any k-OSL class. OSTIA-D (OSTIA-R) (Oncina and Var` o, 1996; Castellanos et al., 1998) identify a class of subse- quential functions with a given domain D (range R) in at least cubic time because it adds steps to OSTIA to prevent merging states that would result in a trans- ducer whose domain (range) is not compatible with D (R). OSTIA-D cannot represent k-OSL functions for the same reasons SOSFIA cannot: domain in- formation is about input strings, not output strings. On the other hand, the range of a k-OSL function is a k-OSL stringset which can be represented with a single acceptor, and thus OSL functions may be learned by OSTIA-R. However, OSLFIA is more ef- ficient both in time and data. 3 To sum up, OSLFIA is the most efficient algo- rithm for learning k-OSL functions. 7 Phonology The example of Johore Malay nasal spreading given in 2 is an example of progressive spreading, since it proceeds from a triggering segment (the nasal) to vowels and glides that follow it. There also exist regressive spreading processes, in which the trigger follows the target(s). An example from the M` ob` a di- alect of Yoruba (Aj  b  oy` e, 2001; Aj  b  oy` e and Pulley- blank, 2008; Walker, 2014) is shown in (2). An un- derlying nasalized vowel spreads its nasality to pre- ceding oral vowels and glides. (2) /uj i/ [  u j i], praise(n.) The difference between progressive and regressive spreading corresponds to reading the input from left- to-right or right-to-left, respectively (Heinz and Lai, 2013). Regressive spreading cannot be modeled with OSL in a left-to-right fashion, because the out- put of the preceding vowels and glides depends on the presence or absence of a following nasal that could be an unbounded number of segments away. By reading from right-to-left, that nasal trigger will always be read before the target(s), making it akin to progressive spreading. Thus there are two overlap- ping but non-identical classes, which we call left(- to-right) OSL and right(-to-left) OSL. There are other types of phonological maps that are neither ISL nor OSL. Consider the optional pro- cess of French @-deletion shown in (3) (Dell, 1973; Dell, 1980; Dell, 1985; Noske, 1993). 3 To our knowledge no analysis of data complexity for OS- TIA, OSTIA-D, and OSTIA-R has been completed (probably because they predate de la Higuera (1997)). Also, an analysis of the data complexity of OSTIA appears daunting. 122 (3) @ / VC CV At issue is how this rule applies. There are two licit pronunciations of /ty d@v@nE/ you became which are [ty dv@nE] and [ty d@vnE]. The form *[ty dvnE] is considered ungrammatical. As Ka- plan and Kay (1994) explain, these outputs can be understood as the rule in (3) applying left-to-right ([ty dv@nE]), right-to-left ([ty dv@nE]) or simultane- ously (*[ty dvnE]). What matters is whether the left and right contexts of the rule match the input or out- put string: if both match the input it is simultaneous application, and if one side matches the input and the other the output it is left-to-right or right-to-left. ISL functions always match contexts against the input and therefore they cannot model @-deletion. In this respect, ISL functions model simultaneous rule application. But there is also a problem with mod- eling the process as OSL, which is what to output when the @ that will be deleted is read. Consider the input VC@CV. When the DSFST reads the @, it can- not decide what to output, because whether or not that @ is deleted depends on whether or not the next two symbols in the input are CV. But since the DS- FST is deterministic, it must make a decision at this point. It could postpone the decision and output . But that would require it to loop at the current state (Lemma 2), which in turn means it cannot distin- guish VC@CV from VC@@@CV, a significant problem since only the former meets the context for deletion. Thus the range of phonological processes that can be modeled with OSL functions is limited to those with one-sided contexts (e.g., either C or D, the former being left OSL and the latter right OSL). In such cases the entire triggering context will be read before the potential target, so there is never a need to delay the decision about what to output. To summa- rize, phonological rules that apply simultaneously are ISL, and phonological rules with one-sided con- texts that apply left-to-right or right-to-left are OSL. In addition to iterative rules with two-sided con- texts, long-distance processes like vowel harmony and consonant agreement and dissimilation are also excluded from the current analysis. While such process have been shown to be subsequential and therefore subregular (see Gainor et al. (2012; Luo (2014; Payne (2013; Heinz and Lai (2013)) they are neither ISL nor OSL because the target and trigger- ing context are not within a fixed window of length k in either the input or output. An example is the long-distance nasal assimilation process in Kikongo (Rose and Walker, 2004), as in (4). (4) /tu+nik+idi/ [tunikini] we ground In Kikongo, the alveolar stop in the suffix /-idi/ sur- faces as a nasal when joined to a stem containing a nasal. Since stem nasals appear to occur arbitrarily far from the suffix, there is no k such that the target /d/ and the trigger /n/ are within a window of size k. Thus the process is neither ISL nor OSL. </chunk></section><section><heading>8 Future Work </heading><chunk>Processes like French @-deletion that have two-sided contexts, with one being on the output side, sug- gest a class that combines the ISL and OSL prop- erties. We are tentatively calling this class Input- Output SL and are currently working on its prop- erties, FST characterization, and learning algorithm. For long-distance processes, we expect other func- tional subclasses will strongly characterize these. SL stringsets are just one region of the Subregular Hierarchy (Rogers and Pullum, 2011; Rogers et al., 2013), so we expect functional counterparts of the other regions can be defined. Some of these other regions model long-distance phonotactics (Heinz, 2007; Heinz, 2010; Rogers et al., 2010), so their functional counterparts may prove equally useful for modeling and learning long-distance phonology. </chunk></section><section><heading>9 Conclusion </heading><chunk>We have defined a subregular class of func- tions called the OSL functions and provided both language-theoretic and automata-theoretic charac- terizations. The structure of this class is sufficient to allow any k-OSL function to be efficiently learned from positive data. It was shown that the OSL functionsunlike the ISL functionscan model lo- cal iterative spreading processes. Future work will aim to combine the results for both ISL and OSL to model iterative processes with two-sided contexts. Acknowledgments We thank three reviewers for useful comments, es- pecially the third, who caught a significant error in the first version of this paper. 123 References Ol  adi  p` o Aj  b  oy` e and Douglas Pulleyblank. 2008. M` ob` a nasal harmony. Ms., University of Lagos and Univer- sity of British Columbia. Ol  adi  p` o Aj  b  oy` e. 2001. Nasalization in M` ob` a. In Suny- oung Oh, Naomi Sawai, Kayono Shiobara, and Rachel Wojdak, editors, Proceedings of the Northwest Lin- guistics Conference, pages 118. University of British Columbia Working Papers in Linguistics 8. Vancou- ver: University of British Columbia, Department of Linguistics. William Bennett. 2013. Dissimilation, Consonant Har- mony, and Surface Correspondence. Ph.D. thesis, Rut- gers. Antonio Castellanos, Enrique Vidal, Miguel A. Var  o, and Jos  e Oncina. 1998. Language understanding and sub- sequential transducer learning. Computer Speech and Language, 12:193228. Jane Chandlee and Jeffrey Heinz. 2012. Bounded copy- ing is subsequential: Implications for metathesis and reduplication. In Proceedings of the 12th Meeting of the ACL Special Interest Group on Computational Morphology and Phonology, pages 4251, Montreal, Canada, June. Association for Computational Linguis- tics. Jane Chandlee and Jeffrey Heinz. to appear. Strictly lo- cal phonological processes. Linguistic Inquiry,. under revision. Jane Chandlee, Angeliki Athanasopoulou, and Jeffrey Heinz. 2012. Evidence for classifying metathesis pat- terns as subsequential. In The Proceedings of the 29th West Coast Conference on Formal Linguistics, pages 303309. Cascadilla Press. Jane Chandlee, R  emi Eyraud, and Jeffrey Heinz. 2014. Learning strictly local subsequential functions. Trans- actions of the Association for Computational Linguis- tics, 2:491503, November. Jane Chandlee. 2014. Strictly Local Phonological Pro- cesses. Ph.D. thesis, The University of Delaware. Noam Chomsky and Morris Halle. 1968. The Sound Pattern of English. New York: Harper &amp; Row. Colin de la Higuera. 1997. Characteristic sets for polynomial grammatical inference. Machine Learning Journal, 27:125138. Franc  ois Dell. 1973. Les r  egles et les sons. Paris: Her- mann. Franc  ois Dell. 1980. Generative phonology and French phonology. Cambridge: Cambridge University Press. Franc  ois Dell. 1985. Les r  egles et les sons. Paris: Her- mann, 2 edition. Jason Eisner. 2003. Simpler and more general minimiza- tion for weighted finite-state automata. In Proceedings of the Joint Meeting of the Human Language Technol- ogy Conference and the North American Chapter of the Association for Computational Linguistics (HLT- NAACL 2003), pages 6471. Robert Frank and Giorgo Satta. 1998. Optimality Theory and the generative complexity of constraint violability. Computational Linguistics, 24(2):307315. Brian Gainor, Regine Lai, and Jeffrey Heinz. 2012. Computational characterizations of vowel harmony patterns and pathologies. In Jaehoon Choi, E. Alan Hogue, Jeffrey Punske, Deniz Tat, Jessamyn Schertz, and Alex Trueman, editors, WCCFL 29: Proceedings of the 29th West Coast Conference on Formal Linguis- tics, pages 6371, Somerville, MA. Cascadilla. E.Mark Gold. 1967. Language identification in the limit. Information and Control, 10:447474. Thomas Graf. 2010. Logics of phonological reasoning. Masters thesis, University of California, Los Angeles. Gunnar Hansson. 2010. Consonant Harmony: Long- Distance Interaction in Phonology. Number 145 in University of California Publications in Linguistics. University of California Press, Berkeley, CA. Avail- able on-line (free) at eScholarship.org. Jeffrey Heinz and Regine Lai. 2013. Vowel harmony and subsequentiality. In Andras Kornai and Marco Kuhlmann, editors, Proceedings of the 13th Meeting on the Mathematics of Language (MoL 13), pages 52 63, Sofia, Bulgaria. Jeffrey Heinz. 2007. The Inductive Learning of Phono- tactic Patterns. Ph.D. thesis, University of California, Los Angeles. Jeffrey Heinz. 2009. On the role of locality in learning stress patterns. Phonology, 26(2):303351. Jeffrey Heinz. 2010. Learning long-distance phonotac- tics. Linguistic Inquiry, 41(4):623661. Adam Jardine, Jane Chandlee, R  emi Eyraud, and Jef- frey Heinz. 2014. Very efficient learning of struc- tured classes of subsequential functions from positive data. In Alexander Clark, Makoto Kanazawa, and Ryo Yoshinaka, editors, Proceedings of the Twelfth Inter- national Conference on Grammatical Inference (ICGI 2014), volume 34, pages 94108. JMLR: Workshop and Conference Proceedings, September. Adam Jardine. 2014. Computationally, tone is different. Under review with Phonology. C. Douglas Johnson. 1972. Formal Aspects of Phonolog- ical Description. The Hague: Mouton. Ronald Kaplan and Martin Kay. 1994. Regular models of phonological rule systems. Computational Linguis- tics, 20(3):331378. Michael Kenstowicz and Charles Kisseberth. 1979. Gen- erative Phonology. Academic Press, Inc. 124 Huan Luo. 2014. Long-distance consonant harmony and subsequantiality. Qualifying paper for the University of Delawares Linguistics PhD Progam. Robert McNaughton and Seymour Papert. 1971. Counter-Free Automata. MIT Press. Mehryar Mohri. 1997. Finite-state transducers in lan- guage and speech processing. Computational Linguis- tics, 23(2):269311. Andrew Nevins. 2010. Locality in Vowel Harmony. MIT Press. Roland Noske. 1993. A theory of syllabification and seg- mental alternation. Niemeyer, T  ubingen. David Odden. 2014. Introducing Phonology. Cam- bridge University Press, 2nd edition. Jose Oncina and Pedro Garcia. 1991. Inductive learning of subsequential functions. Technical Report DSIC II- 34, University Polit  ecnia de Valencia. Jos  e Oncina and Miguel A. Var` o. 1996. Using do- main information during the learning of a subsequen- tial transducer. Lecture Notes in Artificial Intelligence, pages 313325. Jos  e Oncina, Pedro Garc  a, and Enrique Vidal. 1993. Learning subsequential transducers for pattern recog- nition tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 15:448458, May. Farid M. Onn. 1980. Aspects of Malay Phonology and Morphology: A Generative Approach. Kuala Lumpur: Universiti Kebangsaan Malaysia. Amanda Payne. 2013. Dissimilation as a subsequen- tial process. Qualifying paper for the University of Delawares Linguistics PhD Progam. James Rogers and Geoffrey Pullum. 2011. Aural pattern recognition experiments and the subregular hierarchy. Journal of Logic, Language and Information, 20:329 342. James Rogers, Jeffrey Heinz, Gil Bailey, Matt Edlefsen, Molly Visscher, David Wellcome, and Sean Wibel. 2010. On languages piecewise testable in the strict sense. In Christian Ebert, Gerhard J  ager, and Jens Michaelis, editors, The Mathematics of Language, vol- ume 6149 of Lecture Notes in Artifical Intelligence, pages 255265. Springer. James Rogers, Jeffrey Heinz, Margaret Fero, Jeremy Hurst, Dakotah Lambert, and Sean Wibel. 2013. Cog- nitive and sub-regular complexity. In Glyn Morrill and Mark-Jan Nederhof, editors, Formal Grammar, volume 8036 of Lecture Notes in Computer Science, pages 90108. Springer. Sharon Rose and Rachel Walker. 2004. A typology of consonant agreement as correspondence. Language, 80:475531. Jaques Sakarovitch. 2009. Elements of Automata The- ory. Cambridge University Press. Translated by Reuben Thomas from the 2003 edition published by Vuibert, Paris. Keiichiro Suzuki. 1998. A Typological Investigation of Dissimilation. Ph.D. thesis, University of Arizona. Wolfgang Thomas. 1997. Languages, automata, and logic. volume 3, chapter 7. Springer. Rachel Walker. 2011. Vowel Patterns in Language. Cambridge: Cambridge University Press. Rachel Walker. 2014. Nonlocal trigger-target relations. Linguistic Inquiry, 45(3):501523. 125 </chunk></section></sec_map>