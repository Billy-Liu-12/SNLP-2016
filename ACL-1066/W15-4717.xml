<sec_map><section><chunk>Proceedings of the 15th European Workshop on Natural Language Generation (ENLG), pages 100104, Brighton, September 2015. c 2015 Association for Computational Linguistics Generating Descriptions of Spatial Relations between Objects in Images Adrian Muscat Communications &amp; Computer Engineering University of Malta Msida MSD 2080, Malta adrian.muscat@um.edu.mt Anja Belz Computing, Engineering &amp; Maths University of Brighton Lewes Road, Brighton BN2 4GJ, UK a.s.belz@brighton.ac.uk Abstract We investigate the task of predicting prepositions that can be used to describe the spatial relationships between pairs of objects depicted in images. We explore the extent to which such spatial preposi- tions can be predicted from (a) language information, (b) visual information, and (c) combinations of the two. In this paper we describe the dataset of object pairs and prepositions we have created, and report first results for predicting prepositions for object pairs, using a Naive Bayes frame- work. The features we use include object class labels and geometrical features com- puted from object bounding boxes. We evaluate the results in terms of accuracy against human-selected prepositions. </chunk></section><section><heading>1 Introduction </heading><chunk>The task we investigate is predicting the preposi- tions that can be used to describe the spatial rela- tionships between pairs of objects in images. This is not the same as inferring the actual 3-D real- world spatial relationships between objects, but has some similarities with that task. This is an important subtask in automatic image description (which is important not just for assistive technol- ogy, but also for applications such as text-based querying of image databases), but it is rarely ad- dressed as a subtask in its own right. If an im- age description method produces spatial preposi- tions it tends to be as a side-effect of the over- all method (Mitchell et al., 2012; Kulkarni et al., 2013), or else relationships are not between ob- jects, but e.g. between objects and the scene (Yang et al., 2011). An example of preposition selection as a separate sub-task is Elliott &amp; Keller (2013) where the mapping is hard-wired manually. Our main data source is a corpus of images (Ev- eringham et al., 2010) in which objects have been annotated with rectangular bounding boxes and object class labels. For a subset of 1,000 of the images we also have five human-created descrip- tions of the whole image (Rashtchian et al., 2010). We collected additional annotations for the im- ages (Section 2.3) which list, for each object pair, a set of prepositions that have been selected by hu- man annotators as correctly describing the spatial relationship between the given object pair. The aim is to create models for the mapping from image, bounding boxes and labels to spatial prepositions as indicated in Figure 1. In this we use a range of features to represent object pairs, computed from image, bounding boxes and labels. We investigate the predictive power of different types of features within a Naive Bayes framework (Section 3), and report first results in terms of two measures of accuracy (Section 4). </chunk></section><section><heading>2 Data </heading></section><section><heading>2.1 VOC08 </heading><chunk>The PASCAL VOC 2008 Shared Task Competi- tion (VOC08) data consists of 8,776 images and 20,739 objects in 20 object classes (Everingham et al., 2010). In each image, every object belonging to one of the 20 VOC08 object classes is anno- tated with its object class label and a bounding box (among other annotations): 1. class: one of: aeroplane, bird, bicycle, boat, bottle, bus, car, cat, chair, cow, dining table, dog, horse, motorbike, person, potted plant, sheep, sofa, train, tv/monitor. 2. bounding box: an axis-aligned bounding box surrounding the extent of the object visible in the image. </chunk></section><section><heading>2.2 VOC08 1K </heading><chunk>Using Mechanical Turk, Rashtchian et al. (2010) collected five descriptions each for 1,000 VOC08 images selected randomly but ensuring there were 100 beside(person(Obj 1 ), person(Obj 2 )); beside(person(Obj 2 ), dog(Obj 3 )); in front of(dog(Obj 3 ), person(Obj 1 )) Figure 1: Image from PASCAL VOC 2008 with annotations, and prepositions representing spatial rela- tionships (objects numbered in descending order of size of area of bounding box). 50 images in each of the 20 VOC08 object classes. Turkers had to have high hit rates and pass a language competence test before creating descriptions, leading to relatively high quality. We obtained a set of candidate prepositions from the VOC08 1K dataset as follows. We parsed the 5,000 descriptions with the Stanford Parser version 3.5.2 1 with the PCFG model, ex- tracted the nmod:prep prepositional modifier rela- tions, and manually removed the non-spatial ones. This gave us the following set of 38 prepositions: V = { about, above, across, against, along, alongside, around, at, atop, be- hind, below, beneath, beside, beyond, by, close to, far from, in, in front of, inside, inside of, near, next to, on, on top of, opposite, outside, outside of, over, past, through, toward, towards, under, underneath, up, upon, within } </chunk></section><section><heading>2.3 Human-Selected Spatial Prepositions </heading><chunk>We are in the process of extending the VOC08 an- notations with human-selected spatial prepositions associated with pairs of objects in images. So far we have collected spatial prepositions for object pairs in images that have exactly two objects an- notated (1,020). Annotators were presented with images from the dataset where in each image pre- sentation the two objects, Obj 1 and Obj 2 , were shown with their bounding boxes and labels. If there was more than one object of the same class, then the labels were shown with subscript indices (where objects are numbered in order of decreas- ing size of area of bounding box). Next to the image was shown the template sen- tence The Obj 1 is the Obj 2 , and the list of possible prepositions extracted from VOC 1K (see 1 http://nlp.stanford.edu/software/lex-parser.shtml preceding section). The option NONE was also available in case none of the prepositions was suit- able (participants were discouraged from using it). Each template sentence was presented twice, with the objects once in each order, The Obj 1 is the Obj 2 and The Obj 2 is the Obj 1 . 2 Par- ticipants were asked to select all correct preposi- tions for each pair. The following table shows occurrence counts for the 10 most frequent object labels: person dog car chair horse cat bird bicycle motorbike tv/monitor 783 123 112 92 92 88 86 79 77 63 Some prepositions were selected far more fre- quently than others; the top nine are: next to beside near close to in front of behind on on top of underneath 304 211 156 149 141 129 115 103 90 3 Predicting Prepositions When looking at a 2-D image, people infer all kinds of information not present in the pixel grid on the basis of their practice mapping 2-D infor- mation to 3-D spaces, and their real-world knowl- edge about the properties of different types of ob- jects. In our research we are interested in the ex- tent to which prepositions can be predicted with- out any real-world knowledge, using just features that can be computed from the objects bounding boxes and labels. In this section we explore the predictive power of language and visual features within a Naive Bayes framework: 2 Showing objects in both orders is necessary to capture non-reflexive prepositions such as under, in, on etc. 101 P (v j |F) P (v j )P (F|v j ) (1) where v j V are the possible prepositions, and F is the feature vector. Below we look at the predic- tive power of the prior model and the likelihood model as well as the complete model. 3.1 Prior Model The prior model captures the probabilities of prepositions given ordered pairs of object labels L s , L o , where the normalised probabilities are ob- tained through a frequency count on the training set, using add-one smoothing. We then simply construe the model as a classifier to give us the most likely preposition v OL : v OL = argmax v V P (v j |L s , L o ) (2) where v j is a preposition in the set of prepositions V, and L s and L o are the object class labels of the first and second objects. </chunk></section><section><heading>3.2 Likelihood Model </heading><chunk>The likelihood model is based on a set of six geo- metric features computed from the image size and bounding boxes: F 1 : Area of Obj 1 (Bounding Box 1) normal- ized by Image size. F 2 : Area of Obj 2 (Bounding Box 2) normal- ized by Image Size. F 3 : Ratio of area of Obj 1 to area of Obj 2 . F 4 : Distance between bounding box centroids normalized by object sizes. F 5 : Area of overlap of bounding boxes normal- ized by the smaller bounding box. F 6 : Position of Obj 1 relative to Obj 2 . F 1 to F 5 are real valued features, whereas F 6 is a categorical variable over four values (N, S, E, W). For each preposition, the probability distributions for each feature is estimated from the training set. The distributions for F 1 to F 4 are modelled with a Gaussian function, F 5 with a clipped polynomial function, and F 6 with a discrete distribution. The maximum likelihood model, which can also be de- rived from the naive Bayes model described in the next section by choosing a uniform P (v) function, is given by: v M L = argmax v V 6 i=1 P (F i |v j ) (3) </chunk></section><section><heading>3.3 Naive Bayes Model </heading><chunk>The naive Bayes classifier is derived from the maximum-a-posteriori Bayesian model, with the assumption that the features are conditionally in- dependent. A direct application of Bayes rule gives the classifier based on the posterior proba- bility distribution as follows: v N B = argmax v V P (v j |F 1 , ...F 6 , L s , L o ) = argmax v V P (v j |L s , L o ) 6 i=1 P (F i |v j ) (4) Intuitively, P (v j |L s , L o ) weights the likelihood with the prior or state of nature probabilities. 4 Results The current data set comprises 1,000 images, each labelled with one or more prepositions. The aver- age prepositions per image over the whole dataset is 2.01. For training purposes, we create a separate training instance (Obj s , Obj o , v) for each prepo- sition v selected by our human annotators for the given object pair Obj s , Obj o . The models are evaluated with leave-one-out cross-validation, and two methods (Acc A and Acc B ) of calculating accuracy (the percentage of instances for which a correct output is returned). The notation e.g. Acc A (1..n) is used to indicate that in this version of the evaluation method at least one of the top n most likely outputs (preposi- tions) returned by the model needs to match the (set of) human-selected reference preposition(s) for the model output to count as correct. </chunk></section><section><heading>4.1 Accuracy method A </heading><chunk>Acc A (1..n) returns the proportion of times that at least one of the top n prepositions returned by a model for an ordered object pair is in the complete set of human-selected prepositions for the same object pair. Acc A can be seen as a system-level Precision measure. The table below shows Acc A (1) and Acc A (1..2) results for the three models: Model Acc A (1) Acc Syn A (1) Acc A (1..2) v OL 34.4% 43.9% 46.1% v M L 30.9% 35.6% 46.2% v N B 51.0% 57.2% 64.5% 102 Table 1: Acc B (1..n) for v N B model and n 4. Preposition n = 1 n = 2 n = 3 n = 4 next to 23.0% 77.0% 89.8% 93.1% beside 58.3% 81.5% 85.8% 91.9% near 43.6% 55.1% 74.4% 82.7% close to 4.7% 14.8% 51.7% 87.9% in front of 29.1% 39.7% 48.2% 52.5% behind 31.0% 38.0% 50.4% 73.6% on 72.2% 83.5% 85.2% 86.1% on top of 10.7% 76.7% 81.6% 82.5% underneath 53.3% 68.9% 84.4% 86.7% beneath 15.5% 73.8% 79.8% 85.7% far from 44.6% 62.2% 66.2% 68.9% under 22.1% 27.9% 82.4% 83.8% NONE 34.4% 53.1% 67.2% 73.4% Mean 34.0% 57.9% 72.8% 80.7% Mean Acc Syn B 50.9% 66.4% 77.9% 83.1% In addition, the middle column above shows Acc A (1) results when sets of synonymous prepo- sitions are considered identical. The synonym sets we chose for this purpose are: {above, over}, {along, alongside}, {atop, upon, on, on top of}, {below, beneath}, {beside, by, next to}, {beyond, past}, {close to, near}, {in, inside, inside of, within} {outside, outside of}, {toward, towards}, {under, underneath}. 4.2 Accuracy method B Acc B (1..n) computes the mean of preposition- level accuracies. Accuracy for each preposition v is the proportion of times that v is returned as one of the top n prepositions out of those cases when v is in the human-selected set of reference prepo- sitions. Acc B can be seen as a preposition-level Recall measure. Table 1 lists the Acc B (1..n) values for the v N B model for each n up to 4; values are shown for the 13 most frequent prepositions (in order of fre- quency) and for the mean of all preposition-level accuracies. The last row shows the means for a version of Acc B that takes synonyms into account as described in the last section. </chunk></section><section><heading>5 Discussion </heading><chunk>Looking at the naive Bayes results in Table 1, ac- curacy for some prepositions (e.g. close to) im- proves dramatically from Acc B (1) to Acc B (1..4). This implies that where the target preposition is not ranked first, it is often ranked second, third or fourth. There are synonym effects at work as shown by the Acc Syn results; but there also is competition between prepositions that are not near synonyms, as shown by the fact that Acc A (1..2) results are better than Acc Syn A (1) results. For some prepositions, accuracy remains low even at n=4. This may reflect the general issue that human annotators use two different perspec- tives in selecting prepositions: (i) that of a viewer looking at the image, and (ii) that of one or both of the objects involved in the spatial relationship being described. Regarding (i), e.g. in the image in Figure 1, the dog is in front of the person be- cause it is between the viewer and the person. Re- garding (ii), in other examples, a person can be in front of a monitor, or one chair opposite an- other, even when the viewer sees them both from the side. The naive Bayes framework we have investi- gated here is a simple approach which is likely to be outperformed by more sophisticated ML methods. E.g. in calculating the likelihood term P (F |v), our approach assumes the features to be independent; feature weighting per preposition was not carried out; and the data set is small rela- tive to what we are using it for. </chunk></section><section><heading>6 Conclusion </heading><chunk>We have described (i) a dataset we are devel- oping in which object pairs are annotated with prepositions that describe their spatial relation- ship, and (ii) methods for automatically predict- ing such prepositions on the basis of features com- puted from image and object geometry and ob- ject class labels. We have found that on the ba- sis of language information (object class labels) alone we can predict prepositions with 34.4% ac- curacy, rising to 43.9% if we count near synonyms as correct. Using both language and visual infor- mation we can predict prepositions with 51% ac- curacy, rising to 57.2% with near synonyms. We have also found that where the target preposition is not ranked top, it is often ranked very near the top, as can be seen from the Acc B results. The next step in this research will be to increase our dataset and to apply machine learning meth- ods such as support vector machines and neural networks to our learning task. </chunk></section><section><heading>References </heading><chunk>Desmond Elliott and Frank Keller. 2013. Image de- scription using visual dependency representations. In EMNLP13, pages 12921302. 103 Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. 2010. The pascal visual object classes (voc) chal- lenge. International journal of computer vision, 88(2):303338. Gaurav Kulkarni, Visruth Premraj, Vicente Ordonez, Sudipta Dhar, Siming Li, Yejin Choi, Alexander C Berg, and Tamara Berg. 2013. Babytalk: Under- standing and generating simple image descriptions. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 35(12):28912903. Margaret Mitchell, Xufeng Han, Jesse Dodge, Alyssa Mensch, Amit Goyal, Alex Berg, Kota Yamaguchi, Tamara Berg, Karl Stratos, and Hal Daum Iii. 2012. Midge: Generating image descriptions from com- puter vision detections. In Proceedings of EACL12. Cyrus Rashtchian, Peter Young, Micah Hodosh, and Julia Hockenmaier. 2010. Collecting image annota- tions using amazons mechanical turk. In Proceed- ings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazons Mechan- ical Turk, pages 139147. Association for Computa- tional Linguistics. Yezhou Yang, Ching Lik Teo, Hal Daum  e III, and Yian- nis Aloimonos. 2011. Corpus-guided sentence gen- eration of natural images. In Proceedings of the Conference on Empirical Methods in Natural Lan- guage Processing, pages 444454. Association for Computational Linguistics. 104 </chunk></section></sec_map>