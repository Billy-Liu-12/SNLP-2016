<sec_map><section><chunk>Proceedings of the ACL 2015 Workshop on Novel Computational Approaches to Keyphrase Extraction, pages 1924, Beijing, China, July 30, 2015. c 2015 Association for Computational Linguistics Reducing Over-generation Errors for Automatic Keyphrase Extraction using Integer Linear Programming Florian Boudin LINA - UMR CNRS 6241, Universit  e de Nantes, France florian.boudin@univ-nantes.fr Abstract We introduce a global inference model for keyphrase extraction that reduces over- generation errors by weighting sets of keyphrase candidates according to their component words. Our model can be ap- plied on top of any supervised or unsuper- vised word weighting function. Experi- mental results show a substantial improve- ment over commonly used word-based ranking approaches. </chunk></section><section><heading>1 Introduction </heading><chunk>Keyphrases are words or phrases that capture the main topics discussed in a document. Auto- matically extracted keyphrases have been found to be useful for many natural language pro- cessing and information retrieval tasks, such as summarization (Litvak and Last, 2008), opin- ion mining (Berend, 2011) or text categoriza- tion (Hulth and Megyesi, 2006). Despite consid- erable research effort, the automatic extraction of keyphrases that match those of human experts re- mains challenging (Kim et al., 2010). Recent work has shown that most errors made by state-of-the-art keyphrase extraction systems are due to over-generation (Hasan and Ng, 2014). Over-generation errors occur when a system cor- rectly outputs a keyphrase because it contains an important word, but at the same time erroneously predicts other keyphrase candidates as keyphrases because they contain the same word. One reason these errors are frequent is that many unsupervised systems rank candidates according to the weights of their component words, e.g. (Wan and Xiao, 2008a; Liu et al., 2009), and many supervised sys- tems use unigrams as features, e.g. (Turney, 2000; Nguyen and Luong, 2010). While weighting words instead of phrases may seem rather blunt, it offers several advantages. In practice, words are usually much easier to extract, match and weight, especially for short documents where many phrases may not be statistically fre- quent (Liu et al., 2011). Selecting keyphrase candidates according to their component words may also turn out to be useful for reducing over-generation errors if one can ensure that the importance of each word is counted only once in the set of extracted keyphrases. To do so, keyphrases should be ex- tracted as a set rather than independently. Finding the optimal set of keyphrases is a combinatorial optimisation problem, and can be formulated as an integer linear program (ILP) which can be solved exactly using off-the-shelf solvers. In this work, we propose an ILP formulation for keyphrase extraction that can be applied on top of any word weighting scheme. Through experi- ments carried out on the SemEval dataset (Kim et al., 2010), we show that our model increases the performance of both supervised and unsupervised word weighting keyphrase extraction methods. The rest of this paper is organized as follows. In Section 2, we describe our ILP model for keyphrase extraction. Our experiments are pre- sented in Section 3. In Section 4, we briefly review the previous work, and we conclude in Section 5. </chunk></section><section><heading>2 Method </heading><chunk>Our global inference model for keyphrase extrac- tion consists of three steps. First, keyphrase can- didates are extracted from the document using heuristic rules. Second, words are weighted using either supervised or unsupervised methods. Third, finding the optimal subset of keyphrase candidates is cast as an ILP and solved using an off-the-shelf solver. </chunk></section><section><heading>2.1 Keyphrase candidate selection </heading><chunk>Candidate selection is the task of identifying the words or phrases that have properties similar to 19 those of manually assigned keyphrases. First, we apply the following pre-processing steps to the document: sentence segmentation 1 , word to- kenization 2 and Part-Of-Speech (POS) tagging 3 . Following previous work (Wan and Xiao, 2008a; Bougouin et al., 2013), we use the se- quences of nouns and adjectives as keyphrase can- didates. Candidates that have less than three char- acters, that contain only adjectives, or that contain stop-words 4 are filtered out. These heuristic rules are designed to avoid spurious instances and keep the number of candidates to a minimum (Hasan and Ng, 2014). All words are stemmed using Porters stemmer (Porter, 1980). </chunk></section><section><heading>2.2 Word weighting functions </heading><chunk>The performance of our model depends on how word weights are estimated. Here, we ex- periment with three methods for assigning im- portance weights to words. The first two are unsupervised weighting functions, namely TFIDF (Sp  arck Jones, 1972) and TextRank (Mi- halcea and Tarau, 2004), which have been exten- sively used in prior work (Hasan and Ng, 2010). We also apply a supervised model for predicting word importance based on (Hong and Nenkova, 2014). 2.2.1 TFIDF The weight of each word t is estimated using its frequency tf (t, d) in the document d and how many other documents include t (inverse docu- ment frequency), and is defined as: TF IDF(t, d) = tf (t, d) log(D/D t ) where D is the total number of documents and D t is the number of documents containing t. 2.2.2 TextRank A co-occurrence graph is first built from the doc- ument in which nodes are words and edges repre- sent the number of times two words co-occur in the same sentence. TextRank (Mihalcea and Ta- rau, 2004), a graph-based ranking algorithm, is then used to compute the importance weight of each word. Let d be a damping factor 5 , the Tex- tRank score S(V i ) of a node V i is initialized to a 1 We use Punkt Sentence Tokenizer from NLTK. 2 We use Penn Treebank Tokenizer from NLTK. 3 We use the Stanford Part-Of-Speech Tagger (Toutanova et al., 2003). 4 We use the english stop-list from NLTK. 5 We set d to 0.85 as in (Mihalcea and Tarau, 2004). default value and computed iteratively until con- vergence using the following equation: S(V i ) = (1 d) + d V j N (V i ) w ji S(V j ) V k N (V j ) w jk where N (V i ) is the set of nodes connected to V i and w ji is the weight of the edge between nodes V j and V i . TextRank implements the concept of voting, i.e. a word is important if it is highly connected to other words and if it is connected to important words. 2.2.3 Logistic regression We train a logistic regression model 6 for assign- ing importance weights to words in the document based on (Hong and Nenkova, 2014). Reference keyphrases in the training data are used to gener- ate positive and negative examples. For a word in the document (restricted to adjectives and nouns), we assign label 1 if the word appears in the corre- sponding reference keyphrases, otherwise we as- sign 0. We use the relative position of the first oc- currence, the presence in the first sentence and the TFIDF weight as features. These features have been extensively used in supervised keyphrase ex- traction approaches, and have been shown to per- form consistently well (Hasan and Ng, 2014). </chunk></section><section><heading>2.3 ILP model definition </heading><chunk>Our model is an adaptation of the concept- based ILP model for summarization introduced by (Gillick and Favre, 2009), in which sentence se- lection is cast as an instance of the budgeted max- imum coverage problem 7 . The key assumption of our model is that the value of a set of keyphrase candidates is defined as the sum of the weights of the unique words it contains. That way, a set of candidates only benefits from including each word once. Words are thus assumed to be independent, that is, the value of including a word is not affected by the presence of any other word in the set of keyphrases. Formally, let w i be the weight of word i, x i and c j two binary variables indicating the pres- </chunk></section><section><chunk>6 We use the Logistic Regression classifier from scikit- learn with default parameters.  7 Given a collection S of sets with associated costs and a budget L, find a subset S S such that the total cost of sets in S does not exceed L, and the total weight of elements covered by S is maximized (Khuller et al., 1999). 20 ence of word i and candidate j in the set of ex- tracted keyphrases, Occ ij an indicator of the oc- currence of word i in candidate j and N the max- imum number of extracted keyphrases, our model is described as: max i w i x i (1) s.t. j c j N (2) c j Occ ij x i , i, j (3) j c j Occ ij x i , i (4) x i {0, 1} i c j {0, 1} j The constraints formalized in equations 3 and 4 ensure the consistency of the solution: selecting a candidate leads to the selection of all the words it contains, and selecting a word is only possible if it is present in at least one selected candidate. By summing over word weights, this model overly favors long candidates. Indeed, given two keyphrase candidates, one being included in the other (e.g. uddi registries and multiple uddi reg- istries), this model always selects the longest one as its contribution to the objective function is larger. To correct this bias, a regularization term is added to the objective function: max i w i x i j (l j 1)c j 1 + substr j (5) where l j is the size, in words, of candidate j, and substr j the number of times c j occurs as a subtring in the other candidates. This regulariza- tion penalizes the candidates that are composed of more than two words, and is dampened for can- didates that occur frequently as substrings in other candidates. Here, we assume that for multiple can- didates of the same size, the one that is less fre- quent in the document should be stressed first. The resulting ILP is then solved exactly using an off-the-shelf solver 8 . The solving process takes less than a second per document on average. The N candidate keyphrases returned by the solver are selected as keyphrases. 8 We use GLPK, http://www.gnu.org/ software/glpk/ 3 Experiments </chunk></section><section><heading>3.1 Experimental settings </heading><chunk>We carry out our experiments on the SemEval dataset (Kim et al., 2010), which is composed of scientific articles collected from the ACM Digital Library. The dataset is divided into training (144 documents) and test (100 documents) sets. We use the set of combined author- and reader-assigned keyphrases as reference keyphrases. We follow the common practice (Kim et al., 2010) and evaluate the performance of our method in terms of precision (P), recall (R) and f-measure (F) at the top N keyphrases 9 . Extracted and refer- ence keyphrases are stemmed to reduce the num- ber of mismatches. For each word weighting function, namely TFIDF, TextRank and Logistic regression, we compare the performance of our ILP model (here- after ilp) with that of two word-based weighting baselines. The first baseline (hereafter sum) sim- ply ranks keyphrase candidates according to the sum of the weights of their component words as in (Wan and Xiao, 2008b; Wan and Xiao, 2008a). The second baseline (hereafter norm) consists in scoring keyphrase candidates by computing the sum of the weights of their component words nor- malized by their length as in (Boudin, 2013). As a post-processing step, we remove redundant keyphrases from the ranked lists generated by both baselines. A keyphrase is considered redundant if it is included in another keyphrase that is ranked higher in the list. IDF weights are computed on the training set. The regularization parameter is set, for all the experiments, to the value that achieves the best performance on the training set, that is 0.3 for TFIDF, 0.4 for TextRank and 1.2 for Logistic re- gression. </chunk></section><section><heading>3.2 Results </heading><chunk>The performance of our model on top of differ- ent word weighting functions is shown in Table 1. Overall, our model consistently improves the per- formance over the baselines. We observe that the results for sum are very low. Summing the word weights favors long candidates and is prone to over-generation errors, as illustrated by the exam- ple in Table 2. </chunk></section><section><chunk>9 Scores are computed using the evaluation script provided by the SemEval organizers.  21 Top-5 candidates Top-10 candidates Weighting + Ranking P R F P R F TFIDF + sum 5.6 1.9 2.8 5.3 3.5 4.2 + norm 19.2 6.7 9.9 15.1 10.6 12.3 + ilp 25.4 9.1 13.3 17.5 12.4 14.4 TextRank + sum 4.5 1.6 2.3 4.0 2.8 3.3 + norm 18.8 6.6 9.6 14.5 10.1 11.8 + ilp 22.6 8.0 11.7 17.4 12.2 14.2 Logistic regression + sum 4.2 1.5 2.2 4.7 3.4 3.9 + norm 23.8 8.3 12.2 18.9 13.3 15.5 + ilp 29.4 10.4 15.3 19.8 14.1 16.3 Table 1: Comparison of TFIDF, TextRank and Logistic regression for different ranking strategies when extracting a maximum of 5 and 10 keyphrases. Results are expressed as a percentage of precision (P), recall (R) and f-measure (F). indicates significance at the 0.05 level using Students t-test. Normalizing the candidate scores by their lengths (norm) produces shorter candidates but does not limit the number of over-generation er- rors. As we can see from the example in Table 2, 9 out of 10 extracted keyphrases are containing the word nugget. Our ILP model removes these redun- dant keyphrases by controlling the impact of each word on the set of extracted keyphrases. The re- sulting set of keyphrases is more diverse and thus increases the coverage of the topics addressed in the document. Note that the reported results are not on par with keyphrase extraction systems that use ad- hoc pre-processing, involve structural features and leverage external resources. Rather our goal in this work is to demonstrate a simple and intuitive model for reducing over-generation errors. </chunk></section><section><heading>4 Related Work </heading><chunk>In recent years, keyphrase extraction has attracted considerable attention and many different ap- proaches were proposed. Generally speaking, keyphrase extraction methods can be divided into two main categories: supervised and unsupervised approaches. Supervised approaches treat keyphrase ex- traction as a binary classification task, where each phrase is labeled as keyphrase or non- keyphrase (Witten et al., 1999; Turney, 2000; Kim and Kan, 2009; Lopez and Romary, 2010). Unsupervised approaches usually rank phrases by importance and select the top-ranked ones as keyphrases. Methods for ranking phrases in- TFIDF + sum (P = 0.1) advertis bid; certain advertis budget; key- word bid; convex hull landscap; budget op- tim bid; uniform bid strategi; advertis slot; advertis campaign; ward advertis; searchbas advertis TFIDF + norm (P = 0.2) advertis; advertis bid; keyword; keyword bid; landscap; advertis slot; advertis cam- paign; ward advertis; searchbas advertis; ad- vertis random TFIDF + ilp (P = 0.4) click; advertis; uniform bid; landscap; auc- tion; convex hull; keyword; budget optim; single-bid strategi; queri Table 2: Example of the top-10 extracted keyphrases for the document J-3 of the SemEval dataset. Keyphrases are stemmed and whose that match reference keyphrases are marked bold. clude graph-based ranking (Mihalcea and Tarau, 2004; Wan and Xiao, 2008a; Wan and Xiao, 2008b; Bougouin et al., 2013; Boudin, 2013), topic-based clustering (Liu et al., 2009; Liu et al., 2010; Bougouin et al., 2013), statistical mod- els (Paukkeri and Honkela, 2010; El-Beltagy and Rafea, 2010) and language modeling (Tomokiyo and Hurst, 2003). The work of (Ding et al., 2011) is perhaps the closest to our present work. They proposed an ILP formulation of the keyphrase extraction prob- 22 lem that combines TFIDF and position features in an objective function subject to constraints of coherence and coverage. In their model, coher- ence is measured by Mutual Information and cov- erage is estimated using Latent Dirichlet Alloca- tion (LDA) (Blei et al., 2003). Their work dif- fers from ours in that (1) it is phrased-based and thus does not penalize redundant keyphrases, and (2) it requires estimating a large number of hyper- parameters which makes it difficult to generalize. </chunk></section><section><heading>5 Conclusion and Future Work </heading><chunk>In this paper, we proposed an ILP formulation for keyphrase extraction that reduces over-generation errors by weighting keyphrase candidates as a set rather than independently. In our model, keyphrases are selected according to their compo- nent words, and the weight of each unique word is counted only once. Experiments show a sub- stantial improvement over commonly used word- based ranking approaches using either supervised and unsupervised weighting schemes. In future work, we intend to extend our model to include word relatedness through the use of asso- ciation measures. By doing so, we expect to better differentiate semantically related keyphrase can- didates according to the association strength be- tween their component words. Acknowledgments This work was partially supported by the GOLEM project (grant of CNRS PEPS FaSciDo 2015, http://boudinfl.github.io/GOLEM/). We thank the anonymous reviewers, Adrien Bougouin and Evgeny Gurevsky for their insight- ful comments. References G  abor Berend. 2011. Opinion expression mining by exploiting keyphrase extraction. In Proceedings of 5th International Joint Conference on Natural Lan- guage Processing, pages 11621170, Chiang Mai, Thailand, November. Asian Federation of Natural Language Processing. David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. J. Mach. Learn. Res., 3:9931022, March. Florian Boudin. 2013. A comparison of centrality measures for graph-based keyphrase extraction. In Proceedings of the Sixth International Joint Confer- ence on Natural Language Processing, pages 834 838, Nagoya, Japan, October. Asian Federation of Natural Language Processing. Adrien Bougouin, Florian Boudin, and B  eatrice Daille. 2013. Topicrank: Graph-based topic ranking for keyphrase extraction. In Proceedings of the Sixth In- ternational Joint Conference on Natural Language Processing, pages 543551, Nagoya, Japan, Octo- ber. Asian Federation of Natural Language Process- ing. Zhuoye Ding, Qi Zhang, and Xuanjing Huang. 2011. Keyphrase extraction from online news using binary integer programming. In Proceedings of 5th In- ternational Joint Conference on Natural Language Processing, pages 165173, Chiang Mai, Thailand, November. Asian Federation of Natural Language Processing. Samhaa R. El-Beltagy and Ahmed Rafea. 2010. Kp- miner: Participation in semeval-2. In Proceedings of the 5th International Workshop on Semantic Evalu- ation, pages 190193, Uppsala, Sweden, July. Asso- ciation for Computational Linguistics. Dan Gillick and Benoit Favre. 2009. A scalable global model for summarization. In Proceedings of the Workshop on Integer Linear Programming for Nat- ural Language Processing, pages 1018, Boulder, Colorado, June. Association for Computational Lin- guistics. Kazi Saidul Hasan and Vincent Ng. 2010. Conun- drums in unsupervised keyphrase extraction: Mak- ing sense of the state-of-the-art. In Coling 2010: Posters, pages 365373, Beijing, China, August. Coling 2010 Organizing Committee. Kazi Saidul Hasan and Vincent Ng. 2014. Automatic keyphrase extraction: A survey of the state of the art. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers), pages 12621273, Baltimore, Maryland, June. Association for Computational Lin- guistics. Kai Hong and Ani Nenkova. 2014. Improving the estimation of word importance for news multi- document summarization. In Proceedings of the 14th Conference of the European Chapter of the As- sociation for Computational Linguistics, pages 712 721, Gothenburg, Sweden, April. Association for Computational Linguistics. Anette Hulth and Be  ata B. Megyesi. 2006. A study on automatically extracted keywords in text categoriza- tion. In Proceedings of the 21st International Con- ference on Computational Linguistics and 44th An- nual Meeting of the Association for Computational Linguistics, pages 537544, Sydney, Australia, July. Association for Computational Linguistics. 23 Samir Khuller, Anna Moss, and Joseph (Seffi) Naor. 1999. The budgeted maximum coverage problem. Information Processing Letters, 70(1):39 45. Su Nam Kim and Min-Yen Kan. 2009. Re-examining automatic keyphrase extraction approaches in scien- tific articles. In Proceedings of the Workshop on Multiword Expressions: Identification, Interpreta- tion, Disambiguation and Applications, pages 916, Singapore, August. Association for Computational Linguistics. Su Nam Kim, Olena Medelyan, Min-Yen Kan, and Timothy Baldwin. 2010. Semeval-2010 task 5 : Au- tomatic keyphrase extraction from scientific articles. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 2126, Uppsala, Swe- den, July. Association for Computational Linguis- tics. Marina Litvak and Mark Last. 2008. Graph-based keyword extraction for single-document summariza- tion. In Coling 2008: Proceedings of the work- shop Multi-source Multilingual Information Extrac- tion and Summarization, pages 1724, Manchester, UK, August. Coling 2008 Organizing Committee. Zhiyuan Liu, Peng Li, Yabin Zheng, and Maosong Sun. 2009. Clustering to find exemplar terms for keyphrase extraction. In Proceedings of the 2009 Conference on Empirical Methods in Natural Lan- guage Processing, pages 257266, Singapore, Au- gust. Association for Computational Linguistics. Zhiyuan Liu, Wenyi Huang, Yabin Zheng, and Maosong Sun. 2010. Automatic keyphrase extrac- tion via topic decomposition. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 366376, Cambridge, MA, October. Association for Computational Lin- guistics. Zhiyuan Liu, Xinxiong Chen, Yabin Zheng, and Maosong Sun. 2011. Automatic keyphrase extrac- tion by bridging vocabulary gap. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 135144, Portland, Ore- gon, USA, June. Association for Computational Lin- guistics. Patrice Lopez and Laurent Romary. 2010. Humb: Au- tomatic key term extraction from scientific articles in grobid. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 248251, Uppsala, Sweden, July. Association for Computa- tional Linguistics. Rada Mihalcea and Paul Tarau. 2004. Textrank: Bringing order into texts. In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 404411, Barcelona, Spain, July. Association for Computational Linguistics. Thuy Dung Nguyen and Minh-Thang Luong. 2010. Wingnus: Keyphrase extraction utilizing document logical structure. In Proceedings of the 5th Inter- national Workshop on Semantic Evaluation, pages 166169, Uppsala, Sweden, July. Association for Computational Linguistics. Mari-Sanna Paukkeri and Timo Honkela. 2010. Likey: Unsupervised language-independent keyphrase ex- traction. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 162165, Uppsala, Sweden, July. Association for Computa- tional Linguistics. Martin F Porter. 1980. An algorithm for suffix strip- ping. Program, 14(3):130137. Karen Sp  arck Jones. 1972. A statistical interpretation of term specificity and its application in retrieval. Journal of Documentation, 28:1121. Takashi Tomokiyo and Matthew Hurst. 2003. A lan- guage model approach to keyphrase extraction. In Proceedings of the ACL 2003 Workshop on Multi- word Expressions: Analysis, Acquisition and Treat- ment - Volume 18, MWE 03, pages 3340, Strouds- burg, PA, USA. Association for Computational Lin- guistics. Kristina Toutanova, Dan Klein, Christopher D. Man- ning, and Yoram Singer. 2003. Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technology - Volume 1 (NAACL), pages 173180, Stroudsburg, PA, USA. Association for Computa- tional Linguistics. Peter D Turney. 2000. Learning algorithms for keyphrase extraction. Information Retrieval, 2(4):303336. Xiaojun Wan and Jianguo Xiao. 2008a. Col- labrank: Towards a collaborative approach to single- document keyphrase extraction. In Proceedings of the 22nd International Conference on Compu- tational Linguistics (Coling 2008), pages 969976, Manchester, UK, August. Coling 2008 Organizing Committee. Xiaojun Wan and Jianguo Xiao. 2008b. Single document keyphrase extraction using neighborhood knowledge. In Proceedings of the 23rd National Conference on Artificial Intelligence - Volume 2, AAAI08, pages 855860. AAAI Press. Ian H. Witten, Gordon W. Paynter, Eibe Frank, Carl Gutwin, and Craig G. Nevill-Manning. 1999. Kea: Practical automatic keyphrase extraction. In Pro- ceedings of the Fourth ACM Conference on Digital Libraries, DL 99, pages 254255, New York, NY, USA. ACM. 24 </chunk></section></sec_map>