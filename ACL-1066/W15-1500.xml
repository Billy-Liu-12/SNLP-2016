<sec_map><section><chunk>NAACL HLT 2015 The 1st Workshop on Vector Space Modeling for Natural Language Processing (in NAACL 2015) Proceedings of the Workshop June 5, 2015 Denver, Colorado, USA c 2015 The Association for Computational Linguistics Order print-on-demand copies from: Curran Associates 57 Morehouse Lane Red Hook, New York 12571 USA Tel: +1-845-758-0400 Fax: +1-845-758-2633 curran@proceedings.com ISBN 978-1-941643-46-4 ii Introduction The idea of statistical analysis of language is an old idea, but modern NLP started with a focus on methods based on pure symbolic analysis of language. Statistical methods were introduced to NLP in its current form in the 1980s/1990s, allowing soft reasoning about language, and made NLP more data-driven. Over the last decade another step has been taken in this direction it was proposed to represent and analyze language in vector spaces. Now-a-days, context, symbolic and high-dimensional representations are often augmented with relatively low-dimensional vector-space representations. Vector space representations have been successfully used in different areas of NLP such as syntax and semantics. This workshop is an opportunity to explore state of the art in the use of vector spaces in order to computationally analyze natural language. The focus of the workshop is on the use of vector spaces to learn latent representations. The goal of the workshop is to bring together researchers from areas such as deep learning and representation learning, spectral learning, distributional compositional semantics and others, in order to see their relevance to each other, and learn about the state of the art in these areas. This is the first time that this workshop is held. There were other similar workshops in the past, such as the Workshop on Continuous Vector Space Models and their Compositionality. The program this year includes 27 papers that cover different areas under the realm of vector space modeling in NLP, all of which are presented in two poster sessions. There are also 3 invited speakers, Marco Baroni, Chris Manning and Xavier Carreras, with each of their talks covering a different aspect of vector space modeling in NLP. We would like to thank the Program Committee members who reviewed the papers this year. We would also like to thank the workshop participants. Last, a word of thanks also goes to our two sponsors: Google Deepmind and Textkernel. Phil Blunsom, Shay Cohen, Paramveer Dhillon and Percy Liang Co-Organizers iii Organizers: Phil Blunsom (University of Oxford) Shay Cohen (University of Edinburgh) Paramveer Dhillon (Massachusetts Institute of Technology) Percy Liang (Stanford University) Program Committee: Waleed Ammar (Carnegie Mellon University) Apoorv Agarwal (Columbia University) Borja Balle (McGill University) William Blacoe (University of Edinburgh) Jordan Boyd-Graber (University of Colorado) Xavier Carreras (XRCE) Tejaswini Deoskar (University of Edinburgh) Weisi Duan (Carnegie Mellon University) Edward Grefenstette (University of Oxford) Manaal Frauqui (Carnegie Mellon University) Lea Frermann (University of Edinburgh) Karl Moritz Hermann (University of Oxford) Thang Minh Luong (Stanford University) Annie Louis (University of Edinburgh) Jeff Mitchell (University of Edinburgh) Ankur Parikh (Carnegie Mellon University) Rohan Ramanath (Carnegie Mellon University) Siva Reddy (University of Edinburgh) Roi Reichart (Technion) Sebastian Riedel (University College London) Richard Socher (Stanford University) Karl Stratos (Columbia University) Lyle Ungar (University of Pennsylvania) Eva Maria Vecchi (University of Cambridge) Invited Speakers: Marco Baroni (University of Trento) Xavier Carreras (XRCE) Chris Manning (Stanford University) v Sponsors: Google DeepMind Textkernel (http://www.textkernel.com: machine learning for matching people and jobs) vi Table of Contents A Simple Word Embedding Model for Lexical Substitution Oren Melamud, Omer Levy and Ido Dagan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 Unsupervised Text Normalization Using Distributed Representations of Words and Phrases Vivek Kumar Rangarajan Sridhar . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 A Multi-classifier Approach to support Coreference Resolution in a Vector Space Model Ana Zelaia, Olatz Arregi and Basilio Sierra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 Neural context embeddings for automatic discovery of word senses Mikael Kageback, Fredrik Johansson, Richard Johansson and Devdatt Dubhashi . . . . . . . . . . . . . 25 Distributional Representations of Words for Short Text Classification Chenglong Ma, Weiqun Xu, Peijia Li and Yonghong Yan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 Relation Extraction: Perspective from Convolutional Neural Networks Thien Huu Nguyen and Ralph Grishman . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 Distributional Semantic Concept Models for Entity Relation Discovery Jay Urbain, Glenn Bushee and George Kowalski . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 A Deep Architecture for Non-Projective Dependency Parsing Erick Fonseca and Sandra Aluisio . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 Short Text Clustering via Convolutional Neural Networks jiaming xu, peng wang, guanhua tian, bo xu, jun zhao, fangyuan wang and hongwei hao . . . . . . 62 A Word-Embedding-based Sense Index for Regular Polysemy Representation Marco Del Tredici and Nuria Bel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70 Simple Semi-Supervised POS Tagging Karl Stratos and Michael Collins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79 Learning Distributed Representations for Multilingual Text Sequences Hieu Pham, Thang Luong and Christopher Manning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88 Combining Distributed Vector Representations for Words Justin Garten, Kenji Sagae, Volkan Ustun and Morteza Dehghani . . . . . . . . . . . . . . . . . . . . . . . . . . . 95 Dependency Link Embeddings: Continuous Representations of Syntactic Substructures Mohit Bansal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102 DeepNL: a Deep Learning NLP pipeline Giuseppe Attardi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109 A Vector Space Approach for Aspect Based Sentiment Analysis Abdulaziz Alghunaim, Mitra Mohtarami, Scott Cyphers and Jim Glass . . . . . . . . . . . . . . . . . . . . 116 vii Word Embeddings vs Word Types for Sequence Labeling: the Curious Case of CV Parsing Melanie Tosik, Carsten Lygteskov Hansen, Gerard Goossen and Mihai Rotaru . . . . . . . . . . . . . . 123 Morpho-syntactic Regularities in Continuous Word Representations: A multilingual study. Garrett Nicolai, Colin Cherry and Grzegorz Kondrak . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129 Towards Combined Matrix and Tensor Factorization for Universal Schema Relation Extraction Sameer Singh, Tim Rocktaschel and Sebastian Riedel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135 Neural word embeddings with multiplicative feature interactions for tensor-based compositions Joo-Kyung Kim, Marie-Catherine de Marneffe and Eric Fosler-Lussier . . . . . . . . . . . . . . . . . . . . 143 Bilingual Word Representations with Monolingual Quality in Mind Thang Luong, Hieu Pham and Christopher D. Manning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151 Distributed Word Representations Improve NER for e-Commerce Mahesh Joshi, Ethan Hart, Mirko Vogel and Jean-David Ruvini . . . . . . . . . . . . . . . . . . . . . . . . . . . 160 Semantic Information Extraction for Improved Word Embeddings Jiaqiang Chen and Gerard de Melo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168 Named Entity Recognition for Arabic Social Media Ayah Zirikly and Mona Diab . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176 Vector Space Models for Scientific Document Summarization John Conroy and Sashka Davis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186 Unsupervised Topic Modeling for Short Texts Using Distributed Representations of Words Vivek Kumar Rangarajan Sridhar . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192 Estimating User Location in Social Media with Stacked Denoising Auto-encoders Ji Liu and Diana Inkpen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201 viii Conference Program 9:009:15 Opening Remarks 9:1510:15 Invited Talk (Chris Manning) 10:1512:15 Poster session 1 A Simple Word Embedding Model for Lexical Substitution Oren Melamud, Omer Levy and Ido Dagan Unsupervised Text Normalization Using Distributed Representations of Words and Phrases Vivek Kumar Rangarajan Sridhar A Multi-classifier Approach to support Coreference Resolution in a Vector Space Model Ana Zelaia, Olatz Arregi and Basilio Sierra Neural context embeddings for automatic discovery of word senses Mikael Kageback, Fredrik Johansson, Richard Johansson and Devdatt Dubhashi Distributional Representations of Words for Short Text Classification Chenglong Ma, Weiqun Xu, Peijia Li and Yonghong Yan Relation Extraction: Perspective from Convolutional Neural Networks Thien Huu Nguyen and Ralph Grishman Distributional Semantic Concept Models for Entity Relation Discovery Jay Urbain, Glenn Bushee and George Kowalski A Deep Architecture for Non-Projective Dependency Parsing Erick Fonseca and Sandra Aluisio Short Text Clustering via Convolutional Neural Networks jiaming xu, peng wang, guanhua tian, bo xu, jun zhao, fangyuan wang and hongwei hao A Word-Embedding-based Sense Index for Regular Polysemy Representation Marco Del Tredici and Nuria Bel ix No Day Set (continued) Simple Semi-Supervised POS Tagging Karl Stratos and Michael Collins Learning Distributed Representations for Multilingual Text Sequences Hieu Pham, Thang Luong and Christopher Manning Combining Distributed Vector Representations for Words Justin Garten, Kenji Sagae, Volkan Ustun and Morteza Dehghani 12:1513:30 Lunch 13:3014:30 Invited Talk (Xavier Carreras) 14:3016:30 Poster session 2 Dependency Link Embeddings: Continuous Representations of Syntactic Substruc- tures Mohit Bansal DeepNL: a Deep Learning NLP pipeline Giuseppe Attardi A Vector Space Approach for Aspect Based Sentiment Analysis Abdulaziz Alghunaim, Mitra Mohtarami, Scott Cyphers and Jim Glass Word Embeddings vs Word Types for Sequence Labeling: the Curious Case of CV Parsing Melanie Tosik, Carsten Lygteskov Hansen, Gerard Goossen and Mihai Rotaru Morpho-syntactic Regularities in Continuous Word Representations: A multilingual study. Garrett Nicolai, Colin Cherry and Grzegorz Kondrak Towards Combined Matrix and Tensor Factorization for Universal Schema Relation Extraction Sameer Singh, Tim Rocktaschel and Sebastian Riedel Neural word embeddings with multiplicative feature interactions for tensor-based compositions Joo-Kyung Kim, Marie-Catherine de Marneffe and Eric Fosler-Lussier x No Day Set (continued) Bilingual Word Representations with Monolingual Quality in Mind Thang Luong, Hieu Pham and Christopher D. Manning Distributed Word Representations Improve NER for e-Commerce Mahesh Joshi, Ethan Hart, Mirko Vogel and Jean-David Ruvini Semantic Information Extraction for Improved Word Embeddings Jiaqiang Chen and Gerard de Melo Named Entity Recognition for Arabic Social Media Ayah Zirikly and Mona Diab Vector Space Models for Scientific Document Summarization John Conroy and Sashka Davis Unsupervised Topic Modeling for Short Texts Using Distributed Representations of Words Vivek Kumar Rangarajan Sridhar Estimating User Location in Social Media with Stacked Denoising Auto-encoders Ji Liu and Diana Inkpen 16:3017:30 Invited Talk (Marco Baroni) 17:3017:45 Concluding Remarks and Prizes 17:4519:00 Farewell Reception xi </chunk></section></sec_map>