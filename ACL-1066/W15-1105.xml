<sec_map><section><chunk>Proceedings of CMCL 2015, pages 3947, Denver, Colorado, June 4, 2015. c 2015 Association for Computational Linguistics Fusion of Compositional Network-based and Lexical Function Distributional Semantic Models Spiros Georgiladakis School of ECE National Technical University of Athens Athens, Greece sgeorgil@central.ntua.gr Elias Iosif School of ECE National Technical University of Athens Athena Research Center Athens, Greece iosife@central.ntua.gr Alexandros Potamianos School of ECE National Technical University of Athens Athena Research Center Athens, Greece potam@central.ntua.gr Abstract Distributional Semantic Models (DSMs) have been successful at modeling the meaning of individual words, with interest recently shift- ing to compositional structures, i.e., phrases and sentences. Network-based DSMs repre- sent and handle semantics via operators ap- plied on word neighborhoods, i.e., seman- tic graphs containing a targets most similar words. We extend network-based DSMs to address compositionality using an activation model (motivated by psycholinguistics) that operates on the fused neighborhoods of vari- able size activation. The proposed method is evaluated against and combined with the lexi- cal function method proposed by (Baroni and Zamparelli, 2010). We show that, by fusing a network-based with a lexical function model, performance gains can be achieved. </chunk></section><section><heading>1 Introduction </heading><chunk>Vector Space Models (VSMs) have proven their ef- ficiency at representing word semantics, which are vital components for numerous natural language ap- plications, such as paraphrasing and textual entail- ment (Androutsopoulos and Malakasiotis, 2010), af- fective text analysis (Malandrakis et al., 2013), etc. VSMs constitute the most-widely used implemen- tation of Distributional Semantic Models (DSMs) (Baroni and Lenci, 2010). A fundamental task ad- dressed in the framework of DSMs is the computa- tion of semantic similarity between words, adopting the distributional hypothesis of meaning, i.e., simi- larity of context implies similarity of meaning (Har- ris, 1954). DSMs have been successful when ap- plied to the representation of word lexical semantics, enabling the computation of word semantic similar- ity (Turney and Pantel, 2010). However, the ap- plication of DSMs for representing the semantics of more complex structures, e.g., phrases or sen- tences, is not trivial since the meaning of such struc- tures is the result of various compositional phenom- ena (Pelletier, 1994) that are inherent properties of natural language creativity. The key idea behind current approaches in semantic composition (using DSMs) is the combination of word vectors using simple functions, e.g., vector addition or multipli- cation (Mitchell and Lapata, 2008; Mitchell and La- pata, 2010), or other transformational functions. Re- gardless of the used function, the resulting represen- tations adhere to the paradigm of VSMs, while the cosine between the (composed) vectors is used for estimating similarity. Such efforts proved to be ef- fective when computing the similarity between two- word phrases, however, their limitations were re- vealed for the case of longer structures (Polajnar et al., 2014), where the composition of meaning be- comes more complex. Bengio and Mikolov (2003; 2013) proposed an approach based on deep learning for building language models that address the prob- 39 lem of language creativity. The models appear to constantly gain support in comparison with the tra- ditional DSMs. A preliminary comparative analysis of them is provided in (Baroni et al., 2014b) with re- spect to a number of tasks related to lexical seman- tics. In this work, we extend a recent network-based implementation of DSMs (Iosif and Potamianos, 2015) in order to represent the semantics of com- positional structures. The used framework consists of activation models motivated by semantic prim- ing (McNamara, 2005). For each structure, an ac- tivation area (i.e, semantic neighborhood) is com- puted which is regarded as a sub-space within the network. The novelty of the present work is two- fold. First, we propose various approaches for the creation of activation areas for compositional structures, within a framework alternative to VSMs. Second, we investigate the fusion of the proposed network-based model with VSM-based transforma- tional approaches from the literature. In addition, we investigate the role of words as operators on the meaning of the structures they occur in by measur- ing their transformative degree. The remainder of this paper is organized as fol- lows: in Section 2 we describe work related to DSMs. In Section 3 we describe the work on which we based the proposed models. We present the pro- posed models in Section 4. The lexical function model is described in Section 5, and a fusion model integrating the former with network-based models is proposed. We describe the experimental procedure that we followed and evaluate the proposed models in Section 6. We elaborate on the effects of modifiers in compositional structures in Section 7, concluding in Section 8. </chunk></section><section><heading>2 Related Work </heading><chunk>Word-level DSMs can be categorized into unstruc- tured, that employ a bag-of-words model, and struc- tured, that employ syntactic relationships between words (Grefenstette, 1994; Baroni and Lenci, 2010). DSMs are typically constructed from co-occurrence statistics of word tuples. An unstructured approach for the construction of network-based DSMs was proposed in (Iosif and Potamianos, 2015), where nodes represent words, and edges are formulated ac- cording to the semantic similarity of the connected nodes. For each node, the notion of semantic neigh- borhood (i.e., the most semantically similar words) is utilized for estimating an improved similarity be- tween the nodes. Moving beyond the word-level, Turney (2012) proposed a dual-space model that combines re- lational and compositional methods for represent- ing phrasal semantics. This approach utilized two complementary models in an attempt to address a series of phenomena that apply to compositional semantics, namely, linguistic creativity, order sensitivity, adaptive capacity, and information scalability 1 . Three types of phrases were investi- gated: noun-noun (NN), adjective-noun (AN), and verb-object (VO). In (Baroni and Zamparelli, 2010), particular focus was given to the AN type, where adjectives were represented as matrices acting as functions to the vectorial representation of head nouns. Recent research efforts have been expanded to longer text segments such as sentences (Agirre et al., 2012; Agirre et al., 2013; Polajnar et al., 2014). In (Socher et al., 2012), based on the functional space proposed in (Baroni and Zamparelli, 2010), phrase constituents were treated as both a continu- ous vector and a parameter matrix, where the repre- sentation of sentence semantics was constructed via a recursive bottom-up procedure. </chunk></section><section><heading>3 Baseline Network-based Model </heading><chunk>In this section, we generalize the ideas regard- ing network-based DSMs presented in (Iosif and Potamianos, 2015), for the case of more complex structures. The network consists of two layers: 1) activation, and 2) similarity layer. Given a lexical unit, the first layer represents an activation area that includes a set of lexical units that are semantically related with it. The notion of lexical unit refers to any semantically coherent lexical structure, span- ning from words (unigrams) up to word sequences (n-grams). The second layer is used for the com- putation of semantic similarity between two lexi- cal units, based on their respective activation layers. The network can be defined as a graph Q = (V, E) whose set of vertices V includes the lexical units un- </chunk></section><section><heading>1 These phenomena are defined and discussed in (Turney, 2012) </heading><chunk>40 der investigation and whose set of edges E contains links between the vertices. The links between the lexical units in the network are weighted according to their pairwise semantic similarity. </chunk></section><section><heading>3.1 Layer 1: Activation Model </heading><chunk>The activation layer of a lexical unit, , can be re- garded as a sub-graph of Q, Q , also referred to as the semantic neighborhood of . Its vertices (neigh- bors of ) are determined according to their seman- tic similarity with . Given a set of lexical units, the most similar to are selected as neighbors. The activation layer is motivated by the phenomenon of semantic priming (McNamara, 2005), especially for highly coherent lexical units, such as unigrams and bigrams. In the framework of DSMs, activation layers were computed for the case of unigrams in (Iosif and Potamianos, 2015), and were extended to short phrases (bigrams) in (Iosif, 2013). Consider a phrase, i = (i 1 i 2 ), where i 1 and i 2 denote its first and second constituent. Assuming that the N i 1 and N i 2 sets represent neighborhoods of i 1 and i 2 , re- spectively, the neighborhood of i, N i , was computed by taking the intersection of N i 1 and N i 2 . </chunk></section><section><heading>3.2 Layer 2: Semantic Model </heading><chunk>Two similarity metrics are defined for computing the similarity between two lexical units, i and j. The metrics are defined on top of their respective acti- vation models, N i and N j , computed in the previ- ous layer. This approach relies on two assumptions, namely, maximum sense and attributional similarity, for unigrams. In this work, we extend these metrics to bigrams (see Fig. 1 and Fig. 2) in order to com- pute the semantic similarity between two phrases, i = (i 1 i 2 ) and j = (j 1 j 2 ), exploiting their respec- tive activation layers N i and N j . Maximum Neighborhood Similarity. The key idea of this metric, M , is the computation of similar- ities between the constituents of phrase i (i 1 and i 2 ) and the members of N j . The same is done for j 1 and j 2 and the members of N i . The sim- ilarity between i and j (e.g., assistant manager and board member in Fig. 1) is computed by tak- ing the maximum of the aforementioned similarities (0.50 in Fig. 1). The underlying hypothesis is that the neighborhoods encode senses that are shared be- tween the constituents. The selection of the maxi- Figure 1: Maximum neighborhood similarity metric (M ): bigram usecase. mum score suggests that the similarity between i and j can be approximated by considering their closest senses (Iosif and Potamianos, 2015). Attributional Neighborhood Similarity. In this metric, R, similarities between i 1 and i 2 and the members of N j are computed and stored into a vec- tor. This is also done for j 1 and j 2 and the members of N j . The correlation coefficient between the two vectors (e.g., the two right-most vectors in Fig. 2) is computed. The process is repeated, using N i in the place of N j , which results into another corre- lation coefficient. The similarity between i and j Figure 2: Attributional neighborhood similarity metric (R): bigram usecase. is estimated by selecting the maximum correlation coefficient. The underlying motivation is attribu- tional similarity, i.e., the hypothesis that the neigh- borhoods encode semantic or affective features. Se- 41 mantically similar phrases are expected to exhibit correlated similarities with respect to such features (Iosif and Potamianos, 2015). 4 Extended Network-based Model The major limitation of the model presented in Sec- tion 3 is that the neighborhoods of phrase con- stituents (e.g., N i 1 and N i 2 ) are of fixed size. This allows the computation of an empty neighborhood for the phrase (e.g., N i ), when there is no overlap between the neighborhoods of its constituents. In this section, we propose an extension of the aforementioned model by relaxing the hard con- straint regarding the fixed size of neighborhoods. The intuition behind this idea is that the activa- tion areas are not of the same size for all words. For example, a semantically abstract word, such as democracy, is expected to have a larger neighbor- hood compared to semantically concrete words, e.g., computer. Given a phrase, e.g., i = (i 1 i 2 ), in or- der to compute the activation N i , we gradually ex- tend the activation areas (i.e., sizes) of N i 1 and N i 2 until a minimum size for N i is reached. </chunk></section><section><heading>4.1 Layer 1: Activation Model </heading><chunk>We propose three different schemes for the com- putation of neighborhoods. An example of those Figure 3: Activation model schemes for the phrase na- tional government: intersection-based, union-based, and selection of most similar neighbors (words in bold). schemes is depicted in Fig. 3. Scheme 1. The phrase neighborhood is computed by taking the intersection of the constituent neighbor- hoods, i.e., N i = N i 1 N i 2 . This adheres to findings from the literature of psycholinguistics suggesting that the phrase activation (and, thus, the respective meaning) should be more specific than those of its constituents (Osherson and Smith, 1981). Scheme 2. The union of neighborhoods is used, i.e., N i = N i 1 N i 2 . This is motivated by the idea that, in some cases, a phrase may be associated with a larger activation area, compared to those of its con- stituents. Scheme 3. The members of the phrase neighbor- hood are selected based on their average semantic similarity with respect to the phrase constituents. Let N i be {n 1 , ..., n m , ..., n }, where n m {N i 1 N i 2 }. The N i set can be regarded as a list, which is ranked according to 1 2 (S(n m , i 1 )+S(n m , i 2 )), where S(.) stands for a metric of semantic similarity. This scheme is motivated by the idea that different areas of N i 1 and N i 2 may be activated given the context of words i 1 and i 2 , respectively. The scheme also ad- dresses the issue of scalability: the phrase neighbor- hood has the same size as the constituents neigh- borhoods, enabling the recursive application of the model over longer structures. </chunk></section><section><heading>4.2 Layer 2: Semantic Model </heading><chunk>An extension of the M metric (described in Sec- tion 3) is proposed, along with two more metrics for computing the semantic similarity between lexical units utilizing their respective neighborhoods. The metrics are defined with respect to two lexical units, i and j, which are represented by their neighbor- hoods, N i and N j , respectively. Average of top-k similarities (M k ). This metric ex- tends the M metric (see Section 3) by considering the top k similarity scores instead of the maximum score. Similarity between i and j, M k (i, j), is com- puted by taking the arithmetic mean of the k scores. Average of top-k pairwise similarities (P k ). Let C be a ranked list including the pairwise similarities computed between the members of N i and N j : C = { S xN i yN j (x, y) }, (1) where S(.) stands for a metric of semantic similarity. The similarity between i and j is computed as: P k (i, j) = 1 k k l=1 c l , (2) where c l is the lth member of C. Hausdorff-based similarity (H). This metric is 42 motivated by the Hausdorff distance (Hung and Yang, 2004). Let h(N i , N j ) be defined as h(N i , N j ) = min xN i max yN j {S(x, y)}}, (3) where S(.) is a semantic similarity metric. The sim- ilarity between i and j is computed as: H(i, j) = max{h(N i , N j ), h(N j , N i )} (4) 5 Fusion of Lexical Function with Network-based Models The representation of phrase semantics requires the consideration of the consituents functional influ- ence on the composed meaning. For example, when considering an adjective-noun phrase, such as bad cat, the former word (bad) acts as an operator, i.e., modifier, to the latter word (cat), modifying its meaning. In (Baroni and Zamparelli, 2010; Ba- roni et al., 2014a), it was proposed that such modifi- cations can be implemented via the use of functions that act as linear transformations in VSMs. Appli- cation of these functions is realized via matrix-by- vector multiplication as (Baroni et al., 2014a): f () = def F a = b, (5) where F is the matrix-encoded function f , a is the vectorial representation of the argument , and b is the compositional vector output. The F function is learnt according to examples of observed input and output (distributional) representations. The input is the representation of the head word, and the output is the representation of the phrase. Regression is employed for calculating the set of weights in the matrix that best approximate the observed vectors. For example, the function for the modifier bad is learnt by regressing over phrase examples and their head nouns, such as &lt;pet, bad pet&gt;, &lt;dog, bad dog&gt;, &lt;bird, bad bird&gt;. Using the trained set of weights and the vectorial representation of the head noun, e.g., cat, the composite representation for the phrase bad cat is induced. </chunk></section><section><heading>5.1 Fusion </heading><chunk>The proposed network-based model, presented in Section 4, exploits the merging of word senses for computing activation areas for phrases. The model defined by (5) utilizes the transformational function of an operator for changing the meaning of a phrase. Both models (intuitively) seem to be aligned with the human process of phrase comprehension, how- ever, there are cases that one of the models applies better than the other. Consider two example phrases, football manager and successful engineer. The transformational model is expected to perform bet- ter for the latter phrase, while for the first phrase an intersection of word senses (i.e., a network-based model) seems to be more appropriate. Based on the above considerations, we propose a fusion of the lexical function (lf ), defined by (5), with the proposed network-based models. The fu- sion is aimed to model more accurately the seman- tic representations of complex structures. To do so, we measure the Mean Squared Error (MSE) when training the lexical function model, in order to quan- tify the transformative degree of the modifier un- der investigation. The transformative degree is used for deciding whether a network-based or a trans- formational model is more appropriate. Given two phrases, i = (i 1 i 2 ) and j = (j 1 j 2 ), the transforma- tive degree T (i, j) is defined as: T (i, j) = 1 2 (M SE(i 1 ) + M SE(j 1 )), (6) where M SE(i 1 ) and M SE(j 1 ) is the MSE that cor- responds to modifiers i 1 and j 1 , respectively. The proposed fusion metric, lf net (i, j), used for estimat- ing the similarity between the i and j phrases, is de- fined as: lf net (i, j) = (i, j) S N + (1 (i, j)) S LF , (7) where S N and S LF are similarity scores computed by the network-based and lexical function models, respectively. is a function of i and j, computed using a sigmoid function as: (i, j) = 0.5/ 1 + e T (i,j) . (8) The sigmoid function is applied in order to smooth and normalize (within [0,1]) the values of T (i, j). Finally, in addition to the aforementioned fusion, we also implement a fusion combining the lf and the widely-used additive (add) (Mitchell and Lap- ata, 2008; Mitchell and Lapata, 2010) model. This fusion metric, lf add , is defined similarly to (7). 43 6 Experiments and Evaluation The procedure for creating the network and conduct- ing the experiments is described in Section 6.1. In Section 6.2, we evaluate the proposed models and compare them with results from the literature. </chunk></section><section><heading>6.1 Experimental Procedure </heading><chunk>We defined our vocabulary (network nodes) by in- tersecting the English vocabulary found in the AS- PELL 2 dictionary and the Wikipedia dump 3 to de- rive an English vocabulary of approximately 135K words. Using it, a corpus comprising of web- harvested document snippets was constructed by downloading 1000 snippets for each word in the vocabulary. Word-level similarities were computed among all vocabulary entries pairs. To this end, the Normalized Google Distance (G) was utilized, pro- posed in (Vitanyi, 2005; Cilibrasi and Vitanyi, 2007) and motivated by Kolmogorov complexity. Let G be defined as G(w 1 , w 2 ) = max{A} log | D | w 1 , w 2 | log | D | min{A} , (9) where w 1 and w 2 are two vocabulary words under investigation, | D | is the total number of docu- ments in the corpus, | D | w 1 , w 2 | is the total num- ber of documents containing both w 1 and w 2 , and A = {log | D | w 1 |, log | D | w 2 |}. We used a variation of (9), proposed in (Gracia et al., 2006), referred to as Google-based Semantic Relatedness (G ). This variation defines a similarity measure, bounded within the [0, 1] range and defined as G (w 1 , w 2 ) = e 2G(w 1 ,w 2 ) , (10) where G(w 1 , w 2 ) is computed according to (9). In this work, D denotes the sentence rather than the document, as the co-occurrence of words was de- fined at sentence-level. This metric was adopted based on its good performance in word-level seman- tic similarity tasks (Iosif and Potamianos, 2015). Network-based model. We used sizes of = {10, 25, 50, 100, 150, 500} for the case of fixed-size neighborhoods, and = {1, 5, ..., 40} for the ex- tended activation models described in Section 4.1. 2 http://www.aspell.net/ 3 As of the 4th quarter of 2012. We used both the baseline and the extended activa- tion layers for the M model, the latter being defined as M . For M k and P k , we set k = {1, ..., 5}. Transformational model. For the lf model de- scribed in (5), we computed co-occurence counts for bigrams occurring at least 50 times in the corpus. Positive Pointwise Mutual Information (PPMI) was applied to reweigh them. We used a) Singular Value Decomposition (SVD), and b) Non-Negative Matrix Factorization (NMF) (Lee and Seung, 2001) to re- duce the dimensionality of the space down to a) 300, and b) 500 dimensions. To train lf, we selected cor- pus bigrams comprising of a modifier and a noun. We used a) Least Squares (LSR), and b) Ridge (RR) (Hastie et al., 2009) regression. The DIStribu- tional SEmantics Composition Toolkit (DISSECT 4 , (Dinu et al., 2013)) was used to implement lf, as well as the widely-used additive (add) and multi- plicative (mult) models proposed in (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010). Fusion model. We combined the best performing model configurations on NNs (see Section 6.2) in order to implement the proposed fusion models. </chunk></section><section><heading>6.2 Evaluation Results </heading><chunk>For evaluation purposes, we used the widely-used Mitchell &amp; Lapata (2010) datasets comprising of 108 noun-noun (NN), adjective-noun (AN), and verb-object (VO) phrase pairs, evaluated by human judgements and averaged per phrase pair. The mod- els were evaluated using Spearmans correlation co- efficient. Evaluation results are presented in Ta- ble 1. Due to space limitations, only the best per- forming network-based model configurations are re- ported here. Also, since the mult model performs poorly when the composed vectors contain negative values, as is the case with SVD, we only report re- sults for the NMF variations for it. Finally, since training the lf model with RR had significantly su- perior performance over LSR in all configurations, we only report evaluations of the former. The lf model, when using RR in combination with NMF, performs best (.76) for the case of NNs. Best performances for ANs and VOs are obtained by the add model (.63 and .59, respectively). 4 http://clic.cimec.unitn.it/composes/ toolkit/ 44 Model NN AN VO add (NMF300) .67 .61 .53 add (NMF500) .66 .63 .56 add (SVD300) .63 .59 .59 add (SVD500) .66 .63 .59 mult (NMF300) .59 .38 .36 mult (NMF500) .59 .36 .42 lf (NMF300, RR) .76 .46 .35 lf (NMF500, RR) .67 .41 .28 lf (SVD300, RR) .63 .35 .26 lf (SVD500, RR) .56 .33 .23 M (Intersection) .56 .46 .37 M (Intersection) .61 .57 .47 M k=3 (Intersection) .64 .51 .41 P k=3 (Most-similar) .63 .46 .23 H (Intersection) .58 .39 .26 fusion lf net .80 .54 .35 fusion lf add .76 .57 .44 Table 1: Performance of models on NN, AN, and VO phrase pairs. Evaluations are reported using Spearmans correlation coefficient with human ratings. Regarding network-based models, performance is improved when using the extended activation model over the baseline. This is confirmed by the absolute 5%, 11% and 10% increase for the case of NN, AN, and VO pairs, respectively, for the M metric. All the extended network-based models perform consis- tently better than the baseline of M , in the case of NNs, although their performance drops for the case of ANs and VOs. In the case of P k , the scheme that constructs neighborhoods via the selection of the most similar neighbors performs better than the intersection- or the union-based scheme. lf add yields no relative improvements over the best performances of the separate models. lf net pro- vides an improvement for the case of NNs, reach- ing .80, which is also the best observed performance overall. However, lf net does not improve perfor- mance in the case of ANs and VOs. Performance improvements when using the ex- tended activation layer for compositional structures is consistent with experimental observations from psycholinguistics (Osherson and Smith, 1981), and shows that the activation area for phrases might be adaptive to the degree of relatedness between words. </chunk></section><section><heading>7 Discussion </heading><chunk>The results displayed in Table 1 for the fusion mod- els provide an indication of the different ways in which the operator changes the meaning of a phrase. In this section, we investigate the transformational properties of phrases as defined by their modifiers. By observing the properties of modifiers, we discuss whether their use in a phrase has mainly a transfor- mational or a merely compositional effect, based on the goodness of fit of each model, estimated during model training. </chunk></section><section><heading>7.1 The Transformative Effect of Modifiers </heading><chunk>Early research on compositionality involved apply- ing the word-level semantic similarity estimation techniques to phrases using context-based, bag-of- words models, i.e., defining the structures meaning as a function of the words in their context. Though simple and cost-effective, the aforementioned tech- niques fail to detect the effect that a word has to its linguistic context and the semantic changes on its meaning, e.g., a nice table is still a table but a fake or broken table is not. Depending on context, a modifier can affect the meaning of the encompassing phrase in different ways. For example, the modifier normal changes the meaning of normal cat much less than the modifier dead in dead cat. Moreover, the mod- ifier effect may vary for each syntactic category. For example, verbs can be transitive or intransitive, nouns can be abstract or concrete, and adjectives can be intensional or not (Boleda et al., 2013). Words that act as functions on their linguistic context have attracted much interest, and have recently been suc- cessfully handled by computational models. </chunk></section><section><heading>7.2 Estimating the Transformative Degree </heading><chunk>We categorise modifiers based on their regression performance, when training them for the lf model. Specifically, we acquire the MSE of their training as a measure for deciding the degree of their trans- formative effect on a given head noun. Taking the MSE is a sensible approach, since regression tries to derive a close approximation to observed vectorial representations of phrases and head nouns by means of transforming the head noun vector; high error in training indicates that the lf model is a poor match 45 for this modifier. We trained the lf model using Ridge Regression and estimated the MSE for each modifier. In Table 2, we present example modifiers of low, neutral, and high transformative degree, as defined by their MSE score. We observe that highly- Degree Nouns Adjectives Verbs Low news new like service great buy business black help world general use state good provide Neutral company various face care right need community better cut High railway old encourage labour rural attend defence elderly remember personnel efficient satisfy committee practical suffer Table 2: Examples of low, neutral, and high transforma- tive modifiers. transformative modifiers have a more functional in- fluence, when used in bigram structures. For exam- ple, in efficient machine, efficient has a greater effect on the meaning of efficient machine rather than, e.g., new in new machine. A new ma- chine retains the same properties of a generic ma- chine. However, an efficient machine should con- tain mechanisms that account for optimization of speed, cost, etc. Our observations suggest that mod- ifiers affect the structure in which they occur in dif- ferent ways. Some modifiers have a stronger effect on the meaning of the head noun, while others act merely as constituents of simple compositions. The proposed fusion of the transformational, lf model, with network-based or simple compositional models indicates that combining different models can yield improved performance when the transformative de- gree of modifiers is used as a fusion criterion. </chunk></section><section><heading>8 Conclusions </heading><chunk>We presented a network-based model that operates on neighborhoods of variable size to calculate simi- larity of compositional structures. We investigated various methods for composing neighborhoods of adjacent words and presented three metrics, moti- vated by psycholinguistics and metric space alge- bra, for estimating similarity between activation ar- eas. Employing variable size activation improves semantic similarity performance, revealing a differ- ent activational behavior among bigrams. We also presented a fusion of the proposed models with the lexical function model based on the transformative degree of modifiers, achieving an improvement of performance for noun-noun compositions, reaching state-of-the-art performance of 80% Spearman cor- relation with human judgements. We further inves- tigated the transformative degree of modifiers, and elaborated on their role as mostly compositional or transformational. In future work, we will further investigate the role of modifiers and their application in the proposed ac- tivation composition approaches, while also explore the criteria for deriving activations and deciding on fusion strategies. We also plan to apply network- based models on longer semantic structures. Acknowledgments This work has been partially funded by the SpeDial project supported by the EU FP7 with grant num- ber 611396, and the BabyAffect project supported by the Greek General Secretariat for Research and Technology (GSRT) with grant number 3610. References Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pi- lot on semantic textual similarity. In Proceedings of the 6th International Workshop on Semantic Evalua- tion (SemEval 2012), in conjuction with the First Joint Conference on Lexical and Computational Semantic (*SEM 2012), pages 385393. Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez- Agirre, and Weiwei Guo. 2013. Sem 2013 shared task: Semantic textual similarity, including a pilot on typed-similarity. In In* SEM 2013: The Second Joint Conference on Lexical and Computational Semantics. Association for Computational Linguistics. Ion Androutsopoulos and Prodromos Malakasiotis. 2010. A survey of paraphrasing and textual entailment methods. Journal of Artificial Intelligence Research, 38:135187. Marco Baroni and Alessandro Lenci. 2010. Distribu- tional memory: A general framework for corpus-based 46 semantics. Computational Linguistics, 36(4):673 721. Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In Proceedings of EMNLP 2010, pages 11831193. Marco Baroni, Raffaela Bernardi, and Roberto Zampar- elli. 2014a. Frege in space: A program of compo- sitional distributional semantics. Linguistic Issues in Language Technology (LiLT), 9. Marco Baroni, Georgiana Dinu, and Germ  an Kruszewski. 2014b. Dont count, predict! A systematic compari- son of context-counting vs. context-predicting seman- tic vectors. In Proceedings of the 52nd Annual Meet- ing of the Association for Computational Linguistics, volume 1, pages 238247. Yoshua Bengio, R  ejean Ducharme, and Pascal Vincent. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:11371155. Gemma Boleda, Marco Baroni, Nghia T. Pham, and Louise McNally. 2013. Intensionality was only al- leged: On adjective-noun composition in distributional semantics. In Proceedings of IWCS 2013, pages 35 46. Rudi L. Cilibrasi and Paul MB Vitanyi. 2007. The Google similarity distance. IEEE Transactions on Knowledge and Data Engineering, 19(3):370383. Georgiana Dinu, Nghia The Pham, and Marco Baroni. 2013. DISSECT-DIStributional SEmantics Composi- tion Toolkit. In Proceedings of the 51st Annual Meet- ing of ACL: System Demonstrations, pages 3136. Jorge Gracia, Raquel Trillo, Mauricio Espinoza, and Ed- uardo Mena. 2006. Querying the web: A multiontol- ogy disambiguation method. In Proceedings of the 6th International Conference on Web Engineering (ICWE 2006), pages 241248. Gregory Grefenstette. 1994. Explorations in Automatic Thesaurus Discovery. Kluwer Academic Publishers. Zellig Harris. 1954. Distributional structure. Word, 10(23):146162. Trevor Hastie, Robert Tibshirani, and Jerome Friedman. 2009. The Elements of Statistical Learning, volume 2. Springer. Wen-Liang Hung and Miin-Shen Yang. 2004. Sim- ilarity measures of intuitionistic fuzzy sets based on hausdorff distance. Pattern Recognition Letters, 25(14):16031611. Elias Iosif and Alexandros Potamianos. 2015. Similar- ity computation using semantic networks created from web-harvested data. Natural Language Engineering, 21(01):4979. Elias Iosif. 2013. Network-based Distributional Se- mantic Models. Ph.D. thesis, Technical University of Crete, Chania, Greece. Daniel D. Lee and Sebastian H. Seung. 2001. Al- gorithms for non-negative matrix factorization. In Advances in Neural Information Processing Systems, pages 556562. Nikolaos Malandrakis, Alexandros Potamianos, Elias Iosif, and Shrikanth Narayanan. 2013. Distributional semantic models for affective text analysis. IEEE Transactions on Audio, Speech, and Language Pro- cessing, 21(11):23792392. T. P. McNamara. 2005. Semantic Priming: Perspec- tives from Memory and Word Recognition. Psychol- ogy Press. Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word represen- tations in vector space. ICLR Workshop. Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proceedings of ACL, pages 236244. Jeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of semantics. Cognitive science, 34(8):13881429. Daniel N. Osherson and Edward E. Smith. 1981. On the adequacy of prototype theory as a theory of concepts. Cognition, 9(1):3558. Francis J. Pelletier. 1994. The principle of semantic compositionality. Topoi, 13(1):1124. Tamara Polajnar, Laura Rimell, and Stephen Clark. 2014. Evaluation of simple distributional compositional op- erations on longer texts. In 9th Language Resources and Evaluation Conference (LREC 2014). Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceed- ings of EMNLP-CoNLL, pages 12011211. Peter D. Turney and Patrick Pantel. 2010. From fre- quency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37(1):141 188. Peter D. Turney. 2012. Domain and function: A dual- space model of semantic relations and compositions. Journal of Artificial Intelligence Research, 44:533 585. Paul Vitanyi. 2005. Universal similarity. In Proceed- ings of Information Theory Workshop on Coding and Complexity, pages 238243. 47 </chunk></section></sec_map>