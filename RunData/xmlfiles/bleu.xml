<sec_map><section><chunk>RC22176 (W0109-022) September 17, 2001 Computer Science IBM Research Report Bleu: a Method for Automatic Evaluation of Machine Translation Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu IBM Research Division Thomas J. Watson Research Center P.O. Box 218 Yorktown Heights, NY 10598 Research Division Almaden - Austin - Beijing - Delhi - Haifa - India - T. J. Watson - Tokyo - Zurich LIMITED DISTRIBUTION NOTICE: This report has been submitted for publication outside of IBM and will probably be copyrighted if accepted for publication. It has been issued as a Research Report for early dissemination of its contents. In view of the transfer of copyright to the outside publisher, its distribution outside of IBM prior to publication should be limited to peer communications and specific requests. After outside publication, requests should be filled only by reprints or legally obtained copies of the article (e.g. , payment of royalties). Copies may be requested from IBM T. J. Watson Research Center , P. O. Box 218, Yorktown Heights, NY 10598 USA (email: reports@us.ibm.com). Some reports are available on the internet at http://domino.watson.ibm.com/library/CyberDig.nsf/home . Bleu: a Method for Automatic Evaluation of Machine Translation Kishore Papineni Salim Roukos Todd Ward Wei-Jing Zhu IBM T. J. Watson Research Center Yorktown Heights, NY 10598, USA {papineni,roukos,toddward,weijing}@us.ibm.com Abstract Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation eval- uation that is quick, inexpensive, and language- independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which sub- stitutes for them when there is need for quick or frequent evaluations. 1 </chunk></section><section><heading>1 Introduction </heading></section><section><heading>1.1 Rationale </heading><chunk>Human evaluations of machine translation (MT) weigh many aspects of translation, in- cluding adequacy, fidelity, and fluency of the translation[1][2]. Such evaluations are exten- sive, but also expensive[1]. Moreover, they can take weeks or months to finish. This is a big problem because developers of machine transla- tion systems need to monitor the effect of daily changes to their systems in order to weed out bad ideas from good ideas. We believe that MT progress stems from evaluation and that there is a logjam of fruitful research ideas waiting to be released from the evaluation bottleneck. Developers would benefit from an inexpensive automatic evaluation that is quick, language- independent, and correlates highly with human evaluation. We propose such an evaluation method in this note. </chunk></section><section /><section><heading>1.2 Viewpoint </heading><chunk>How does one measure translation perfor- mance? The closer a machine translation is to a professional human translation, the better it is. This is the central idea behind our proposal. To judge the quality of a machine translation, one measures its closeness by a numerical met- ric to one or more reference human translations. Thus, our MT evaluation system requires two ingredients: 1. a numerical translation closeness metric 2. a corpus of good quality human reference translations These reference translations can be reused over and over again and incur only a one-time startup expense. Each evaluation can be ac- complished in seconds. We fashion our closeness metric after the highly successful word error rate metric used by the speech recognition community, appro- priately modified for multiple reference trans- lations and allowing for legitimate differences in word choice and word order. The main idea is to use a weighted average of variable length phrase matches against the reference transla- tions. This view gives rise to a family of met- rics using various weighting schemes. We have selected a promising baseline metric from this family. Although our baseline metric correlates very highly with human judgments, we do know that there are subtleties and stylistic variations that are better appreciated by humans than ma- chines. For the foreseeable future, we believe these subtleties will remain relatively small ef- fects compared with other MT phenomena. We 2 present our method as a virtual apprentice or understudy to skilled human judges. We call this method Bleu, for BiLingual Evaluation Understudy. In Section 2, we describe the baseline metric in detail. In Section 3, we evaluate the perfor- mance of Bleu. In Section 4, we describe a human evaluation experiment. In Section 5, we compare our baseline metric performance with human evaluations. </chunk></section><section><heading>2 The Baseline Bleu Metric </heading><chunk>Typically, there are many perfect translations of a given source sentence. These translations may vary in word choice or in word order even when they use the same words. And yet hu- mans can clearly distinguish a good translation from a bad one. For example, consider these two candidate translations of a Chinese source sentence. Example 1: Candidate 1: It is a guide to action which ensures that the military always obeys the commands of the party. Candidate 2: It is to insure the troops forever hearing the activity guidebook that party direct. Although they appear to be about the same subject, they differ markedly in quality. For comparison, we provide three reference human translations of the same sentence below. Reference 1: It is a guide to action that ensures that the military will forever heed Party commands. Reference 2: It is the guiding principle which guarantees the military forces always being under the command of the Party. Reference 3: It is the practical guide for the army always to heed the direc- tions of the party. It is clear that the good translation, Candi- date 1, shares many words and phrases with these three reference translations, while Can- didate 2 does not. We will shortly quan- tify this notion of sharing. But first observe that Candidate 1 shares "It is a guide to action" with Reference 1, "which" with Ref- erence 2, "ensures that the military" with Reference 1, "always" with References 2 and 3, "commands" with Reference 1, and finally "of the party" with Reference 2 (all ignoring cap- italization). In contrast, Candidate 2 exhibits far fewer matches and their extent is less. For this example, it is at once clear that a program can rank Candidate 1 higher than Can- didate 2 simply by comparing n-gram matches between each candidate translation and the ref- erence translations. Experiments over large col- lections of translations presented in Section 5 show that this ranking ability is a general phe- nomenon, and not an artifact of a few examples. The primary programming task in a Bleu im- plementation is to compare n-grams of the can- didate with the n-grams of the reference trans- lation and count the number of matches. These matches are position-independent. The more the matches, the better the candidate transla- tion. For simplicity, we first focus on computing unigram matches. </chunk></section><section><heading>2.1 Modified n-gram precision </heading><chunk>The cornerstone of our metric is the famil- iar precision measure. To compute precision, one simply counts up the number of candi- date translation words (unigrams) which occur in any reference translation and then divides by the total number of words in the candidate translation. Unfortunately, MT systems can overgenerate reasonable words, resulting in improbable, but high-precision, translations like that of example 2 below. Intuitively the prob- lem is clear: a reference word should be consid- ered exhausted after a matching candidate word is identified. We formalize this intuition as the modified unigram precision. To compute this, one first counts the maximum number of times a word occurs in any single reference transla- tion. Next, one clips the total count of each can- didate word by its maximum reference count, adds these clipped counts up, and divides by the total (unclipped) number of candidate words. 3 Example 2. Candidate: the the the the the the the. Reference 1: The cat is on the mat. Reference 2: There is a cat on the mat. Modified Unigram Precision = 2/7. 2 In Example 1, Candidate 1 achieves a modi- fied unigram precision of 17/18; whereas Candi- date 2 achieves a modified unigram precision of 8/14. Similarly, the modified unigram precision in Example 2 is 2/7, even though its standard unigram precision is 7/7. Modified n-gram precision is computed sim- ilarly for any n: all candidate n-gram counts and their corresponding maximum reference counts are collected. The candidate counts are clipped by their corresponding reference maxi- mum value, summed, and divided by the total number of candidate n-grams. In Example 1, Candidate 1 achieves a modified bigram preci- sion of 10/17, whereas the lower quality Candi- date 2 achieves a modified bigram precision of 1/13. In Example 2, the (implausible) candi- date achieves a modified bigram precision of 0. This sort of modified n-gram precision scoring captures two aspects of translation: adequacy and fluency. A translation using the same words (1-grams) as in the references tends to satisfy adequacy. The longer n-gram matches account for fluency. 2.1.1 Modified n-gram precision on blocks of text How do we compute modified n-gram precision on a multi-sentence test set? Although one typi- cally evaluates MT systems on a corpus of entire documents, our basic unit of evaluation is the sentence. A source sentence may translate to many target sentences, in which case we abuse terminology and refer to the corresponding tar- get sentences as a sentence. We first com- pute the n-gram matches sentence by sentence. Next, we add the clipped n-gram counts for all the candidate sentences and divide by the num- ber of candidate n-grams in the test corpus to 2 As a guide to the eye, we have underlined the im- portant words for computing modified precision. compute a modified precision score, p n , for the entire test corpus. p n = C{Candidates} n-gram C Count clip (n-gram) C{Candidates} n-gram C Count(n-gram) . In other words, we use a word-weighted average of the sentence-level modified precisions rather than a sentence-weighted average. As an exam- ple, we compute word matches at the sentence level, but the modified unigram precision is the fraction of words matched in the entire test cor- pus. 2.1.2 Ranking systems using only modified n-gram precision To verify that modified n-gram precision distin- guishes between very good translations and bad translations, we computed the modified preci- sion numbers on the output of a (good) human translator and a standard (poor) machine trans- lation system against 4 reference translations for each of 127 source sentences. The average pre- cision results are shown in Figure 1. Figure 1: Distinguishing Human from Machine The strong signal differentiating human (high precision) from machine (low precision) is strik- ing. The difference becomes stronger as we go from unigram precision to 4-gram precision. It appears that any single n-gram precision score can distinguish between a good translation and a bad translation. To be useful, however, the metric must also reliably distinguish between translations that do not differ so greatly in quality. Furthermore, it must distinguish be- 4 tween two human translations of differing qual- ity. This latter requirement ensures the con- tinued validity of the metric as MT approaches human translation quality. To this end, we obtained a human transla- tion by someone lacking native proficiency in both the source (Chinese) and the target lan- guage (English). For comparison, we acquired human translations of the same documents by a native English speaker. We also obtained ma- chine translations by three commercial systems. These five systems two humans and three machines are scored against two reference pro- fessional human translations. The average mod- ified n-gram precision results are shown in Fig- ure 2. Figure 2: Machine and Human Translations Each of these n-gram statistics implies the same ranking: H2 (Human-2) is better than H1 (Human-1), and there is a big drop in quality between H1 and S3 (Machine/System-3). S3 appears better than S2 which in turn appears better than S1. Remarkably, this is the same rank order assigned to these systems by hu- man judges, as we discuss later. While there seems to be ample signal in any single n-gram precision, it is more robust to combine all these signals into a single number metric. 2.1.3 Combining the modified n-gram precisions How should we combine the modified precisions for the various n-gram sizes? A weighted linear average of the modified precisions resulted in en- couraging results for the 5 systems. In this case, we found that underweighting the unigram pre- cision yields a score which more closely matches human judgments. However, as can be seen in Figure 4, the mod- ified n-gram precision decays roughly exponen- tially with n: the modified unigram precision is much larger than the modified bigram precision which in turn is much bigger than the modi- fied trigram precision. A reasonable averaging scheme must take this exponential decay into account: a weighted average of the logarithm of the modified precisions would do so. Bleu uses the average logarithm with uni- form weights, which is equivalent to using the geometric mean of the modified n-gram preci- sions. 3,4 As a result, the Bleu metric is now more sensitive to longer n-grams. Experimen- tally, we obtain the best correlation with mono- lingual human judgments using a maximum n- gram order of 4, although 3-grams and 5-grams give comparable results. </chunk></section><section><heading>2.2 Sentence length </heading><chunk>A candidate translation should be neither too long nor too short, and an evaluation metric should enforce this. To some extent, the n- gram precision already accomplishes this. N - gram precision penalizes spurious words in the candidate that do not appear in any of the ref- erence translations. Additionally, modified pre- cision is penalized if a word occurs more fre- quently in a candidate translation than its max- imum reference count. This rewards using a word as many times as warranted and penalizes using a word more times than it occurs in any of the references. However, modified n-gram pre- cision alone fails to enforce the proper transla- tion length, as is illustrated in the short, absurd example below. Example 3: Candidate: of the 3 The geometric average is harsh if any of the modified precisions vanish, but this should be an extremely rare event in test corpora of reasonable size (for Nmax 4). 4 The geometric average also slightly outperforms our best results using an arithmetic average. 5 Reference 1: It is a guide to action that ensures that the military will forever heed Party commands. Reference 2: It is the guiding principle which guarantees the military forces always being under the command of the Party. Reference 3: It is the practical guide for the army always to heed the direc- tions of the party. Because this candidate is so short compared to the proper length, one expects to find in- flated precisions: the modified unigram preci- sion is 2/2, and the modified bigram precision is 1/1. 2.2.1 The trouble with recall Traditionally, precision has been paired with recall to overcome such length-related prob- lems. However, Bleu considers multiple refer- ence translations, each of which may use a dif- ferent word choice to translate the same source word. Furthermore, a good candidate transla- tion will only use (recall) one of these possible choices, but not all. Indeed, recalling all choices leads to a bad translation. Here is an example. Example 4: Candidate 1: I always invariably perpetu- ally do. Candidate 2: I always do. Reference 1: I always do. Reference 2: I invariably do. Reference 3: I perpetually do. The first candidate recalls more words from the references, but is obviously a poorer transla- tion than the second candidate. Thus, na  ve re- call computed over the set of all reference words is not a good measure. Admittedly, one could align the reference translations to discover syn- onymous words and compute recall on concepts rather than words. But, given that reference translations vary in length and differ in word order and syntax, such a computation is com- plicated. 2.2.2 Sentence brevity penalty Candidate translations longer than their refer- ences are already penalized by the modified n- gram precision measure: there is no need to penalize them again. Consequently, we intro- duce a multiplicative brevity penalty factor that only penalizes candidates shorter than their ref- erence translations. With this brevity penalty in place, a high-scoring candidate translation must now match the reference translations in length, in word choice, and in word order. Note that neither this brevity penalty nor the modi- fied n-gram precision length effect directly con- siders the source length; instead, they consider the range of reference translation lengths in the target language. The brevity penalty is a multiplicative factor modifying the overall Bleu score. We wish to make the penalty 1 when the candidates length is the same as any reference translations length. For example, if there are three references with lengths 12, 15, and 17 words and the candi- date translation is a terse 12 words, we want the brevity penalty to be 1. We call the clos- est reference sentence length the best match length. If we computed the brevity penalty sentence by sentence and averaged the penalties, then length deviations on short sentences would be punished harshly. Instead, we compute the brevity penalty over the entire corpus to al- low some freedom at the sentence level. We first compute the test corpus effective reference length, r, by summing the best match lengths for each candidate sentence in the corpus. The brevity penalty is a decaying exponential in r/c, where c is the total length of the candidate translation corpus. </chunk></section><section><heading>2.3 Bleu details </heading><chunk>We take the geometric mean of the test corpus modified precision scores and then multiply the result by an exponential brevity penalty factor. Currently, case folding is the only text normal- ization performed before computing the preci- sion. 6 We first compute the geometric average of the modified n-gram precisions, p n , using n-grams up to length N and positive weights w n sum- ming to one. Next, let c be the length of the candidate translation and r be the effective reference cor- pus length. We compute the brevity penalty BP, BP = 1 if c &gt; r e (1r/c) if c r . Then, Bleu= BP exp N n=1 w n log p n . The ranking behavior is more immediately ap- parent in the log domain, log Bleu = min(1 r c , 0) + N n=1 w n log p n . In our baseline, we use N = 4 and uniform weights w n = 1/N . 3 The Bleu Evaluation The Bleu metric ranges from 0 to 1. Few trans- lations will attain a score of 1 unless they are identical to a reference translation. For this rea- son, even a human translator will not necessar- ily score 1. It is important to note that the more reference translations per sentence there are, the higher the score is. Thus, one must be cautious making even rough comparisons on evaluations with different numbers of refer- ence translations: on a test corpus of about 500 sentences (40 general news stories), a human translator scored 0.3468 against four references and scored 0.2571 against two references. Ta- ble 1 shows the Bleu scores of the 5 systems against two references on the test corpus men- tioned above. Table 1: Bleu on 500 sentences S1 S2 S3 H1 H2 0.0527 0.0829 0.0930 0.1934 0.2571 The MT systems S2 and S3 are very close in this metric. Hence, several questions arise: Table 2: Paired t-statistics on 20 blocks S1 S2 S3 H1 H2 Mean 0.051 0.081 0.090 0.192 0.256 StdDev 0.017 0.025 0.020 0.030 0.039 t 6 3.4 24 11 How reliable is the difference in Bleu met- ric? What is the variance of Bleu score? If we were to pick another random set of 500 sentences, would we still judge S3 to be better than S2? To answer these questions, we divided the test corpus into 20 blocks of 25 sentences each, and computed the Bleu metric on these blocks in- dividually. We thus have 20 samples of the Bleu metric for each system. We computed the means, variances, and paired t-statistics which are displayed in Table 2. The t-statistic com- pares each system with its left neighbor in the table. For example, t = 6 for the pair S1 and S2. Note that the numbers in Table 1 are the Bleu metric on an aggregate of 500 sentences, but the means in Table 2 are averages of the Bleu metric on aggregates of 25 sentences. As expected, they are close for each system and dif- fer only by small finite block size effects. Since a paired t-statistic of 1.7 or above is 95% signifi- cant, the differences between the systems scores are statistically very significant. The reported variance on 25-sentence blocks serves as an up- per bound to the variance of sizeable test sets like the 500 sentence corpus. How many reference translations do we need? Our experiments in this direction are prelimi- nary. We simulated a single-reference test cor- pus in the following manner. For each of the 40 stories, we randomly selected one of the 4 reference translations as the single reference for that story. In this way, we ensured a degree of stylistic variation. Then we ranked S1, S2, and S3 by Bleu computed against this reduced ref- erence corpus. The systems maintain the same rank order as with multiple references. We do 7 not have significance numbers on this experi- ment. However, the outcome suggests that we may use a big test corpus with a single refer- ence translation provided that the translations are not all from the same translator. </chunk></section><section><heading>4 The Human Evaluation </heading><chunk>We now describe the human evaluation. We had two groups of human judges. The first group, called the monolingual group, consisted of 10 native speakers of English. The second group, called the bilingual group, consisted of 10 na- tive speakers of Chinese who had lived in the United States for the past several years. None of the human judges was a professional trans- lator. The humans judged our 5 standard sys- tems on a Chinese sentence subset extracted at random from our 500 sentence test corpus. We paired each source sentence with each of its 5 translations, for a total of 250 pairs of Chinese source and English translations. We prepared a web page with these translation pairs randomly ordered to disperse the five translations of each source sentence. All judges used this same web- page and saw the sentence pairs in the same order. They rated each translation from 1 (very bad) to 5 (very good). A description of the rat- ing scheme was displayed on mouseover and is reproduced in the Appendix. The monolingual group made their judgments based only on the translations readability and fluency. As must be expected, some judges were much more liberal than others. And some sentences were easier to translate than others. To account for the intrinsic difference between judges and the sentences, we compared each judges rating for a sentence across systems. We performed four pairwise t-test comparisons between adja- cent systems as ordered by their aggregate av- erage score. </chunk></section><section><heading>4.1 Monolingual group pairwise judgments </heading><chunk>We first present the pairwise comparative judg- ments of the monolingual group. Figure 3 shows the mean difference between the scores of two consecutive systems and the 95% confidence in- terval about the mean. The confidence inter- vals are shown as vertical bars in the plot. We see that S2 is quite a bit better than S1 (by a mean opinion score difference of 0.326 on the 5-point scale), while S3 is judged a little better (by 0.114). Both differences are significant at the 95% level. 5 The human H1 is much better than the best system, though a bit worse than human H2. This is not surprising given that H1 is not a native speaker of either Chinese or En- glish, whereas H2 is a native English speaker. Again, the difference between the human trans- lators is significant beyond the 95% level. Figure 3: Monolingual Judgments - pairwise dif- ferential comparison </chunk></section><section><heading>4.2 Bilingual group pairwise judgments </heading><chunk>Figure 4 shows the same results for the bilingual group. They also find that S3 is slightly better than S2 (at 95% confidence) though they judge that the human translations are much closer (judged as indistinguishable at 95% confidence) suggesting that the bilinguals tended to focus more on adequacy than on fluency. </chunk></section><section><heading>5 Bleu vs The Human Evaluation </heading><chunk>Figure 5 shows a linear regression of the mono- lingual group scores as a function of the Bleu score for the 5 systems. We computed the Bleu scores using two reference translations. The 5 The 95% confidence interval comes from t-test, as- suming that the data comes from a T-distribution with N degrees of freedom. N varied from 350 to 470 as some judges have skipped some sentences in their evaluation. Thus, the distribution is close to Gaussian. 8 Figure 4: Bilingual Judgments - pairwise differ- ential comparison high correlation coefficient of 0.99 indicates that Bleu tracks human judgment well. Particularly interesting is how well Bleu distinguishes be- tween S2 and S3 which are quite close. Figure 6 shows the comparable regression results for the bilingual group. The correlation coefficient is 0.96. Figure 5: Bleu predicts Monolingual Judg- ments We now take the worst system as a reference point and compare the Bleu scores with the human judgment scores of the remaining sys- Figure 6: Bleu predicts Bilingual Judgments tems relative to the worst system. Recall that the judges rated systems from 1 to 5, but the Bleu score ranges from 0 to 1. We took the Bleu, monolingual group, and bilingual group scores for the 5 systems and linearly normal- ized them by their corresponding range (the maximum and minimum score across the 5 sys- tems). The normalized scores are shown in Fig- ure 7. This figure illustrates the high correla- tion between the Bleu score and the monolin- gual group. Of particular interest is the accu- racy of Bleus estimate of the small difference between S2 and S3 and the larger difference be- tween S3 and H1. The figure also shows the rel- atively large gap between MT systems and hu- man translators. 6 In addition, we surmise that the bilingual group was very forgiving in judg- ing H1 relative to H2 because the monolingual group found a rather large difference between the fluency of their translations. </chunk></section><section><heading>6 Conclusion </heading><chunk>We believe that the simplicity of Bleu is a very appealing feature for use in the research and development cycle of machine translation tech- nology. Furthermore, given Bleus sensitivity in distinguishing small differences between sys- 6 Crossing this chasm for Chinese-English translation appears to be a significant challenge for the current state- of-the-art systems. 9 Figure 7: Bleu vs Bilingual and Monolingual Judgments tems, we anticipate that machine translation re- search is at the dawn of a new era in which sig- nificant progress occurs because researchers can more easily home in on effective modeling ideas. While we have performed an encouraging ini- tial study, we anticipate that a larger study with a more highly parameterized form of the brevity penalty, modified n-gram precision, or precision averaging expressions could be conducted to de- velop a more accurate estimator of translation quality. We also look forward to evaluating the Bleu metric on other language pairs. Finally, we believe that our approach of using the n-gram similarity of a candidate to a set of references has wider applicability than MT; for example, it could be extended to the evaluation of natural language generation and summariza- tion systems. Acknowledgments This work was partially supported by the Defense Advanced Research Projects Agency and monitored by SPAWAR under contract No. N66001-99-2-8916. The views and findings contained in this material are those of the authors and do not necessarily reflect the position of policy of the Government and no official endorsement should be inferred. We gratefully acknowledge comments about the geometric mean by John Makhoul of BBN and ongoing discussions with George Dodding- ton of NIST. We especially wish to thank our colleagues who served in the monolingual and bilingual judge pools for their perseverance in judging the output of Chinese-English MT sys- tems. References [1] E.H. Hovy. Toward finely differentiated evaluation metrics for machine translation. In Proceedings of the Eagles Workshop on Standards and Evaluation, Pisa, Italy, 1999. [2] J.S. White and T. OConnell. The ARPA MT evaluation methodologies: evolution, lessons, and future approaches. In Proceed- ings of the First Conference of the Associa- tion for Machine Translation in the Amer- icas, pages 193205, Columbia, Maryland, 1994. [3] James Child, Ray Clifford, and Pardee Lowe, Jr. Proficiency and performance in language testing. Applied Language Learn- ing, 4:1-2, pages 1954, 1993. Appendix The human judges rated each translation from 1 to 5. The following rating scheme ex- planations were provided to them by mouseover bubble help in the annotation tool. These were excerpted from the U.S. Governments In- teragency Language Roundtable Language Skill Level Descriptions: Writing[3]. 7 1. Writing vocabulary is inadequate to ex- press anything but elementary needs. Writing tends to be a loose collection of sentences on a given topic and provides little evidence of con- scious organization. 2. Can write simply about a very limited number of current events or daily situations. 3. Errors virtually never interfere with com- prehension and rarely disturb the native reader. 4. Consistently able to tailor language to suit audience and able to express subtleties and nu- ances. 5. Has writing proficiency equal to that of a well-educated native. 7 A copy is on the web at http://fmc.utm.edu/rpeckham/ilrwrite.html </chunk></section></sec_map>